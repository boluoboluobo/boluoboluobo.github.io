[{"title":"深度学习入门笔记07","date":"2021-10-07T11:59:12.000Z","url":"/2021/10/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B007/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"过拟合过拟合: 指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。 发生过拟合的原因： 模型拥有大量参数、表现力强 训练数据少 以下是故意产生过拟合的代码 减少了训练数据为300，为增加网络复杂度，设定了7层网络，每层100个神经元。效果如下 可以看到测试数据识别精度到后边都是100%，而测试数据并不理想。 抑制过拟合权值衰减该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。 具体操作：神经网络的学习目的是减小损失函数的值，这时损失函数加上权重的平方范数。即可抑制过拟合。也就是在计算loss的时候加上如下值：此处的λ是控制正则化强度的超参数，λ越大惩罚越严重。 python实现 λ=0.1时的图像 DropoutDropout是一种通过在学习过程中随机删除神经元的方法。 python实现 卷积神经网络全连接层： 相邻层的所有神经元之间都有连接。 卷积层学过opencv的知道，卷积的意思，边缘检测的sobel算子，双边滤波，高斯模糊，掩模操作都是用到的卷积。卷积核就是将一个nxn的矩阵，里面有不一样的权重，将对应像素的值乘上权重后相加，成为中间的值。同样的概念引用到这里。 卷积核也称为滤波器，填充就是做卷积图像边上的一圈填入固定值，步幅就是滤波器的位置间隔。 特征图： CNN中，卷积层的输入输出数据。有输入特征图，输出特征图 输出图像大小计算假设： 输入大小为 (H,W) H: high, W: width 滤波器大小为 (FH,FW) FH: Filter High, FW: Filter Width 步幅为 (S) S: Stride 填充为 (P) P: Padding 池化层池化是缩小高、长方向上的空间的运算。常见的是Max池化（将目标区域中取出最大元素） 池化层的特征没有要学习的参数：池化只是目标区域中最大值（或平均值）。 通道数不会变化：经过池化运算，输入输出的通道数不会变化。 微小位置变化具有鲁棒性：输入数据发生微小偏差时，仍会返回相同结果。 卷积层实现util工具包 python实现 池化层实现python实现 "},{"title":"深度学习入门笔记06","date":"2021-09-30T02:08:23.000Z","url":"/2021/09/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B006/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"参数更新这里就是将之前的权重更新的算法，封装成一个类。每个类里面都有一个update方法，这样往后如果需要修改更新权重的方法（SGD，Momentum，AdaGrad，Adm），直接更改对应类的对象即可。 e.g SGD随机梯度下降法，和之前一样，这里只是做了封装 python实现 SGD缺点SGD的下降方式呈现“之”字形，比较低效。 Momentum数学表达：此处的v相当于物理中的速度，α模拟的阻力。这种方法会更快的向x轴靠近，减少之字形带来的影响。 AdaGrad学习率衰减法：随着学习进行，学习率逐渐变小，开始多学，后面少学。 数学公式：新变量h，它保存了以前所有梯度值的平方和，更新参数时，通过乘以1除根号h，调整学习尺度。参数的元素中变动较大的元素的学习率将变小。 python实现 Adm这里具体原理没有详讲，只是贴下代码 python实现 权重的初始值权重的初始值设定很重要，这个会导致学习是否会快速进行 隐藏层的激活值的分布假设这里有5层神经网络，激活函数使用sigmoid，传入随机数据，观察分布。 out: 这个代码中主要关注的是权重的尺度，标准差的选择。从图中看，由于是sigmoid函数，随着输出不断靠近0或1，他的导数值逐渐接近0，因此偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失。这个问题就是梯度消失。层次加深的深度学习中，梯度消失问题会更加严重。 将标准差变为0.01后 out: 这次呈现集中在0.5分布，不会发生梯度消失问题。激活值的分布有所偏向，在表现力上会有问题，多个神经元输出几乎相同的值，那他们也没有存在的意义了。 尝试Xavier Glorot推荐的权重初始值: 如果前一层的节点数为n，则初始值使用标准差为1/√n 的分布 从结果可知，越是后面的层，图像变得更歪斜，呈现了比之前更有的广度分布。因为各层间传递的数据有适当的广度，sigmoid表现力不受限制。 ReLu初始值推荐Kaiming He初始值，也称为“He初始值”，使用标准差为√(2/n)的高斯分布。 Batch Normalization使用Batch Normalization，使得各层拥有适当的广度，“强制性”的调整各层的激活值分布。 优点： 使得学习更加快速进行 不那么依赖初始值 抑制过拟合 为了有适当的广度，所以需要在神经网络中添加正规化层，就是Batch Normalization层。 BN层做的事情就是使得数据分布为0、方差为1的正规化。过程如下 这里ε是一个极小值，防止下边变为0导致计算错误。 此处γ的值初始值为1，β值为0，后续通过学习调整到合适的值。 python实现： "},{"title":"pytorch笔记02","date":"2021-09-29T05:14:47.000Z","url":"/2021/09/29/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B002/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"pytorch自动微分torch.Tensor是包的核心类，有个属性.requires_grad设为True就会跟踪tensor所有操作。计算完成后调用backward()自动计算所有梯度。这个张量的梯度将累计到.grad属性中 调用.detach()可以与计算历史记录分离。 也可以用 with torch.no_grad(): 包起来。 Tensor当中有一个属性grad_fn用来记录创建了张量的Function引用。 out: tensor([[1., 1.], [1., 1.]], requires_grad=True) 做一次加法操作 out tensor([[3., 3.], [3., 3.]], grad_fn=) 做更多操作 out: tensor([[27., 27.], [27., 27.]], grad_fn=) tensor(27., grad_fn=)"},{"title":"深度学习入门笔记05","date":"2021-09-28T13:55:08.000Z","url":"/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B005/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"误差反向传播法计算图计算图：将计算过程用图形表达出来。 首先说下书上的例子： 问题1：太郎在超市买了2个100日元一个的苹果，消费税是10%，请计算支付金额。 相当简单的一道题， 2 * 100 * 1.1即可。计算图表示则是： 将x2和x1.1节点中的数字取出，符号单独放在○当中。 这种从左往右的计算方式为正向传播 从右往左的计算方式则是反向传播 局部计算简单来说就是只用关注当前的简单计算部分，其他复杂的部分不需要管。意思就是计算偏导的那种感觉。 为何用计算图解题优点： 局部计算，无论全局是多么复杂，都可以通过局部计算使各个节点致力于简单计算，简化问题。 可以将中间的计算结果全部保存起来。 通过反向传播高效计算导数。 基于反向传播的导数的传递 计算图的反向传播先看一个的反向传播例子 就是将信号E乘以节点的局部导数，然后传递到下一个节点。如果假设y = x^2 那么导数为2x，那么向下传播的值就是 E*2x，这里的x是正向传递时记录的。 链式法则 就是高数里对复合函数求导。 e.g令链式法则和计算图 反向传播对于每个层，都有forward方法和backward方法，对应正向反向传播。在训练时创建网络时，将每一层存在一个列表当中，顺序正向传播。当需要计算梯度时，将列表翻转，依次执行backward方法进行反向传播，求得梯度。 加法节点的反向传播正向计算图 此处是 z = x + y，分别对x，y求偏导。结果都是1。按照前面说的反向传播的话就是将前面给的E乘上局部的导数可知，对于加法来说，都是将E*1传给后面。 python实现： 乘法节点的反向传播考虑 f = xy的导数公式正向传播图 反向传播图 由图中可知对于乘法，是将E乘上输入信号的翻转值。 python实现: 回到苹果的例子对于买苹果的例子反向传播则是 首先最终结果是220，导数就是1，向后传播，是一个乘法传播，E乘上输入信号的翻转值，也就是200和1.1交换相乘，得到1.1和200。继续向后传播，也是乘法，交换相乘得到2.2和110。 激活函数层实现ReLU层数学公式：求关于x的导数计算图 python实现： sigmoid层数学公式计算图 python实现： Affine/Softmax 层Affine层： 就是原先乘上权重加上偏置的操作，对于矩阵的正向反向传播 python实现 Softmaxwithloss层 python实现 "},{"title":"深度学习入门笔记04","date":"2021-09-28T12:02:02.000Z","url":"/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"神经网络的学习这章主要讲的是函数斜率的梯度法 计算机视觉领域常用的特征量包括 SIFT、SURF、HOG等。 训练数据和测试数据训练数据和测试数据 ： 首先使用训练数据进行学习，寻找最优的参数，然后使用测试数据评价训练得到的模型的实际能力。 为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。训练数据又称为监督数据。 过拟合：只对某个数据集过度拟合的状态。 损失函数损失函数：表示神经网络性能的“恶劣程度”的指标，当前神经网络对监督数据再大多成都上不拟合，大多程度不一致。通常乘上一个负值，解释为“多大程度上不坏”，“性能上有多好“。 损失函数可以用任何函数，不过一般都用的均方误差或交叉熵 个人理解：就是神经网络的预测与实际的标签，相差距离的多少。 均方误差公式：yk : 神经网络的输出 tk : 监督数据 k : 数据维数 python表示： 拿MNIST数据做一次尝试 假设： 预测数据 [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0] 正确解(one_hot) [0,0,1,0,0,0,0,0,0,0] #正确解为2 out: 0.097500000000000030.5975 很明显显示第一个例子与监督数据更加吻合。 交叉熵误差公式：符号解释，同上面 由于tk中只有正确解为1，其他为0，所以只用计算正确解标签的输出的自然对数。因此可知，交叉熵误差的值是由正确解标签所对应的输出的结果决定的 python实现： 同样拿之前的假设做对比 out: 0.05108254570993380.23025840929945457 mini-batch学习​ 在机器学习使用训练数据进行学习，需要针对训练数据计算损失函数值，找出使这个值尽可能小的参数。计算损失函数的时候需要将所有训练数据作为对象。也就是说，如果有100个训练数据，需要把100个的损失函数的总和作为学习的指标。 ​ 以交叉熵为例：就只是把单个的损失函数数据扩大到了N份，最后还是需要除以N做一次正规化。就是求的“平均损失函数”。对于MNIST数据集有60000个训练数据，不可能每个都加上，这样花费的时间较多。因此会从中随机选择100笔，再用100笔数据进行学习。这就是mini-batch学习 python实现 np.random.choice() 从指定的数字中随机选择想要的数字。 mini-batch交叉熵的实现 另一种实现 设定损失函数原因为什么不以精度为指标？ 为了使损失函数的值尽可能小，需要计算参数的导数(梯度),以导数为指引，逐步更新参数值。 ​ 对权重参数的损失函数求导，表示对如果稍微改变这个权重的值，损失函数的值该如何变化。 ​ 如果为负： 通过改变权重向正方向改变，即可减小损失函数的值。 ​ 如果为正： 通过改变权重向负方向改变，即可减少损失函数的值。 如果用精度作为指标，大多数地方导数变为0，无法更新。 数值微分导数就和高数讲的一样 偏导也是和高数一样 假设公式：偏导 梯度梯度：由全部变量的偏导汇总而成的向量 e.gpython实现也很简单 梯度法梯度法：通过不断地沿梯度方向前进逐渐减小函数值的过程 寻找最小值是梯度下降法，寻找最大值是梯度上升法 数学表达：其中η是更新量，在机器学习中是学习率 学习率是实现设定的值 init_x 是初始值， lr是学习率，step_num 是重复次数 现在可以尝试求解 f(x0,x1) = x0^2 + x1 ^2 的最小值 out: 微分2.0000000000042206梯度[6. 8.]梯度下降法求 f(x0,x1) = x0^2 + x1^2 最小值[-6.11110793e-11 8.14814391e-10] 结果是十分接近（0,0）点，而事实上最低点就是（0,0）点 学习率这样的参数称为超参数。它和权重参数是不同的，权重是可以通过数据和学习自动获得的。学习率这样的超参数是人工设定的。 2层神经网络的类通过梯度下降法更新参数，由于数据是随机选择的mini batch数据，又称之为随机梯度下降法（stochastic gradient descent）简称SGD 写一个名为TwoLayerNet类 首先看__init__方法，input_size，hidden_size，output_size。依次表示的是输入层神经元数、隐藏层神经元数、输出层神经元数。input_size是784，因为输入的图像是（28x28）的，output_size也就是对应输出层，总共10个类型，所以是10。中间隐藏层是设定合适的值即可。书上设定的50个，我设定的100个，感觉效果比50的要好一点。权重使用的高斯分布的随机数进行初始化。 ​ 预测就是和之前的神经元一样，两层。乘上权重加上偏置，过一次sigmoid，然后进入二层，乘上权重加上偏置，过softmax，看概率。 ​ 损失函数，用的mini batch的交叉熵。 ​ 梯度下降，用的偏导，这个地方也是很耗时的，所以最终使用的时候用下边的gradient，使用的误差反向传播算法。 mini batch的实现 整个就是梯度下降实现了，先设定好超参数，循环10000次，每次循环都是随机抽取batch_size个训练数据，扔到梯度下降的函数中去。获得偏导的向量，然后依据偏导向量乘上学习率，一点点修改权值。最终查看loss_list会发现，损失值在不断减小。 基于测试数据的评价这里引入一个新词 **epoch : **epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时更新数据。如10000个数据，用大小为100笔数据的mini batch训练学习时，重复SGD100次，所有的数据都被“看过”则是一个epoch。更正确的做法是，10000个数据全部打乱，然后100个一组训练，顺序训练100组。为一个epoch。 加了epoch的代码，后面有绘制图形。也有将训练好后将网络保存。方便随时读出，用opencv + numpy做识别手写数字。 "},{"title":"pytorch入门笔记01","date":"2021-09-25T09:56:25.000Z","url":"/2021/09/25/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B001/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"Pytorch 张量标量（0D）只包含一个元素的张量为标量。类型通常为FloatTensor或LongTensor 向量（1D） out: torch.Size([4]) 矩阵（2D） out: torch.Size([2, 2]) 三维向量多个矩阵累加在一起。比如一张图片，有三个通道，每个通道都有一个矩阵。此处的pic2.jpg是一个28x28x3的图片。 out: torch.Size([28, 28, 3]) 切片张量这里和python的用法一样 out: tensor([1., 2.]) 对于前面的如果需要其中一个通道。一种用切片向量，一种用opencv的split方法。 4维张量对于多张图片，批处理那种 5维张量视频数据。 GPU上的张量 out: CPU cost: 7.888931035995483GPU cost: 0.1765275001525879 常见的方法生成空矩阵，没有初始化 out: tensor([[9.3673e-39, 9.5511e-39, 1.0194e-38], [4.2246e-39, 1.0286e-38, 1.0653e-38], [1.0194e-38, 8.4490e-39, 1.0469e-38], [9.3674e-39, 9.9184e-39, 8.7245e-39], [9.2755e-39, 8.9082e-39, 9.9184e-39]]) 随机初始化 out: tensor([[0.4749, 0.0095, 0.4786], [0.5207, 0.4228, 0.0364], [0.8313, 0.9352, 0.6975], [0.2701, 0.5206, 0.8709], [0.3670, 0.3378, 0.9704]]) 创建零矩阵 out: tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) 构造一个张量 out: tensor([5.5000, 3.0000]) 基于一个已存在的tensor创建一个 out: tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64)tensor([[-0.7830, 0.2870, 0.3721], [ 0.2931, 0.4255, 0.3800], [-0.1016, 0.6011, -0.7567], [ 0.4526, -1.0510, -0.4116], [ 1.4605, 1.4378, 0.4322]]) 获取维度信息 out: torch.Size([5, 3]) 加法 out: tensor([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.], [2., 2., 2.], [2., 2., 2.]]) 乘法 改变tensor的形状 out: torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) 获取标量的值 out: tensor([1])1 numpy转pytorch 主要是torch.from_numpy() pytorch转numpy "},{"title":"hexo数学公式显示问题","date":"2021-09-24T03:28:58.000Z","url":"/2021/09/24/hexo%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/","tags":[["hexo","/tags/hexo/"]],"categories":[["hexo","/categories/hexo/"]],"content":"解决方案使用 hexo-math安装： 需要Hexo 5版本以上 使用KaTeX 使用mathjax 详情见：hexo-math 使用hexo-filter-mathjax安装： 更多查看：hexo-filter-mathjax"},{"title":"深度学习入门笔记03","date":"2021-09-23T13:32:13.000Z","url":"/2021/09/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B003/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"输出层设计神经网络可以分为两类问题： 分类问题，回归问题 分类问题：属于哪一类问题。 (e.g 根据图片判断人物是男的还是女的) 回归问题：根据输入的值，预测数值问题。(e.g 根据图片预测这个人的身高体重) 恒等函数和softmax函数恒等函数：将输入原样输出。用于回归问题。 softmax函数：用于分类问题 softmax函数公式表示：python实现： 这里有个操作 exp_a = np.exp(a - c) 用来解决溢出问题。因为是指数函数，a的值如果变得很大，那会使得发生溢出现象。解决方法很简单，分子分母同时乘上一个常数C，推导如下：为一个常数这样一波操作下来，溢出问题解决，y的值是正确的，同时输出的值在0.0到1.0之间，而且softmax输出的值总和为1，因此可以把softmax的输出解释为“概率” 一般来说，神经网络是以输出值最大的作为结果，因为使用了softmax，最大的值概率也是最大的。可以为了减少计算量，不使用softmax，用最大值代替。 输出层神经元数量根据待解决问题决定，分类问题按照类别数量设定。 前向传播前向传播：使用学习到的参数，实现神经网络的“推理处理”，这个推理处理也成为神经网络的前向传播 下面通过实例——手写数字识别，完成前向传播过程 MNIST数据集MNIST数据集是手写数字图像集，有60000个训练样本，10000个测试样本。每张图像是28像素x28像素的单通道灰度图像。 书中提供了方便mnist下载的python文件。python运行mnist.py即可下载数据集，以及训练好的pickle文件。 pickle： python将程序运行中的对象保存为文件，通过加载保存过的pickle文件，可以立刻复原之前运行中的对象。 通过mnist.py中的load_mnist()函数，读出MNIST数据 这里只是读出数据，检查是否读出成功。 load_mnist(flatten=True,normalize=False,one_hot_label=False)中 flatten是确定是否将图像展开，意思是如果图像是 1x28x28的三维数组，展开后就是一个784个元素的一维数组。 normalize就是正规化的意思，打开mnist文件可以发现，他做的一件事就是每个像素的值除了一个255，让他在0.0 ~ 1.0之间。这个操作十分重要。 one_hot_label表示将正确解标签为1，意思是有0 - 9这10个数，10个类别。[0,0,1,0,0,0,0,0,0,0] 表示当前的标签正确的是2 显示图像 神经网络的推理处理 批处理"},{"title":"深度学习入门笔记02","date":"2021-09-23T09:27:36.000Z","url":"/2021/09/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B002/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"神经网络一些概念 输入层：最左边的一列，用来输入数据的一层 输出层：最右边的一列，用来输出数据 隐藏层：中间层。 神经网络和感知机是相似的，二者最大的区别就在于激活函数。 激活函数先看一下之前感知机的函数式简单做下变形引入一个函数 h(x)，将函数式继续简化其中这种会将输入信号的总和转换为输出信号，将这个函数h(x)成为激活函数 sigmoid函数 python实现 阶跃函数和之前感知机部分一样，给定一个阈值，当超过阈值就切换输出。这种函数成为阶跃函数 ReLU函数ReLU函数：在输入大于0时，直接输出该值；在小于等于0时为0python实现 图形显示 sigmoid和阶跃函数比较不同点： ​ “平滑性”不同，sigmoid函数是一条平滑的曲线，阶跃函数以0为界，输出发生急剧变化。另一个不同则是，感知机当中神经元之间流动的是0,1的二元信号，而神经网络中流动的是实数信号。 共同点： ​ 二者有相似形状，输入越小，越接近0，输入增大接近1。输出都是在0和1之间。 非线性函数激活函数都是非线性函数，为啥是非线性函数呢？ 举个栗子就能证明出来了，如果激活函数使用了线性函数，h(x) = cx，那么经过三层神经网络， 就变成了 h(h(h(x))) = c * c *c * x，完全相当于 h(x) = c^3 *x , 令a = c^3，那和h(x) = a * x 就没区别了，叠加层数变得毫无意义。为了发挥叠加层的优势，需要用非线性函数。"},{"title":"深度学习入门笔记01","date":"2021-09-22T12:03:16.000Z","url":"/2021/09/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B001/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"感知机​ 笔记是学习《深度学习入门 基于python理论实践》记录的。由于会一些python基础，所以直接从第二章开始记起。 感知机概念​ 感知机接收多个输入信号，输出一个信号。 x1,x2是输入信号，w1,w2是权重，y是输出信号。信号送往神经元时，都会乘以相对应的权重(x1w1, x2w2)。神经元计算传来的信号总和，当总和超过一定阈值则输出1，否则输出0。 数学表达： 简单的逻辑电路与门 X1 X2 Y 0 0 0 1 0 0 0 1 0 1 1 1 通过感知机来表示与门，也就是说需要找到(w1,w2,θ) 相应的权重和偏置，满足条件。如：(0.5,0.5,0.7)满足条件，即 （0.5 * x1 + 0.5 * x2 &lt;= 0.7 时为0， &gt; 0.7时为1）当然(0.5,0.5,0.8)也可以 python实现 与非门 X1 X2 Y 0 0 1 1 0 1 0 1 1 1 1 0 与非门就是跟与门相反，对应(w1,w2,θ)相反即可。(-0.5,-0.5,-0.7) python实现 或门 X1 X2 Y 0 0 0 1 0 1 0 1 1 1 1 1 这部分书上没有给对应的 (w1,w2,θ) ，我自己算了一个。计算过程就是 将x1,x2，作为横竖坐标，并将(0,0),(1,0),(0,1),(1,1)点画到坐标中，每个点Y如果是0，画○，如果是1，画×。 得到一幅图像，那么需要找到一条线将×和○分开，这条线假设为 y = kx + b 转到 x1,x2坐标上就是 x2 = kx1 + θ. 对应 w2x2 = w1x1 + θ，图中可知这个线必然是 大于0，小于1，故 0 &lt; θ &lt; 1，w1 &lt;= -1 ,将w2除到右边去，因为x2 &gt;= 0 ,可知 w2 &lt; 0。 得到条件 0 &lt; θ &lt; 1，w1 &lt;= -1， w2 &lt; 0 我设定的值为 (-1,-1,0.5)，但是好像刚好和或门相反，然后将值取反即可(1,1,0.5) python实现 通过这些代码，就能发现，与门，与非门，或门都是具有相同构造的感知机，他们的区别就在于权重参数的值 多层感知机异或门异或门：仅当x1，x2当中的一方为1时才会输出1 X1 X2 s1 s2 Y 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 对于前面的与门，与非门，或门用一层感知机即可实现 或门感知机可视化。可以发现，与门，与非门，或门都可以用一条线来划分开来。那么异或门 无法用一个感知机划分开。单层感知机无法分离非线性空间。 解决方法： 感知机可以叠加，多层感知机即可完成。 上面x1,x2到s1是做的与非门，到s2是做的或门，s1,s2到y做的是与门。 python实现： 注意：感知机总共由3层构成，有权重的层只有2层（0层 ~ 1层，1层 ~ 2层），因此称为”2层感知机”。 具体过程： 第0层的两个神经元接收输入信号，并将信号发送至第1层神经元 第1层的神经元将信号发送至第2层神经元，第2层神经元输出y 单层感知机无法表示的东西，通过增加一层就可以解决。"},{"title":"tensorflow笔记03","date":"2021-09-18T03:23:53.000Z","url":"/2021/09/18/tensorflow%E7%AC%94%E8%AE%B003/","tags":[["python","/tags/python/"],["TensorFlow","/tags/TensorFlow/"]],"categories":[["TensorFlow","/categories/TensorFlow/"]],"content":"Tensorflow TensorBoard数据流图可视化tf.summary.scalar() tf.summary.histogram() tf.summary.Filewriterr: ​ writer = tf.summary.Filewriter(“summary_dir”,sess.graph) 将摘要和图形写入summary_dir中 在控制台输入tensorboard –logdir=summary_dir, 在浏览器中输入他给的地址即可。 拿上一篇的代码举例 浏览器中显示 指定CPU和GPU设备 配置显示使用设备 使用配置： 使用CPU0运行 使用GPU0运行 "},{"title":"tensorflow笔记02","date":"2021-09-18T03:18:23.000Z","url":"/2021/09/18/tensorflow%E7%AC%94%E8%AE%B002/","tags":[["python","/tags/python/"],["TensorFlow","/tags/TensorFlow/"]],"categories":[["TensorFlow","/categories/TensorFlow/"]],"content":"矩阵基本操作及实现常见矩阵操作 "},{"title":"tensorflow笔记01","date":"2021-09-17T07:14:31.000Z","url":"/2021/09/17/tensorflow%E7%AC%94%E8%AE%B001/","tags":[["python","/tags/python/"],["TensorFlow","/tags/TensorFlow/"]],"categories":[["TensorFlow","/categories/TensorFlow/"]],"content":"Tensorflow安装安装python，安装CUDA CUDA版本号查看，打开英伟达控制面板，左下角系统信息 对应的CUDA11，网上的pip安装TensorFlow如果失败，或者想要用GPU的，可以用这里有大佬编译好的包即可。 TensorFlow常量、变量和占位符 一些补充"},{"title":"OpenGl笔记02","date":"2021-06-13T11:26:31.000Z","url":"/2021/06/13/OpenGL%E7%AC%94%E8%AE%B002/","tags":[["OpenGL","/tags/OpenGL/"]],"categories":[["OpenGL","/categories/OpenGL/"]],"content":""},{"title":"OpenGL笔记01(建立OpenGL和用C++创建一个窗口)","date":"2021-06-13T10:01:08.000Z","url":"/2021/06/13/OpenGL%E7%AC%94%E8%AE%B001/","tags":[["OpenGL","/tags/OpenGL/"]],"categories":[["OpenGL","/categories/OpenGL/"]],"content":"建立OpenGL学习使用的环境： Windows 10, Visual studio 2017, 项目架构x86, GLFW 32位。 下载GLFW进入网站： GLFW网站 Download下载GLFW，这里是源码，下边有预编译好的版本。Document可以查看文档（其中就有创建窗口的方法）。 设置库添加依赖文件在项目中添加Dependency文件夹，用来存放GLFW依赖。上边下载的GLFW的预加载文件，解压后。 主要用到两个。在lib-vc2017中，glfw3.dll和glfw3dll.lib暂时可以不需要，主要用到静态链接库glfw3.lib 配置属性打开VS右边的属性管理器，右键项目 -&gt; 属性(properties)，如下配置 点击附加包含目录，添加include位置，但是不推荐直接写绝对路径，因为项目不一定会在这个位置，当然vs提供了一个宏定义(Macros)，点击宏,搜索SolutionDir即可找到。 设置配置 Dependency位置，验证配置正确与否，可以在包含目录点击三角，下边有个编辑，有个计算的值，把值拿出来放在文件夹路径下，看看能不能访问到，能访问到就ok了。 同样的方法设置链接器(linker) 设置需要的依赖项 (Additional Dependencies) 创建一个窗口打开GLFW官网的Document页面，复制代码 F5运行即可得到一个窗口。 绘制一个三角形绘制是在glClear之后进行绘制。需要glBegin(GLenum) glEnd()。添加代码 运行即可。"},{"title":"Kolin笔记08(高阶函数，内联函数)","date":"2021-05-09T12:51:59.000Z","url":"/2021/05/09/Kotlin%E7%AC%94%E8%AE%B008/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"高阶函数定义：如果一个函数接收另一个函数作为参数，或者返回值的类型是另一个函数，该函数称为高阶函数。 Kotlin增加了一个函数类型概念，也就是说可以传递函数作为参数 基本规则 解释： -&gt; 左边部分用来声明该函数接收什么参数，多个参数之间用逗号隔开。 右边部分用于声明该函数返回值是什么类型，如果没有就是 Unit 相当于java中的Void 使用 这里 ::plus ,::minus 的写法是函数的引用写法，表示将plus(),minus()函数作为参数传递给num1AndNum2()函数中。 简化：显然不可能每次使用高阶函数都要创建新的函数，这样太麻烦了。因此可以这么写。 内联函数: inline简单分析下高阶函数的实现原理 用法： noinline与crossinline前面说明了，inline是直接将Lambda表达式替换到了调用的地方，减小了内存开销。考虑一个情形：如果高阶函数接收了两个或更多的函数类型参数，此时加上了inline关键字，Kotlin编译器会自动将所有的引用全部进行内联。如果我们只想让它内联其中一个Lambda表达式怎么办呢？ 这时就可以考虑使用noinline: block2 参数前加上了noinline关键字，现在就对block1进行内联，block2就不会了。 为什么Kotlin要提供noinline关键字 由于内联函数需要进行代码替换，因此它不是真正的参数属性，非内联函数类型参数可以自由的传递给其他任何函数，因为他是一个真实的参数，而内联函数，只能传给另一个内联函数。 绝大多数高阶函数是可以直接声明成内联函数的，但也有少部分情况。 这里会进行报错，首先，在runRunnable函数中，创建了一个Runnable对象，在Runnable的Lambda表达式当红传输了函数类型参数。而Lambda表达式在编译的时候会被转成匿名类的实现方式，实际上上述的代码是在匿名类当中调用了传入的函数类型参数。 内联函数中引用 Lambda表达式允许用return关键字进行返回，但是由于在匿名类中调用的函数类型参数，此时不能进行外层调用函数返回的，只能对匿名类中的函数调用返回。:dizzy_face::dizzy_face::dizzy_face: 此时就可以用crossinline关键字 加上了crossinline就不会报错了，加上crossinline后，无法再调用runRunnable函数时的lambda表达式中用return进行返回了，但是可以使用 return@runRunnable进行局部返回。"},{"title":"Kmp算法","date":"2021-05-05T08:20:14.000Z","url":"/2021/05/05/Kmp%E7%AE%97%E6%B3%95/","tags":[["算法","/tags/%E7%AE%97%E6%B3%95/"],["kmp","/tags/kmp/"],["kotlin","/tags/kotlin/"],["C/C++","/tags/C-C/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"kmp算法本身匹配的方法理解感觉不太难，最大的难点可能就是next数组的计算，不容易理解。匹配的原理就是，提前算出子串的一个next数组，这个next数组记录了子串中到当前位置的字符串的最长前后缀（前后部分的相同部分的最长长度），利用这个next表，当子串和主串比对的时候，如果不同，就会找next表，利用这个最长长度，计算移动到对应的位置，使得最长前面的部分不用在去匹配，用来提高匹配效率。本文是参考的《王道考研数据结构》上写的，只是记录的一些自己的理解哈。 前缀，后缀，部分匹配值前缀：除最后一个字符以外，字符串的所有头部子串 后缀：除第一个字符以外，字符串的所有尾部子串 部分匹配值：字符串的前后缀的最长相等前后缀长度 以 “ababa”为例 字符串 前缀 后缀 部分匹配值 a ∅ ∅ 0 ab {a} {b} 0 aba {a,ab} {a,ba} 1 abab {a,ab,aba} {b,ab,bab} 2 ababa {a,ab,aba,abab} {a,ba,aba,baba} 3 部分匹配值 = (前缀集合 ∩ 后缀集合 ) 的长度 恭喜！这里next数组的雏形就出来了，说实话这就是next数组。真正最后用的也是在这个基础上优化出来的Next数组。接下来就是使用这个数组，和如何优化。 S a b a b a next 0 0 1 2 3 部分匹配值的使用主串：a b a b c a b c a c b a b 子串：a b c a c 首先拿到next数组 序号 1 2 3 4 5 S a b c a c next 0 0 0 1 0 第一次匹配： ababcabcacbab abc a 和 c不匹配。 前面 ‘ab’ 匹配。最后一个匹配字符b对应的匹配值为0 按照 ： 移动位数 = 已匹配的字符数 - 对应的部分匹配值 得出： 移动位数 = 2 - 0 = 2 第二次匹配： ababcabcacbab ​ abcac b 和 c不匹配。前面’abca’匹配，最后一个匹配字符a对应的匹配值为1 移动位数 = 4 - 1 = 4 第三次匹配： ababcabcacbab ​ abcac 子串比对完成。主串没有回退，所以KMP算法时间复杂度为O(n + m) 算法改进当前的算法 移动位数 = 已匹配的字符数 - 对应的部分匹配值 转为伪代码。 优化使用部分匹配的时候，每次失败都需要找到前一个元素匹配的部分匹配值，使用起来不太方便。将数组右移一位，这样一旦失败直接取到对应的值 序号 1 2 3 4 5 S a b c a c next -1 0 0 0 1 第一个元素为右移之后空缺，用-1填充。目的是第一位不匹配，向右移动一位。就是：移动位数 = 已匹配的字符数（0） + 1 = 1 最后一个元素溢出，因为根本用不到。 这样上面的式子就改为 那么计算移动应该到达的位置（子串中指针j应该移动到的位置） 因为每次j = next[j] + 1每次都要+1，很麻烦，把+1放到数组的值里面。这样一旦不匹配就移动到第next[j]的位置即可。 对于数组是从0开始计算的话，不用这个+1操作了，本身就已经是对应的坐标位置了。 OK，以上就是next怎么来的原因。 接下来就是计算next的代码了。 next数组计算代码书上的错误 这块部分我照着改成C++，Kotlin的方式写了，得出来的结果都不对。自己推逻辑感觉没问题，但是计算机算出来就是不对。最终的问题还是出在指针的位置问题。因为书上讲的编号都是以1开头来算的，但是字符串可不是从1开始算的啊。导致字符串和next数组根本是错开的。问题就出在 T.ch[i] == T.ch[j]上面。得到的next数组就是 0,1,1,1,1。当然如果传进来的字符串也是从1开始算的就没问题了。 书上代码纠正 按照从0开始计算的代码从0开始的话就不会出现上面的问题，从1开始逻辑没理清就很容易出现错位问题。 代码讲解额，这块还是按照书上从1开始的方式讲解吧。首先是初始化，i = 1,j = 0。i是持续向后走的，j指针是来回跳的。j主要作用就是和i进行比较，如果i，j的字符相等，i，j同时向后移动，并将j的当前位置赋给i对应的next元素，为什么这么做？分开说 i,j同时向后移动做的事就是为了下一次循环再次比对，是否还是相等 将j的当前位置赋给i对应的next元素，为的是记录当出现不匹配的时候，跳转的位置。同时也是完成了前面说的右移操作。也就是说这个位置是记录在匹配好的时候i位置的后一位，也就是做了++i之后的操作。 还有一个操作就是判断了 j == 0，刚好是第一个的前一个，j = 0是为了方便后面+1直接刚好是第一个，这样只要不匹配，得到的坐标就是第一个的位置，直接子串拉回第一个，从头判断 else部分就是当不匹配也不是最初状态的时候，j回退到上一次匹配到的位置，下次循环紧接着之前的位置继续匹配。 KMP全部代码 这块没有什么难度了。大家自行分析吧。 速成KMP，推荐一个视频。觉得文章有些长，可以看这个b站大佬的KMP视频，讲的非常好。 KMP字符串匹配算法1 KMP字符串匹配算法2"},{"title":"Kotlin笔记07(延迟初始化，密封类，扩展函数，运算符重载)","date":"2021-05-05T01:14:27.000Z","url":"/2021/05/05/Kotlin%E7%AC%94%E8%AE%B007/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"延迟初始化Kotlin的判空等特性，都是为了保证程序安全所设计的，但是有时这些设计会变得比较麻烦。 对于类中存在很多全局变量，为了保证满足kotlin的空指针检查，不得不写上很多非空判断保护才行。问题解决办法则是，对全局变量进行延迟初始化。 这里lateinit关键字，告诉kotlin我会晚些对这个变量初始化。此时value就不用在一开始就赋值为null，并且也不用做判空处理。 注意：使用lateinit也是有风险的，如果在没有初始化的时候使用它，同样会崩溃。所以使用lateinit的前提是，必须确保在他被任何地方调用之前，已经完成初始化工作。 Android例子： 判断变量是否被初始化 ::adapter.isInitialized可以用来判断变量是否初始化，这个是固定用法。 密封类密封类关键字：sealed class 场景： 创建一个Result.kt 定义Result接口，Success 和Failure实现这个接口。然后使用： 这里的else是必须写的，否则Kotlin认为缺少分支条件无法编译通过，实际上Result的结果只可能是Success和Failure，这里的else完全是为了通过kotlin的检查语法而已。另一个缺点就是，当需要添加新的类UnKnown类，也是实现Result接口，用来实现未知情况的处理，但是忘了修改getResultMsg方法，此时编译器也不会提示，而是进入else。 利用 sealed class 密封类是可以被继承的，因此需要添加括号。 此时getResultMsg方法可以写成 当when语句传入密封类时，kotlin会自动检查这个密封类有哪些子类，并强制要求每个子类对应的条件全部处理，这样能够保证没有else条件，也不可能出现漏写分支的情况。 扩展函数扩展函数：即使在不修改某个类的源码的情况下，仍然可以打开这个类，向该类添加新的函数。 语法结构： 情景：一段字符串可能有字母、数字、特殊符号，我们希望统计其中的字母数量 这是常见的实现过程，使用扩展函数 将lettersCount方法定义为String的扩展函数，他就自动带有了String上下文，不再需要接收参数了。定义好扩展函数之后，就可以直接这么用了 扩展函数能够让API更加简洁，String类是一个final类，任何一个类都不能继承他，但是在kotlin就不一样了，可以向String扩展任何函数，让他的api更加丰富。让编程更加简便。 运算符重载语法结构： Kotlin的运算符重载允许我们让任意两个对象进行相加 举个栗子:grin: Money类，默认赋值value 接下来重载实现两个Money相加 那么我想Money直接和数字相加呢，Kotlin是允许运算符多重重载的。 附一下运算符的对照表 扩展函数和运算符重载的应用这里比较好玩的东西就是这俩结合起来。 让String类型的字符串用上乘法表达式，让他重复n遍。 使用： kotlin也提供了重复n遍的repeat函数，因此可以这么写 "},{"title":"Kotlin笔记06(标准函数和静态方法)","date":"2021-05-04T15:32:13.000Z","url":"/2021/05/04/Kotlin%E7%AC%94%E8%AE%B006/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"标准函数with、run和applywith有两个参数，第一个参数传递一个对象，第二个参数是Lambda表达式，with会在lambda表达式中提供第一个参数对象的上下文，并且lambda表达式中最后一行代码作为返回值返回。 作用：在多次调用一个对象的时候，让代码变得更加简洁 使用with之后 runrun函数和with是差不多的。只不过run函数是不能被直接调用的，需要用某个对象进行调用，run也是接受一个Lambda，并传入调用对象的上下文。其他和with一样 applyapply和run是极其类似的，都是在某个对象上使用，都是接受一个Lambda，但是能指定返回值，只会返回调用对象本身 定义静态方法companion object静态方法对于java来说是很简单的一件事，就是在方法上声明一个static Kotlin弱化了静态方法，因为Kotlin提供了一个更好的语法特性，那就是单例类。比如这个工具类，Kotlin推荐的是单例写法 调用方式也是Util.doActioin() 但是如果我们只是希望一个类当中的某个方法变为静态方法怎么办呢？这就用到kotlin的companion object doAction2()方法其实也并不是静态方法，companioin object 这个关键字实际上会在Util类的内部创建一个伴生类，而doAction2()方法就是定义在这个伴生类里面的实例方法。只是Kotlin会保证Util类始终只会存在一个伴生类对象，因此调用Util.doAction2()实际上是调用了Util的伴生类对象的doAction2()方法 定义真正的静态方法Kotlin没有直接定义静态方法的关键字，只是提供了一些语法特征来支持类似静态方法的调用。因此用java调用Kotlin的静态方法时，会发现找不到这个方法。 这时就需要定义真正的静态方法了，两种方法：注解 和 顶层方法 注解 注意：@JvmStatic注解只能加在单例类或者companion object中的方法上。 顶层方法顶层方法：指的是那些没有定义在任何类当中的方法 Kotlin会将所有顶层方法全部编译成静态方法。首先创建一个Kotlin文件，比如Helper.kt文件，里面可以定义一个方法。 顶层方法都是编译成静态方法，那么怎么用它呢？ Kotlin当中：顶层方法可以在任意位置使用，直接调doSomething()就可以了。 java当中：因为java中没有顶层方法的概念，Kotlin编译器会自动创建一个 HelperKt的java类，doSomething就是以静态方法的形式定义在HelperKt类里面。这个类的名字就是 Kotlin文件的名字加上Kt。 tips: 看到这里的时候，我才发现定义真正静态方法的知识点，都忘了。之前代码里的笔记也没写过这部分。哈哈，这里就补上。:joy:"},{"title":"A*寻路算法，启发式搜索（超详细实现）","date":"2021-05-03T06:21:18.000Z","url":"/2021/05/03/AStar/","tags":[["A*","/tags/A/"],["算法","/tags/%E7%AE%97%E6%B3%95/"],["java","/tags/java/"],["寻路算法","/tags/%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"A*寻路算法&nbsp;&nbsp;&nbsp;&nbsp;关于A算法网上优秀的文章有很多，本篇只是参考了那些文章，我自己实现了A算法后，对A的个人理解，在此记录下A的实现过程，同时防止时间久了自己忘了，随时可以回来看。如有不对的地方，欢迎评论指正。 &nbsp;&nbsp;&nbsp;&nbsp;A启发式搜索，什么是启发式？就是给搜索的时候有一个参考，大致的方向，让搜索的时候有一定的方向性的去寻找，这样相比广度遍历要少一些搜索范围。那么A是如何做到启发式搜索，在我看来就是那个公式F = G + H。 G：当前点到起始点路径上的消耗，H：当前点到终点的距离。通过公式可以看出F是受到G和H的影响的，可以说F越小所得到的路径就越小，所以按照F的大小顺序来进行遍历，即可获得达到终点的最短路径。下面先上代码，具体详细实现步骤和思路在代码后面。 MapItem.java 这个只是记录每个格子的F值，G值，H值，前一个格子坐标用的，至于为什么记录前一个格子坐标，原因很简单，就是为了找到路径后，按照坐标回溯从而将路线展现出来。 Main.java Ok,全部代码就在这里了。 &nbsp;&nbsp;&nbsp;&nbsp;Openlist :开放列表，存放预选节点，也就是预选列表，选择下一个被遍历节点就是从这里面选的。 &nbsp;&nbsp;&nbsp;&nbsp;Closelist:关闭列表，存放已经遍历过的节点，也就是说已经确定了，被检查过周围节点的节点，确定了周围节点都是最优路线的，这个节点不能被重复遍历的。 &nbsp;&nbsp;&nbsp;&nbsp;关于F,G,H计算：F = G + H，G的话就是从开始到当前点的消耗，上下左右四个方向走一步距离为1，斜对角那么就是约为1.4，为了方便计算就都扩大10倍，10和14。其实不用那么严格，只要保证两边之和大于第三边即可，朝正方向的步长小于斜对角长度。 H的距离的话，愿意用勾股定理计算也可以，不过为了方便我还是用的x向的距离加上y向的距离。也可表示距离长度。没什么影响的。 大体思路：​ 将起始节点加入openlist当中，然后就进入循环查找路线。一个循环过程： 从openlist当中找F值为最小的节点作为父节点然后遍历周围的节点，若有F相同选择G最小的 将父节点从openlist中移除，加入到closelist当中 遍历父节点周围的节点，如果是墙壁或边界，跳过该节点。如果在closelist当中，跳过该节点。 如果该节点（父节点周围的节点）不在openlist当中，计算F,G,H,并记录父节点坐标，G值为父节点的G值加上到达当前点的步长消耗。（因为这种节点，第一次遍历到，那么到达它目前的最优路径就是通过父亲节点到达）。 如果该节点在openlist当中，说明他也是待选节点，他已经被遍历过了，那么他就有了选择，选择原先的父节点，还是当前的父节点，这取决于G值，因为此时H值是不会变的，G值越小消耗越小，F就会越小，路径会越短。那么F的大小决定了优先级，决定了该节点被当做父节点的优先级。F越小优先级越高，优先级越高被先遍历到的可能性越大。（这块就是启发式搜索，我个人感觉这个F起到优先级作用，改变了搜索的优先顺序，从而使得寻路更加的高效。） 重复以上步骤，直到openlist为空（为什么会空呢，当开始节点在一个被障碍物组成的封闭空间内，边界被跳过，closelist被跳过，每次循环都有节点放入close当中，那么封闭空间内节点终将会被遍历完，全部放入closelist中，自然就为空了），或者目标节点放入了openlist（找到路径）。 我写的时候一些担心和小问题： F值相同的情况 &nbsp;&nbsp;&nbsp;&nbsp;不用担心F值相同的节点，因为都相同的话，位置不同，每次遍历都是优先选择F最小的，终会遍历完所有的相同F的坐标，那么它如果离终点较远的话，要知道H的值是不会改变的，他的G值会变得很大，导致F也会变得很大，那么它周围的节点很大可能是不会被下一次循环选中作为父节点的。 遍历周围节点太麻烦 ​ 当初最早的时候还傻乎乎的用一堆if去判断方向，利用偏移量，这个思路我也忘了是从哪里学过来的。自己先定好一个遍历顺序，顺时针啊逆时针啊随你，然后按照顺序将x,y的偏移写到两个数组中去，反正就装起。遍历的时候，循环这个数组，只用写一个当前节点坐标加上偏移量即可，计算G值也很简单，由于遍历是有顺序的，那么就可以通过循环次数的奇偶性判断是对角线还是正方向，对应加上值即可。 以上就是思路，至于代码详细解释，emmm…代码里的注释应该写的比较清楚吧，havePointInCloseList(),havePointInOpenList(),就是判断节点是否在关闭列表和开启列表内用的。"},{"title":"BFS广度优先遍历寻找最短路径(超详细实现过程)","date":"2021-05-03T06:05:19.000Z","url":"/2021/05/03/bfs/","tags":[["算法","/tags/%E7%AE%97%E6%B3%95/"],["java","/tags/java/"],["寻路算法","/tags/%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/"],["bfs","/tags/bfs/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"广度优先遍历寻找最短路径&nbsp;&nbsp;&nbsp;&nbsp;最近一直想搞A算法，发现有部分没理解清楚。于是找到了广度优先遍历寻路算法学习了下，想看看可不可以对写A有什么帮助。广度优先遍历寻路算法本身并不难，概括来说就是像雷达一样，一层一层进行寻找目标点。当找到目标点后进行回溯。从而找到最佳路径。也就是说每走一步都要找到到达该点的最短的路径，最终得到到达所有点的最短路径。 废话不多说上代码。具体解释在代码后面 代码Point.java MyMap.java test.java 效果 类的解释： &nbsp;&nbsp;&nbsp;&nbsp;Point.java里面写的就是一个点的类里面就是装的x，y没啥说的。MyMap.java写的类就是记录前一个点的位置prex 和prey其实这两个参数用一个Point来记就好了。还有一个最重要的就是那个price，记录到达当前点所需要的消耗。也就是路径长度。然后就是test.java了。test当中就是bfs函数最重要了。 大体遍历的思路：&nbsp;&nbsp;&nbsp;&nbsp;将开始节点加入队列，然后在循环中先读出队列头，即出队列，读出的头就是当前节点，围绕该节点遍历周围的所有节点，分为：左上，上，右上，右，右下，下，左下，左共8个方向。然后将周围的节点依次加入到队列中，并且设置该节点的权值和前一节点坐标。不断循环重复以上操作，逐层遍历直到找到目的节点，或者队列为空，若队列为空都没有找到目标节点那么就是该节点不可达。 程序的超详细实现过程：&nbsp;&nbsp;&nbsp;&nbsp;首先为了方便起见设置一个char类型的二维数组当做地图，大小是10*10的。然后遍历找到起始点和结束点的位置坐标，Point类型的start和end。还有dx和dy两个数组。里面存的是遍历的顺序，也就是坐标的偏移量，两个数组每个都是8个元素，因为是8个方向的嘛。然后直接进重点bfs()函数，先建立一个queue队列，将开始节点加入队列，设置与地图等大的MyMap类的一个数组，并初始化。 &nbsp;&nbsp;&nbsp;&nbsp;开始遍历，一个while循环，循环条件是queue大小要大于0，循环体中则是读出队列头，出队列，一个for循环用来遍历该点周围8个方向的点，当然要有约束条件周围的点不能超过10*10的范围同时不能为墙壁，判断墙壁用的是之前char数组的地图，而遍历记录price和前一个点的坐标的是那个MyMap数组。queue出来的点为当前点，遍历的是周围点，周围的点因为是8个方向，当扩大的时候必然会存在重复遍历的问题，不能单纯的用一个布尔来标记是否重复遍历，因为如果遍历到就进行标记的话，得到的路径不一定是最短的。因此我用的是一个price来记录到达该点的路径长度，判断是否遍历过了也很简单，如果price为0的话那么说就是第一次遍历，若不为0那就说明不是第一次遍历。两种情况分开讨论。（1）第一次遍历到该点：因为是第一次遍历到，所以要先将其加入到队列中，然后将设置消耗price，就是获取当前点（队列里出来的点）的price加上两点间路径长度，两点间路径长度：斜对角是14，上下左右相邻是10。为什么这么设，其实就是勾股定理的出来乘以10，也可以不这么设定只要相邻的距离相加大于斜对角距离就可以，满足三角形两边之和大于第三边就可以。然后设置前一点坐标为当前点。（2）第二次遍历到该点：因为不是第一次了，所以这里就不用再将其加入队列中，这里要做的就是判断应不应该与这个点连线，怎么说呢，因为他已经有price记录了，说明他已经有主子了，那么此时就要判断他指向的主子称不称职。那他的price与自己的price加上距离进行比较，如果他的price较小说明主子很称职，也就是路径较短，如果自己的price加距离要小于他自己的price说明他当前的主子不称职，就改变他的前一点的标记，改成当前点的坐标，并改变price值。如此一来可以保证到达每个点都是最小权值，按照点记录的前一点坐标进行回溯即可得到到达点的最短路径。 发现节点重复后判断消耗，发现有更好的路径则改记录坐标和消耗值 然后找到路径直接回溯，更改地图，将走过的节点改为‘’符号，最后整体打印地图。即为最短路径。这个算法寻路效率与A算法相比还是比较低。不过还是挺好理解的。"},{"title":"Mysql数据转移Redis","date":"2021-05-03T05:58:57.000Z","url":"/2021/05/03/mysql2Redis/","tags":[["mysql","/tags/mysql/"],["redis","/tags/redis/"]],"categories":[["数据库","/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"]],"content":"​ 最近毕业设计上想加上一个redis，就必须要把MySQL里面的数据转移到redis当中。那么问题来了，关系型数据库的数据怎么转移到非关系型数据库中呢？然后百度MySQL数据迁移redis，然后果不其然，各个博客保持了高度统一，看那么几篇博客，几乎都是转载的，有原创的也很少。当然我也只是个小白，摸索了很久才成功了。 ​ 如果百度过了的同学对events_all_time这个表很熟悉吧，哈哈。我也不多说了，原文写的没问题，也不是说没问题，是思路没问题，语法也没问题，但是实际操作后会报错，这个报错原因也不知道为啥。我在Stack Overflow上面看到的解决方法不是用的redisprotocol。对于我这个小白来说，文章里面有的地方没有解释。看起来有点费劲，搞不明白为啥这么写。然后我这篇就是先解释下redisprotocol，再说下mysql转redis命令的方法，最后说下mysql里面多条数据以什么格式，怎么存到redis。（这里附一下那个不知道被转了多少遍的events_all_time 2333） 正文百度上的mysql迁移redis的方法，看不懂的同学，结合官网的解释，食用更佳哦。 ​ 用redis普通的客户端插入大量数据并不好，所以官方推荐的方法是生成一个符合redis协议的text文件，用redis统一去调用。text文件可以写redis命令，也可以写redis协议。协议格式如下 ​ 对应的 ‘\\r’ (ASCII码13),对应的 ‘\\n’ (ASCII码10)。然后那个和$符号的意义就是。号后面加数字，表示整个命令总共有多少个参数，包括命令本身。$后面加数字，表示对应的参数，有多少字节。 ​ 举个栗子 SET key1 value1 转化为redis的协议就是 *3\\r\\n$3\\r\\nSET\\r\\n$4\\r\\nkey1\\r\\n$6\\r\\nvalue1\\r\\n *3–&gt;有set,key1,value1三个 $3–&gt;后面SET有三个字节 $4–&gt;后面key1有4个字节 $6–&gt;后面value1有6个字节 是不是很简单^_^!!!!! 然后看看那个不知道被转了多少遍的events_to_redis.sql。明白了redis protocol之后再看这个代码，一目了然。作者的意图就是把数据库里面查询出的值，拼接成相应的命令。 最终用 mysql -h ‘ip地址’ -u’用户名’ -p’密码’ ‘database’ –skip-column-names –raw &lt; events_to_redis.sql | redis-cli –pipe 这条命令意思就是 mysql登录后，用‘database’这个数据库， –skip-column-names(使mysql输出不包含列名) –raw（使mysql不转换字段值中的换行符），然后就是运行events_to_redis.sql，用管道传入redis-cli当中运行即可。 这里有一个问题，就是按照 *3\\r\\n$3\\r\\nSET\\r\\n$4\\r\\nkey1\\r\\n$6\\r\\nvalue1\\r\\n这样的方式是不行的。总是报错 ERR Protocol error: expected ‘$’, got ‘ ‘ 具体原因我并不太清楚，如果有同学解决了，希望可以评论说一下，让我这个小白也学习一下。 解决方案 我最后的解决方案其实就是在前面了解了的基础上，把sql语句修改了，让mysql最终输出的格式是普通的 SET key value的格式，然后运行即可。 总而言之，不管你用的是mysql，java，python，C/C++，甚至是记事本。只要输出的格式是redis protocol或redis命令，传入redis-cli当中就可以完成这个任务。 mysql多条数据存入redis 然后对于mysql 的多条数据，我使用的是redis的hmset命令。 HMSET key field value [field value ..] key就是表名称+id，这样查询的时候进行简单处理即可得到对应数据表一条的数据。hmset 表名+id 列名 值 列名 值 … 其实用HSET也是可以的，key就是表名称+id，然后一条数据，可以用String存上，同样简单处理也可得到数据。"},{"title":"pip安装报错 HTTPSConnectionPool：Read timed out.","date":"2021-05-03T05:57:52.000Z","url":"/2021/05/03/pythonQs/","tags":[["python","/tags/python/"],["pip","/tags/pip/"]],"categories":[["python","/categories/python/"]],"content":"pip –default-timeout=1000 install +软件名 e.g. pip –default-timeout=1000 install mxnet-cu90"},{"title":"scheme打开手机内APP","date":"2021-05-03T05:52:43.000Z","url":"/2021/05/03/scheme%E6%89%93%E5%BC%80%E6%89%8B%E6%9C%BA%E5%86%85APP/","tags":[["html5","/tags/html5/"],["android","/tags/android/"]],"categories":[["Android","/categories/Android/"]],"content":"需求：网页上一个链接，点击后会唤醒手机内响应的app，打开指定APP的功能页面。 方法：在AndroidManifest.xml里面对需要打开的页面设置action，两个category, data 一会儿url请求格式是： scheme://host/path?传的参数 对应页面获取数据： kotlin java 本文章参考的博客： 本文也是看这个博客，记录一下。"},{"title":"Studio下载依赖慢","date":"2021-05-03T05:50:17.000Z","url":"/2021/05/03/Studio%E4%B8%8B%E8%BD%BD%E4%BE%9D%E8%B5%96%E6%85%A2/","tags":[["Android","/tags/Android/"],["app","/tags/app/"]],"categories":[["Android","/categories/Android/"]],"content":"在项目下的build.gradle(不是app下的)，把jcenter(),google()改成阿里云的即可 "},{"title":"Kotlin笔记05(空指针检查，小技巧)","date":"2021-05-03T05:45:17.000Z","url":"/2021/05/03/Kotlin%E7%AC%94%E8%AE%B005/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"空指针检查Android系统中，最常见的崩溃问题就是空指针异常，相信其他的系统或者程序，也会常常备受空指针问题的影响。这类问题只能由程序员自己主动通过逻辑判断来避免。可是，很难做到完全考虑到所有潜在的空指针异常。 Kotlin的做法则是将空指针异常检查提前到了编译期，在程序员写程序的时候，时刻能够检测出潜在的空指针问题，及时进行处理。 举例： java代码 这段代码java编译肯定没问题，但是他安全么？不一定。如果这个方法当中传入是一个null，将会导致空指针异常。 修改一下： 这样才会安全。 转到kotlin： 对于kotlin来说，这是没有问题的，没有空指针的风险。因为，在kotlin当中，默认所有参数和变量都不能为空，所以这里传进来的study一定不为空。可以放心的调用它，一旦传入null，在编译的时候直接报错。 可空类型系统 对于业务逻辑需要参数为null的话，kotlin提供的方法是，在参数类名后面加？号，但是函数里参数也是会被检测的，必须要有空的判断。 ?.操作符?.操作符：当对象不为空时正常调用，当为空时什么都不做。 ?:操作符?:操作符：左右两边都接收一个表达式，如果左边结果不为空就返回左边结果，否则返回右边表达式结果 相当于： !!.操作符!!.操作符：非空断言工具，告知kotlin这里的对象不会为空，不用它做非空检查。出错可以抛异常。 这个就和java的那个版本一样了，不安全了，照样运行报错了。 let函数obj对象调用let函数，obj本身会作为参数传递到Lambda表达式中，当列表只有一个参数时用it。 用法： 例子： let通常是和?.操作符结合，之前的study?.doHomework(),study?.readBooks()两个，相当于每次调用方法都需要对study做了if判空处理，相当麻烦。可以先study?. 如果study不为空就运行let，如果为空就什么都不做。此时let里面的lambda表达式当中的it肯定就不是空的了，因此let当中运行是安全的。 let的另一个优势： let函数时可以处理全局变量的判空问题，而if语句是无法做到的 Kotlin的小技巧字符串内嵌表达式Kotlin一开始就支持了字符串内嵌表达式，我们不再需要像java那样傻傻的拼接字符串了。 当表达式中仅有一个变量的时候，大括号可以省略 kotlin的这种方式，不论是易读性易写性方面都比java更胜一筹 函数的参数默认值简单来说就是，kotlin允许我们在定义函数的时候给任意参数设定一个默认值，这样当调用这个函数的时候不强求传此参数。 调用的时候可以 当然默认值也可以给第一个参数 那么填一个参数，就默认到num参数上了。问题不大，kotlin提供了键值对的方式来传参 "},{"title":"Kotlin笔记04(集合,Lambda表达式)","date":"2021-05-03T03:44:54.000Z","url":"/2021/05/03/Kotlin%E7%AC%94%E8%AE%B004/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"Kotlin的集合创建也是相当简单的。集合主要就是List和Set，Map这些的。 集合的创建与遍历List数组java的List 当然kotlin也是可以这么做的 这么多代码，很明显不是kotlin的作风，当然可以进行简化 listOf方法创建的是不可变的集合，需要可变的集合使用mutableListOf() Set集合和List一样 Map集合传统方式，Kotlin不推荐 kotlin推荐的方式 简化方式 这里的to看似是关键字进行关联，实际上to不是关键字，而是infix函数，后面可以自己写类似to的infix函数。也可以自己实现to的方式。 for-in遍历map集合 集合的函数式apiLambda表达式lambda表达式的语法结构： 首先外层大括号，如果有参数传入lambda表达式中，我们需要声明参数列表，参数列表之后 -&gt;符号，表示参数列表的结束以及函数体的开始，函数体可以写任意行代码，并以最后一行代码会作为lambda表达式的返回值。 maxBy函数从一个字符串数组当中找出最长的字符串。 第一时间想到的实现方法是： 如果用上集合的函数式api，可以让这件事变得更加简洁容易 结合maxBy，Lambda定义，还原简化过程 可以不用专门定义lambda Kotlin规定当Lambda参数，(即{fruit:String -&gt; fruit.length})是函数最后一个参数时，可以把Lambda放在括号外面 如果Lambda是函数的唯一一个参数，可以省略括号 利用kotlin的类型推到机制，Lambda中的参数列表，大多数情况不用声明参数类型 当只有一个参数时，可以不写参数名。可以用it关键字代替 map函数map即映射，他可以将每个元素映射成另一个值 filter函数filter函数，就是过滤 any函数any用于判断集合中是否至少存在一个元素满足条件。 all函数all用于判断集合中是否所有元素满足条件 Java函数式API的使用Kotlin中调用一个java方法，并且该方法接受一个java单抽象方法接口函数，就可以用函数式API 单抽象方法接口： 接口中只有一个待实现的方法 比如：Java中的子线程创建 Kotlin当中就是这么写的 由于kotlin舍去了new，因此匿名类实例的时候不能用new了，而是改用object。 进一步精简： 再利用上面的简化方法 其实kotlin提供了一个更简单的方法 这个thread可选参数有很多，可以自己点进去看，这个方法的实现很简单。完全可以不用说，看一眼他的实现就能明白怎么用的。"},{"title":"Kotlin笔记03(数据类，单例类)","date":"2021-05-03T02:04:26.000Z","url":"/2021/05/03/Kotlin%E7%AC%94%E8%AE%B003/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"常见的架构模式有MVP,MVC,MVVM不论是哪个架构，都有M，M就是Model（数据类）。 数据类与单例类数据类通常要写toString(), eauals()等方法。比如java写数据类 tips: java里面的Object ，在kotlin当中是 Any。 instanceof在kotlin中是 is 是不是感觉很长，好的，轮到kotlin了。简单到难以想象 结束了。就只有一句话。kotlin将那些toString，equals等等那些毫无实际逻辑的方法自动生成，大大减少工作量。仅仅需要在class前面加上data即可。’==’相当于equals，’===’就是比较哈希值看是不是同一个对象。 接下来可以测试下 单例模式java常见的单例模式 轮到kotlin，简单到可怕 ok了这个就是单例了，接下来添加方法 Kotlin当中不需要私有化构造函数，不用提供getInstance(),只要把class改为object关键字，单例类就创建完了。 使用： "},{"title":"Kotlin笔记01(变量，函数，逻辑控制)","date":"2021-05-02T05:23:01.000Z","url":"/2021/05/02/Kotlin%E7%AC%94%E8%AE%B001/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"​ 写过Android的童鞋，应该都听说过kotlin，谷歌也是极力推荐使用kotlin来写Android。kotlin有着很多的优点，比如：代码精简，空指针异常判断，协程的优势等等。我是看了郭霖大佬《第一行代码Android》第三版之后，当初笔记是写在代码里的，现在把代码重新梳理一遍，写到博客里，顺道照着书再复习一遍Android和kotlin。虽然很多公司仍然用的java写的Android，但是技多不压身嘛嘻嘻。这个笔记只是记录kotlin相关的学习。 Kotlin简介 kotlin是jetBrains公司开发与设计的，因此现在IntelliJ IDEA本身就加入了kotlin支持。 kotlin和java是可以共存的，像Android当中，可以同时拥有.kt文件也可以有.java文件，为什么呢？ 首先先搞明白java是 编译型语言 还是 解释型语言。虽然java是先编译后运行，但是他并不是编译型语言。因为java编译之后生成的是class文件，class文件则是放入java虚拟机识别，虚拟机担任的是解释器的角色，class文件解释成二进制数据让计算机执行 java虚拟机只是与class文件打交道，它不关心class文件是哪里来的，因此java与kotlin共存的原理也就了解了，不论是java还是kotlin他们编译生成的文件都是class文件。 第一个HelloKotlin 首先安装IDEA，这个直接官网下载就可以了。打开IDEA ，File-&gt;new Project，选择Kotlin，然后next，写项目名称，然后finish就可以啦。在项目目录下，src右击-&gt; New -&gt; Kotlin file/Class，填写文件名字，类型选择File即可。然后输入以上代码，第一个Hello Kotlin就ok了 变量和函数变量kotlin的变量和java有很大的不同，java中定义变量需要具体声明变量类型，而kotlin只有两种val和var。因为kotlin拥有很优秀的类型推导机制。 注意： kotlin代码是不用写分号的，其实写了也没关系，编译器直接无视掉了 kotlin的类型推导也不总是正常工作的，比如某个变量需要延迟赋值的话仍然需要显示声明变量类型才可以 注意：永远优先使用val，val无法满足需求的时候再改var。 在java中除非主动设定final，否则这个变量一直是可变的。当项目变得复杂的时候，永远不知道这个变量什么时候被谁改掉了，容易导致各种问题，kotlin强制要求必须让开发者主动声明变量是否可变。 函数 语法规则： fun 方法名(参数) : 返回值类型{} 参数格式：参数名:参数类型 无返回值就不写返回值类型，不用写return。 语法糖： 这也是我比较喜欢kotlin的一方面，可以少写很多东西(oﾟ▽ﾟ)o ，越往后学，就会发现很多看起来很复杂的代码，改到kotlin当中，轻轻松松没几行就完成了。 逻辑控制if条件语句kotlin的条件语句有两种实现方式 if 和 when kotlin的if和java的基本没区别 Kotlin的if语句相比java还有一个额外的功能，他是可以有返回值的 if语句将每个条件的最后一行代码作为返回值，并将返回值赋值给了value变量。最终将value变量返回。当然以上代码还可以更加精简。 继续简化 是不是觉得kotlin的简化爽的一批。。。 when语句会java的同学，应该了解switch语句吧，非常不好用，只能传整型变量最为条件，一个一个case比对，还必须每个case写break，十分不爽。那么看了kotlin的when语句解决了以上问题 写个例子：输入学生姓名，返回对应的成绩分数 上面使用了if判断学生姓名，返回分数，用了很多if，else，这个时候就可以考虑使用when了 清清爽爽，而且when语句和if一样也是可以有返回值的，因此也可以用语法糖。 when语句可以使用任意类型的参数，包括类型匹配 循环语句"}]