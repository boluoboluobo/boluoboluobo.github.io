[{"title":"模型评估","date":"2021-11-23T08:26:26.000Z","url":"/2021/11/23/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/","tags":[["model evaluation","/tags/model-evaluation/"]],"categories":[["undefined",""]],"content":"模型评估模型评估主要是针对已经训练好的模型，评判模型的泛化能力。对不同的超参设置，得到不同的训练好的模型参数。通过评估模型在未知的数据上性能，得到最佳的模型参数。 AUC&amp;ROCAUC是指ROC曲线下方的区域(the area under ROC curve)，衡量一个模型能够区分类别的概率。 x轴：错误预测/负样本数 （给的负样本，但是预测成了正类） y轴：正确预测/正样本数（给的正样本，预测成了正类） 选择一个θ值，对于预测的概率判断 yhat &gt;= θ？，这样就知道预测正确与否。取不同的θ值就会得到ROC曲线。 Latency模型计算时间。eg.广告应该与其他广告同时呈现给用户 RSNR峰值信噪比RSNR(Peak signal to noise ratio)为峰值信噪比。通常在经过影像压缩之后，输出的影像都会在某种程度与原始影像不同。为了衡量经过处理后的影像品质，我们通常会参考PSNR值来衡量某个处理程序能否令人满意。它是原图像与被处理图像之间的均方误差相对于(2^n-1)^2的对数值(信号最大值的平方，n是每个采样值的比特数)，它的单位是dB。 均方误差$$MSE = \\frac {1}{H \\times W} \\sum^H_{i = 1}\\sum^W_{j = 1}(X(i, j) - Y(i, j))^2$$X为当前图像，Y为参考图像，按照以上公式即可得到均方误差。 PSNR$$PSNR = 10\\log_{10}(\\frac {(2^n - 1)^2}{MSE})$$n为每像素的比特数，一般是8，因为阶数一般是256个(0 ~ 255)，可以看出主要影响PSNR数值的是MSE这个均方误差。PSNR越大表示失真越小，MSE越小，两张图片越靠近。 SSIM结构相似性SSIM（Structural similarity index）基于样本X和Y之间三个比较衡量：亮度 (luminance)、对比度 (contrast) 、结构 (structure)$$l(x,y) = \\frac {2\\mu_x\\mu_y + c_1}{\\mu_x^2+\\mu_y^2+c_1} \\\\c(x,y) = \\frac {2\\sigma_x\\sigma_y + c_2}{\\sigma_x^2 + \\sigma_y^2 + c_2} \\\\s(x,y) = \\frac {\\sigma_{xy} + c_3}{\\sigma_x\\sigma_y + c_3}\\\\\\mu_x为x的均值\\space\\space\\space\\space \\mu_y为y的均值\\\\\\sigma_x^2为x的方差\\space\\space\\space\\space \\sigma_y^2为y的方差\\\\\\sigma_{xy}为x和y的协方差\\\\c_1 = (K_1L)^2\\space\\space\\space\\space c_2 = (K_2L)^2\\space\\space\\space\\space c_3= \\frac {c_2}{2}\\\\K_1 = 0.01\\space\\space\\space\\space K_2 = 0.03$$c1,c2,c3为常数避免除零，k1=0.01, k2 = 0.03为常用取值。ssim公式如下$$SSIM = l(x,y) * c(x,y) * s(x,y)\\\\当c_3 = c_2 / 2时为\\\\SSIM = \\frac {(2\\mu_x\\mu_y + c_1)(2\\sigma_{xy} + c_2)}{(\\mu_x^2+\\mu_y^2+c_1)(\\sigma_x^2 + \\sigma_y^2 + c_2)}$$ssim一般使用一个N*N的窗口，分别求出对应的分数之后做平均值作为全局评分 协方差 有X,Y两个变量，每个时刻的 “X值与其均值之差”乘以 ”Y值与其均值之差“得到一个乘积，然后对每个时刻的成绩求和并求期望。主要目的是判断两个变量在变化过程中是同向变化还是反向变化。同向的话协方差是正的，反向的话协方差是负数，协方差数值越大同向程度越大，反之亦然。公式如下：$$Conv(X,Y) = E[(X - \\mu_x)(Y - \\mu_y)]$$ FID距离得分FID(Fréchet Inception Distance score)，FID计算了真实图像和假图片在特征层面的距离。公式如下：$$FID = ||\\mu_r - \\mu_g||^2 + Tr(\\sum_r + \\sum_g - 2(\\sum_r\\sum_g)^{1/2})$$μ：经验均值 Σ：经验协方差 Tr：矩阵的迹 r：真实数据集 g：生成数据集 对目标数据集的N张图片用InceptionV3生成 N*2048向量，取平均值获得μr 对生成数据集的M张图片用InceptionV3生成M*2048向量，取平均值获得μg 通过μr和μg获得Σr和Σg计算FID 参考材料PSNR百度百科 图像质量评价指标之PSNR和SSIM——知乎 图像质量评估中的PSNR和SSIM的定义，公式和含义 GAN 的六种衡量方法——知乎"},{"title":"白嫖googleGPU资源训练模型","date":"2021-11-15T08:59:14.000Z","url":"/2021/11/15/%E7%99%BD%E5%AB%96googleGPU%E8%B5%84%E6%BA%90%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/","tags":[["python","/tags/python/"]],"categories":[["博文","/categories/%E5%8D%9A%E6%96%87/"]],"content":"白嫖googleGPU资源训练模型最近跑模型，苦于笔记本显卡的显存太小。学校服务器的显存被同学占的蛮多，自己的模型也跑不起来。租服务器又嫌贵，然后就找了一个白嫖GPU的方法。:happy: 利用Google免费GPU跑深度学习模型 方法内容为上方博主的内容。这里只是简单记录一下。 需要翻墙，使用Google的云硬盘。在云硬盘在中新建一个Colaboratory,没有的话在关联更多应用中安装 由于是虚拟环境，当前位置并不是在当前的云盘，所以需要colab去挂载云盘，才可以。 就可以看到当前云盘位置。 如果想用python script.py的方式运行 即可"},{"title":"CGAN学习及实现","date":"2021-11-12T05:57:21.000Z","url":"/2021/11/12/CGAN%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E7%8E%B0/","tags":[["pytorch","/tags/pytorch/"],["cgan","/tags/cgan/"],["conditional GAN","/tags/conditional-GAN/"]],"categories":[["博文","/categories/%E5%8D%9A%E6%96%87/"]],"content":"CGAN学习及实现 在GAN的学习中，生成了随机的二次元人物头像。人物随机生成，随机生成的人物不可控，如何让Generator按照我们想的样子的二次元头像呢。这里使用了CGAN(conditional GAN)条件GAN完成这个任务。 论文地址 效果图这个是跑了150个epoch的效果，训练集和写GAN的那篇用的是一样的。 CGAN思想 这个图网上到处都有，一搜全都是。我也简单说一下吧。CGAN是在原始GAN的基础上，在训练生成器G和判别器D的时候都加入了新的条件变量y，y可以是各种信息，标签等信息。y在训练过程中起到指导作用。 图中的做法是，对于生成器G，将y条件信息，和noise拼接，放入生成器中用来生成图片。对于判别器D，将y条件信息和x图像信息拼接。拼接方式可以是用全连接将类别转为相应大小的图片，拼接到原图片的通道当中，然后继续和GAN的D一样进行评分。这个是网上很多人的做法，我的代码也使用的这个。不过我觉得也可以是，图片经过一个网络生成embedding，label通过一个网络生成一个Embedding然后两个特征相组合在进行评分也可以。 训练的细节训练目标根据输入的头发颜色，眼睛颜色，让Generator生成出对应的二次元头像。 使用的数据集这里用的是和GAN那篇一样的，需要用到tags.csv 最上面的一行是我自己加的。tags格式为：图片号，头发颜色 hair 眼睛颜色 eyes。读取csv使用的是pandas的包，没有的可以自行安装 读csv格式标签的方法。 开始训练 这里是训练判别器D的方式，不同于GAN那样，只判断图像是否为二次元头像。这里需要想到几个问题：这个图片是否清晰，这个label条件是否正确。需要考虑三种情况，也就是三个loss： 第一种，我们先给D一个训练集当中的图片，和对应的label。此时是最好的情况(图片清晰，条件正确)，我们应当给高分，让它靠近1. 第二种，我们给G一个随机noise加上当前的label，让它产生out图片，此时的图片变得不清晰，但是label是正确的。将out图片和label送给D评分，此时分数应当降低，让它靠近0 第三种，我们重新从数据集当中选出和当前label不一样的图片，这时属于是label错误(图片足够清晰，条件错误)，D同样需要给他低分。 这个是两种评分方式，第一种上面的是常用方法，图片和条件分别产生Embedding然后合拼到新的网络中产生分数。第二种是李宏毅老师说的方法，觉得更符合逻辑一些。这个做法是，一个图片经过一个网络，产生一个图像分数（用来判定图像是否清晰）和一个Embedding特征，将特征和条件结合，进入新的网络，产生图像和条件是否满足的分数。这样让网络知道是哪个部分导致的分数降低，从而达到更好的效果。这个我打算去实现以下，做个尝试。 这个是整个训练过程：(c代表条件label，x代表图片，z代表随机噪声) D训练 从数据集选择m个正向样本。 产生m个随机noise样本 获取生成数据，将noise和条件c加入G中产生新的图片 从数据集中获取m个新的图片。 更新判别器参数，下面的公式可以那GAN那篇的方式来看。 G训练 产生m个随机噪声样本noise 从数据集中拿到m个条件c 用noise和c产生图片，放入D中评分，让评分靠近1 代码实现一些参数配置，这里用的label是one-hot形式，2 x 13的大小，总共13种颜色，第一行是头发颜色，第二行是眼睛颜色。 ConditionalGANSet.py Generator Discriminator 训练 相关链接李宏毅GAN视频"},{"title":"Cycle GAN学习及实现","date":"2021-11-09T10:40:33.000Z","url":"/2021/11/09/Cycle-GAN%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E7%8E%B0/","tags":[["pytorch","/tags/pytorch/"],["CycleGan","/tags/CycleGan/"]],"categories":[["博文","/categories/%E5%8D%9A%E6%96%87/"]],"content":"Cycle GAN学习及实现之前看的AnoSeg的那篇论文，其中使用了两个Generator，但并不太清楚另一个Generator的作用。最近找到一个蛮有意思的GAN，就是CycleGAN做的是风格迁移。这个GAN中也使用了两个Generator，想学习一下，从中看看有没有帮助理解的地方。也是挺好玩的一个GAN，下面是我训练100个Epoch出来的效果。 马—&gt;斑马 斑马—-&gt;马 Cycle GAN原理CycleGAN主要是做的在没有成对的图像情况下，弄清楚如何将一个图像的特征转化为另一个图像的特征。假设X是A类型的图，Y是B类型的图。要训练一种Generator，将X的原图生成新的y，让判别器D无法分辨生成的y和Y，认为他们两个是同一类。然而实际操作中发现无法单独优化判别器。然后，从翻译中找到的灵感，假如从英语翻译到法语，再从法语翻译回英语，那么原来的英语应该和翻译回的英语相同。 ​ 就是说CycleGAN当中有两个Generator，一个将X类转为Y类，一个是将Y类转为X类。对应的也有两个Discriminator，一个是认X类的，一个是认Y类的。目标是把X转成Y。 ​ 如果只训练一个Generator直接转换的话存在一个问题 这里生成器生成一张图片，判别器的任务是判别是否是Y图像，生成器的任务是生成图片尽可能的变成Y类型的图。但问题是，从X-&gt;Y有无数种映射可以完成X-&gt;Y。生成器可以完全无视输入条件，直接生成一个与原始图像无关的东西，但也被判别器认为是Y类型的图。因此要求生成器生成的图片不光要欺骗过判别器，同时还要与原图像有一定的关系。 ​ CycleGan的做法： 为了保证生成图像和原始图像有关联，它使用了一个G(x-&gt;y)先将x原图转为fake_y的图，然后用G(y-&gt;x)将生成的fake_y图转回x类型的图，这两张x图应该是尽可能相同，因为应当是同一张x图。对于G(y-&gt;x)也是同理的训练方法。这个模型可以视为两个自动编码器(auto-encoder)。 训练生成器的时候，就和普通的GAN是一样的，用的对抗损失。x -&gt; G(x) -&gt; F(G(x)) ≈ x，两个x应当尽可能相等，使用的是循环一致性损失(cycle-consistency loss) 训练过程训练过程中使用了3个loss，主要是对抗损失(adversarial loss)、循环一致性损失(Cycle consistency loss)、身份损失(identity loss)。其中身份损失可以不加。不使用Identityloss会使得两个生成器自由改变输入图像的色调，但是会一定程度上提高训练速度。开头的我跑的效果图是使用了identity loss的效果。 对抗损失 对于Generator，将D(G(x))的得分尽量趋于1，对于Discriminator将D(x)得分趋于1，D(G(x))尽量趋于0。论文中第4部分Implementation中的训练细节中说道，使用最小二乘损失取代原来的负对数似然损失，使用最小二乘损失效果更好。 循环一致性损失就是前面说过的例子，英语-&gt;法语-&gt;英语的过程。前后英语应当是一样的。应用到图片上，公式如下： 损失函数如下 将原始的x和经过G生成y，y在经过F生成的新的x之间使用L1Loss。然后对y也是这个操作，两个loss求和。 身份损失这个损失主要是解决色调问题。不加影响不大。 就是将生成器生成的图像和原图做一次L1loss，然后相加 完整模型对象 两个生成器的对抗损失和一个循环一致性损失乘上λ加和即为损失 过程参数：学习率为0.0002，batch_size 为1，λ为10 以 斑马 &lt;=&gt; 马 为例。 gen_h为生成马的生成器。gen_z为生成斑马的生成器。 disc_h为判别马的判别器。disc_z为判别斑马的判别器。 先训练两个Discriminator gen_h输入一个zebra图像，生成fake_horse gen_z输入一个horse图像，生成fake_zebra 将两个生成器生成的fake_horse，fake_zebra分别放到disc_h，disc_z产生分数，让分数靠近0. disc_z和disc_h在分别接收之前给生成器的zebra和horse图像，这个是真图，所以训练的时候让分数靠近1。 损失使用mseloss 训练两个Generator 将之前生成的fake_zebra和fake_horse分别给对应的判别器，得到分数，靠近1。 然后计算Cycle consistency loss。用fake_zebra放到gen_h，fake_horse放到gen_z。产生cycle_zebra和cycle_horse，这两个应当和最初始的zebra和horse相同。使用l1loss计算。 计算identity loss 用gen_z喂入zebra产生identity_zebra，gen_h为入horse产生identity_horse，zebra对identity_zebra，horse对identity_horse使用L1loss计算。 最终的G_loss = 两个分数的loss + CycleLoss + identityloss(可以不加这个) 网络结构论文中提供了Pytorch和Torch版本的实现源码 访问慢的可以使用这个链接：Pytorch，Torch 生成器结构对于128 x 128的训练图像使用了6个残差块。对于256 x 256或更高分辨率的图像使用了9个残差块。 说明： c7s1-k：表示 7 x 7的卷积-InstanceNorm-ReLU层，k个滤波器，步长为1. dk：表示3 x 3卷积层-InstanceNorm-ReLU层，k个滤波器，步长为2 Rk：表示一个残差块，这个残差块有两个3 x 3卷积层，两层之间有相同数量的滤波器。 uk：表示一个3 × 3分数步卷积-InstanceNorm-ReLU层，具有k个滤波器和步长 1/2。分数步长卷积也就是反卷积。 判别器结构对于判别器，论文中使用的70 x 70的PatchGAN。 说明： Ck：表示一个4 x 4 卷积-InstanceNorm-LeakyReLU层，有k个滤波器，步长为2。在最后一层，应用了一个卷积去产生一个以为的输出。对于第一层C64层不使用InstanceNorm，LeakyReLU使用的斜率为0.2. 代码实现config.py Discriminator.py Generator.py train.py dataset.py 相关链接论文原文 李宏毅GAN CycleGAN实现"},{"title":"ResNet学习及实现","date":"2021-11-05T05:01:34.000Z","url":"/2021/11/05/ResNet%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E7%8E%B0/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["论文学习","/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"]],"content":"ResNet学习及实现ResNet论文地址 最近想学下Cycle GAN，然而Cycle GAN当中应用到了ResNet相关的知识，查了下ResNet还是蛮重要的，干脆先学了ResNet。本篇记录学习Res残差网络相关的知识，通过网上的视频以及知乎大佬的解释学习的。链接放在后面。 什么是残差ResNet（Residual Network）残差网络，残差指的就是实际值和预测值之间的差距。这个网络的意思就是训练的网络不再拟合原始数据H(x)，而是拟合的是上一层网络和预测值之间的残差。 残差网络解决的什么问题网络退化深度神经网络难以训练，同时深度越深的网络，可以通过堆叠网络层数提取的特征更加丰富。然而网络越深带来了新的问题。更深层次的网络模型效果却不如层次较少的网络。 这个是论文中的图表，左侧是训练，右侧是测试。在CIFAR-10中，56层和20层的简单神经网络比对。很显然，56层的错误率比20层要高，层数越深反而效果更差。这里既不是梯度爆炸/消失，也不是过拟合。而是网络退化。 什么导致的网络退化当堆叠模型时，模型层数越多觉得就会越好，因为当一个浅层网络已经能够达到一个很好的效果时，我们也会认为即便后面更多的层数什么都不做，至少能保证不比现在差。然而问题就出现在什么都不做，神经网络很难做到什么都不做。“什么都不做”的意思简单说就是啥也不干，原样输出，就是恒等映射。ResNet的初衷就是让神经网络拥有恒等映射的能力。 残差网络前面说的ResNet解决的是神经网络没有恒等映射的能力，因此ResNet是由如下的残差块完成的，相当于说上一个模型传进来，通过一个浅层模型加上一个恒等映射求和在输出，这样可以保证至少不比现在差。因为如果上一个模型传下来的效果已经很好了，我完全可以将这层浅模型的参数调为0，完全变成了恒等映射进行输出。 如上图，为一个残差块，右边是short Cut Connection（短连接），也就是恒等映射，就是将上层输出直接和下层输出加和汇总输出。因为这里不再拟合原始函数H(x)，而是拟合的F(x)残差函数，F(x) = H(x) - x，转换后的到H(x) = F(x) + x ,所以在拟合的时候使用的 F(x) + x去拟合H(x)即可，这里要注意一下，中间的浅模型至少是两个，不然会发生很有意思的事，假设中间第一层是W1x，第二层是W2x，正常情况是W2(W1x) + x。如果只有一层的话就变成，W1x + x = (W1 + 1)x，这里W1和W1 + 1没区别了。 网络结构及维度 这个是论文中的34层网络，其中实线部分的shortcut是直接将输入的数据恒等映射加到下层输出中，汇总输出。虚线部分的shortcut都是出现在下采样，提升维度。所有的/2的意思就是图像大小缩小一倍，维度提升一倍。这个是采用和VGG一样的做法。因为中间卷积之后尺寸改变维度提升，无法和输入相加，所以输入需要提升维度。这里有三种方案： A: x维度增加，用0填充空缺维度。 B: x维度增加，用1 * 1的卷积核提升维度。 C： 所有的Short Cut都使用1 * 1卷积 效果是C &gt; B &gt; A，其实影响并不大，我后面代码中使用的是B方案。 这里是残差块的选择，第一种是基础的残差块，通常用于ResNet-18和ResNet-34，第二种通常用于ResNet-50/101/152 各种ResNet网络结构 ResNet-18这里我画一个详细的ResNet-18网络结构，方便后面代码实现。其中每个ResBlock中第一层conv后需要做一次BN，然后ReLU激活，进入第二层，第二层之后直接和输入相加，之后再激活。 代码实现ResNet代码部分首先是先写一个残差块 这个就按照图画的来就行。每个残差块对输入输出通道进行判断，如果通道数变了，提升维度了，就修改对应的输入值的维度，进行汇总。 接下来是ResNet 最后的sigmoid可有可无，因为总是取最大概率的类别作为最终结果。而概率就是由这个全连接层输出值大小决定的。 实战训练猫狗识别这里用的猫狗识别数据集是kaggle注册及下载里面讲的数据集。使用了2万张猫狗图片，测试集是2千张。 代码： 经过25个epoch，在测试集上表现正确率达到90%以上。 测试训练结果网上随机下载几张图片测试。 测试代码： 结果： 相关链接对于论文的中文讲解 知乎大佬"},{"title":"pytorch常用方法记录","date":"2021-11-03T07:19:07.000Z","url":"/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"pytorch常用方法记录 本篇记录在学习使用pytorch时，常用的东西记录，以便随时查看。遇到新学到的会更新在这里。 常用损失函数交叉熵CrossEntropyLoss计算公式：这个公式在深度学习入门04中讲过。 python代码： 注意：CrossEntropyLoss的输入就直接是Model的输出，不要是softmax之后的。还有就是label必须是long类型在[0, class-1]之间。 查看源码中的解释即可知道，CrossEntropyLoss = log_softmax + NllLoss，因此不需要softmax。其中的C代表的类型。 均方差MSELoss (L2Loss)计算公式： BCELossbce loss分类，用于二分类问题。数学公式如下pytorch中bceloss weight: 初始化权重矩阵 size_average: 默认是True，对loss求平均数 reduction: 默认求和， 对于batch_size的loss平均数 L1Loss (MAE)指的是一范数损失，也叫MAE，就是估计值和目标值做差的绝对值 公式： 常用激活函数Sigmoid函数公式： 代码： ReLu函数ReLU函数：在输入大于0时，直接输出该值；在小于等于0时为0 公式： 代码： inplace若为True，则覆盖之前的值，可以减少内存消耗。 LeakyReLU虽然relu在SGD中收敛很快，但是对于小于0的值梯度永远是0，那么激活函数收到的值通常都会有bias，如果偏置很小，导致值一直都是负数，使得梯度无法更新，最终导致神经元“死亡”。所以提出了LeakyReLU，当x小于0时不再等于0而是等于一个weight * x。通常这个权重取0.2代码： 各种层二元自适应均值汇聚层在实现ResNet的时候，看了下pytorch里面的ResNet源码，看到平均池化层使用的是**nn.AdaptiveAvgPool2d((1, 1))**，就觉的好奇查了下。 其实就是平均池化层，好处就是不需要我们自己算平均池化的步长啊，核大小啊，填充啊这些，只用传入一个我想要的大小，其他都是去自适应，还是蛮舒服的哈:happy: 常用方法torch.squeeze()对数据的维度进行压缩，去掉某行或某列。只能对于为1的维度。比如： out: torch.Size([2, 3, 1])torch.Size([2, 3]) torch.unsqueeze()将数据维数进行扩充。比如： out： torch.Size([2, 3, 3])torch.Size([2, 3, 1, 3]) torch.empty()之前pytorch入门笔记01讲过，这里将如何赋值 data找到对应的值，然后copy进去即可。 torch.split()数据分割使用的 四种padding模式四个模式为Reflect，Zero，Replication，Constant。注意padding的大小不能超过matrix的大小。这部分的四个模式也是对应Convolution中的padding_mode设置。该设置有四种模式（’zeros’, ‘reflect’, ‘replicate’, ‘circular’） 一下例子以3 x 3大小的矩阵举例。这部分是pytorch源码中用的例子。 out： tensor([[[[0., 1., 2.], [3., 4., 5.], [6., 7., 8.]]]]) 注意：还有这里就是所有的Pad参数都是填padding大小，ConstantPad当中需要在填入一个value。padding的参数如果是int就是周围都填充这么多。如果是一个有4个参数的tuple，表示的是左，右，上，下的顺序依次填充数量。 nn.ReflectionPad2d字面意思镜像复制，就是以当前位置作为对称轴，将后面的值按照镜像翻转一份出来。 nn.ZeroPad2d这个是使用0进行填充 nn.ReplicationPad2d复制填充，将周围的值复制一份填充到边框。 nn.ConstantPad2d设定固定值作为填充 自动混合精度(torch.cuda.amp)作用：减小显存占用，加快训练速度。 适用设备：支持Tensor Core的CUDA设备。 使用方式：torch.cuda.amp.autocast和torch.cuda.amp.GradScaler结合适用 混合精度：表示不止一种精度的Tensor，在AMP中有两种：FloatTensor，HalfTensor 自动：框架可以按需自动调整Tensor的dtype 为什么使用自动混合梯度torch.HalfTensor优势： 存储小、计算快、更好利用Tensor Core，减少显存占用意味着可以多添加batchsize，训练加快。 torch.HalfTensor劣势： 数值范围小容易Overflow、舍入误差。 解决方案 梯度Scale，放大loss的值防止梯度underflow。 回落到torch.FloatTensor，至于什么时候回落这个由框架自行决定 使用方法 这里不需要在input上手动调用.half(),这个是框架自动去做的。 图片存储 或 "},{"title":"GAN生成动漫头像","date":"2021-10-29T01:12:07.000Z","url":"/2021/10/29/GAN%E7%94%9F%E6%88%90%E5%8A%A8%E6%BC%AB%E5%A4%B4%E5%83%8F/","tags":[["pytorch","/tags/pytorch/"],["gan","/tags/gan/"]],"categories":[["博文","/categories/%E5%8D%9A%E6%96%87/"]],"content":"GAN生成动漫头像这篇文章主要是听的李宏毅老师的GAN课程，结合了一些《深度学习框架pytorch入门与实践》中的代码实现的。 GAN原理简介GAN（Generative adversarial Networks）生成对抗网络，GAN解决了一个著名问题：给定一批样本，训练一个系统可以生成类似的新样本。生成对抗网络，顾名思义，有两个部分一个是生成器(Generator)，一个判别器（Discriminator），两者相互对抗，左右互博。 生成器（Generator）：输入一个随机噪声，生成一张图片 判别器（Discriminator）：判别图像是真图片还是假图片 如图展示了基本训练过程，第一代的生成器最开始由于是随机初始化，传入一个随机噪点，产生的图片是模糊一片，第一代判别器做的事情就是，能够辨别图片是第一代的生成器产生的图片，还是真实图片。可能第一代判别器认为有颜色的是真图，而生成器要骗过判别器，所以要进化，进化成了第二代，产生有颜色的图片。判别器也随之进化，找出有嘴巴的是真图，辨别出了生成图和真图。那么生成器也进化到第三代，产生了嘴巴的图片，骗过第二代的判别器，判别器又进化成第三代。一步一步对抗，相互进化，最终生成二次元头像。 算法过程训练Discriminator首先初始化生成器和判别器，在每次训练迭代中，先固定住生成器，传入随机噪点给生成器，生成对应的图片。前面说了判别器的任务是判别真假图片，判别器主要是给图片打分，分数越高认为真图片的概率越高，分数越低认为是假图片概率越高。所以训练判别器时，收到Database中的图和生成器给的图，调整参数，如果是Database的图就给高分，如果是生成器给的图就给低分。换句话说，我们训练处的判别器就是要数据集给的图越接近1越好，生成器给的图越接近0越好。 训练Generator前面训练好了判别器，现在训练生成器，生成器需要做的事情是，收到随机噪点，产生一张图片，要“骗”过判别器。如何“骗”过生成器呢。实际上的做法是，将生成的图片，送到判别器中进行评分，目标是让评分越高越好（越接近1越好）。整个部分可以看做整体，就是一个巨大的hidden layer，其中有Generator和Discriminator，输入噪点，生成评分。调整参数让评分不断靠近1，但是中间部分Discriminator不能调整。 整个过程 从数据集中选出m个样本{x1,x2….xm}；接着创建m个噪声，这个z的维度由自己决定，这个就是后面生成器输入的噪声；获取生成数据，就是G(z)生成器生成的；更新判别器参数，使得下面的公式最大化，下边公式的意思是：D(x)判别器判别真图片的分数去log平均值 加上 1 - 判别器判别假图片的值 的log平均值。简单说就是要判别真图片的分数越大越好，判别假图片时，假图片的评分距离1的值越远越好。（相当于训练二分类器，用bceloss） m个噪点z，更新生成器，生成器根据z产生的图片，喂给判别器，产生的分数越高越好。 BCE Lossbce loss分类，用于二分类问题。数学公式如下pytorch中bceloss weight: 初始化权重矩阵 size_average: 默认是True，对loss求平均数 reduction: 默认求和， 对于batch_size的loss平均数 代码实现前面的理论了解之后，可以着手实现了我这里用的数据集是Extra Data，也可以用Anime Dataset尝试。请自行找梯子。 初始化 生成器 判别器 优化器 损失函数 开始训练 结果这个是1个Epoch的效果 5个Epoch 10个Epoch 25个Epoch "},{"title":"力扣每日一题 2021/10/26","date":"2021-10-26T01:39:42.000Z","url":"/2021/10/26/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211026/","tags":[["leetcode(简单)","/tags/leetcode-%E7%AE%80%E5%8D%95/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"496.下一个更大元素I给你两个 没有重复元素 的数组 nums1 和 nums2 ，其中nums1 是 nums2 的子集。 请你找出 nums1 中每个元素在 nums2 中的下一个比其大的值。 nums1 中数字 x 的下一个更大元素是指 x 在 nums2 中对应位置的右边的第一个比 x 大的元素。如果不存在，对应位置输出 -1 。 示例 1： 示例 2： 提示： 1 &lt;= nums1.length &lt;= nums2.length &lt;= 1000 0 &lt;= nums1[i], nums2[i] &lt;= 104 nums1和nums2中所有整数 互不相同 nums1 中的所有整数同样出现在 nums2 中 原文链接 解题思路1.暴力解法 遍历nums1，每次都在nums2中找到对应元素，寻找到后面一个更大的元素，填入数组中，没有就填-1 2.栈和Map 将nums2的元素依次入栈，如果小于栈顶元素则入栈，如果大于栈顶则出栈，并写道map当中。最后遍历一次nums1从map中寻找对应的值，如果没有则是-1即可 解题代码"},{"title":"力扣每日一题 2021/10/25","date":"2021-10-25T01:57:06.000Z","url":"/2021/10/25/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211025/","tags":[["leetcode(中等)","/tags/leetcode-%E4%B8%AD%E7%AD%89/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"240.搜索二维矩阵II编写一个高效的算法来搜索m x n矩阵matrix中的一个目标值target。该矩阵具有以下特性： 每行的元素从左到右升序排列。 每列的元素从上到下升序排列。 示例 1： 示例 2： 提示： m == matrix.length n == matrix[i].length 1 &lt;= n, m &lt;= 300 -109 &lt;= matrix[i][j] &lt;= 109 每行的所有元素从左到右升序排列 每列的所有元素从上到下升序排列 -109 &lt;= target &lt;= 109 原文链接 解题思路遍历每一行的最大值，也就是最右边的一列，如果这一行最大的值都比target小，那说明不存在，直接跳到下一行，直到最大值大于target，然后继续从右往左找到target，如果没有说明没有该值。 解题代码"},{"title":"力扣每日一题 2021/10/24","date":"2021-10-24T02:35:44.000Z","url":"/2021/10/24/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211024/","tags":[["leetcode(中等)","/tags/leetcode-%E4%B8%AD%E7%AD%89/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"638.大礼包在 LeetCode 商店中， 有n件在售的物品。每件物品都有对应的价格。然而，也有一些大礼包，每个大礼包以优惠的价格捆绑销售一组物品。 给你一个整数数组 price 表示物品价格，其中 price[i] 是第 i 件物品的价格。另有一个整数数组 needs 表示购物清单，其中 needs[i] 是需要购买第 i 件物品的数量。 还有一个数组 special 表示大礼包，special[i] 的长度为n + 1，其中special[i][j]表示第i个大礼包中内含第 j 件物品的数量，且 special[i][n] （也就是数组中的最后一个整数）为第i个大礼包的价格。 返回 确切 满足购物清单所需花费的最低价格，你可以充分利用大礼包的优惠活动。你不能购买超出购物清单指定数量的物品，即使那样会降低整体价格。任意大礼包可无限次购买。 示例 1： 示例 2： 提示： n == price.length n == needs.length 1 &lt;= n &lt;= 6 0 &lt;= price[i] &lt;= 10 0 &lt;= needs[i] &lt;= 10 1 &lt;= special.length &lt;= 100 special[i].length == n + 1 0 &lt;= special[i][j] &lt;= 50 原文链接 解题思路最开始大体思路就是递归，每次调用获得当前情况的最优解。递归到无法使用礼包，将当前剩下部分物品所需的价格进行回溯。每次方法内部检查可以使用的礼包，避免无效递归。接着就是对比使用礼包和不使用礼包以及不同礼包之间最好的方案返回。最终得到最优解。 解题代码"},{"title":"力扣每日一题 2021/10/23","date":"2021-10-23T02:21:11.000Z","url":"/2021/10/23/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211023/","tags":[["leetcode(简单)","/tags/leetcode-%E7%AE%80%E5%8D%95/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"492.构造矩形作为一位web开发者， 懂得怎样去规划一个页面的尺寸是很重要的。 现给定一个具体的矩形页面面积，你的任务是设计一个长度为 L 和宽度为 W 且满足以下要求的矩形的页面。要求： 你需要按顺序输出你设计的页面的长度 L 和宽度 W。 示例 1： 提示： 给定的面积不大于 10,000,000 且为正整数。 你设计的页面的长度和宽度必须都是正整数。 原文链接 解题思路由于需要L和W差距尽可能小，所以最小的情况就是两个相等，应当开根即可找到，由于要求是整数，所以取整之后，若是无法整除，可以依次向下减找到最小差距的两个值即可。 解题代码"},{"title":"力扣每日一题 2021/10/22","date":"2021-10-22T02:45:50.000Z","url":"/2021/10/22/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211022/","tags":[["leetcode(中等)","/tags/leetcode-%E4%B8%AD%E7%AD%89/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"229.求众数 II给定一个大小为 n 的整数数组，找出其中所有出现超过 ⌊ n/3 ⌋ 次的元素。 示例 1： 示例 2： 示例3 提示： 1 &lt;= nums.length &lt;= 5 * 104 -109 &lt;= nums[i] &lt;= 109 进阶：尝试设计时间复杂度为 O(n)、空间复杂度为 O(1)的算法解决此问题。 原文链接 解题思路解法一 将数组排序，分别计算每个数字的数量，将符合要求的记录并返回 解法二 【笔记】摩尔投票法。该算法用于1/2情况，它说：“在任何数组中，出现次数大于该数组长度一半的值只能有一个。” 那么，改进一下用于1/3。可以着么说：“在任何数组中，出现次数大于该数组长度1/3的值最多只有两个。” 于是，需要定义两个变量。空间复杂度为O(1)。（摘自LeetCode评论区） 解题代码"},{"title":"力扣每日一题 2021/10/21","date":"2021-10-21T02:36:52.000Z","url":"/2021/10/21/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211021/","tags":[["leetcode(简单)","/tags/leetcode-%E7%AE%80%E5%8D%95/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"66. 加一给定一个由 整数 组成的 非空 数组所表示的非负整数，在该数的基础上加一。 最高位数字存放在数组的首位， 数组中每个元素只存储单个数字。 你可以假设除了整数 0 之外，这个整数不会以零开头。 示例 1： 示例 2： 示例3 提示： 1 &lt;= digits.length &lt;= 100 0 &lt;= digits[i] &lt;= 9 原文链接 解题思路这个题就是简单的大数计算常见操作，手动实现进位问题。题中说整数不能以0开头，因此只要第一位为0直接返回即可。后边就是从后往前计算加1的情况，如果进位则前一位加1，继续判断是否进位，以此类推。直到最前位置如果需要进位，数组长度短了，所以需要重新创建一个原始长度+1的数组添加。 解题代码"},{"title":"ANOSEG:用自监督学习的异常分割网络（论文学习）","date":"2021-10-21T02:26:37.000Z","url":"/2021/10/21/ANOSEG-%E7%94%A8%E8%87%AA%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%BC%82%E5%B8%B8%E5%88%86%E5%89%B2%E7%BD%91%E7%BB%9C%EF%BC%88%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%EF%BC%89/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["论文学习","/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"]],"content":"ANOSEG:用自监督学习的异常分割网络（论文学习）这是自己读的第一篇英文论文，在其中发现自己有很多不会的知识点和术语之类的，现在还没完全读懂，目前记录学习到的知识补充，后续会继续补充。论文地址 知识点补充IoUIoU：（intersection over union）交并比，产生的候选框(Candidate bound)和原标记框(Ground Truth bound)的交叠率，就是交集和并集的比值 GTGT: (Ground Truth)人工标注的正确label，训练集认为100%正确的标签。 SOTASOTA：(state of the art)技术发展最新水平。比如 SOTA model表示在该项研究任务中，目前最好/最先进的模型。 CoordConv论文地址 在原始的卷积层上面，添加两个层，一个是x坐标的层，一个是y层的坐标。给卷积添加了空间感知能力，在回归问题和分类问题上表现都比卷积效果更好。 python实现 简单了解，代码转自 这个论文还没有实现成功，这里只讲一下论文中的一些翻译以及我理解到的一些吧。等实现成功后会分享出代码以及最终跑出来的结果。 应用简介异常分割是大规模工业制造中的一个重要部分，用来定位缺陷区域。该论文提出了一种新的异常分割网络能够通过自监督学习直接生成精确的异常图。 方法提出AnoSegAnoSeg是一个“整体”的方法，包括了三个技术：用硬增强自监督学习、对抗学习和坐标通道连接 用硬增强自监督学习为了直接训练异常分割，需要一张有异常区域的图像和对应的GT mask。然而，在现实情况下很难获取到这些图片和GT mask。因此，提出使用硬增强的方法人工生成异常数据和GT Mask。硬增强指的是生成样本偏离原样本的分布。硬增强样本可以被用于作为负样本。因此正如图3，我们用硬增强的三种类型：旋转，置换，和颜色抖动。每个增强有50%被应用。接下来，将增强数据粘贴到正常数据的随机区域，生成合成异常数据和对应的分割mask。最终，异常分割数据组成如下：Xseg是正常和生成异常图像的集合，Xnor和Xano对应的正常图像和合成异常图像。Aseg是正常和合成异常mask， 其中Anor和Aano分别对应正常mask里面的值全都是1和合成异常mask。 使用具有像素级损失的异常分割数据集，我们能够直接训练我们的AnoSeg。 异常分割损失Lseg如下：其中Âseg表示的是生成异常图（正常和异常类型）。生成异常图有着和输入图像一样的大小，根据输入图像像素的重要性，对每个像素输出一个范围为[0,1]的值。然而，由于合成的异常数据只是各种异常数据的子集，很难生成在训练阶段看不到的真实异常图。 对抗学习和重建为了提高对各种异常数据的通用性，训练正常区域分布准确性是很重要的。因此，AnoSeg利用只在正常区域使用重构损失的掩码重构损失，只学习正常区域的分布，避免了合成异常区域分布的偏差。此外，由于鉴别器为输入图像及其GT mask输入一对，判别器和发生器可以聚焦于正常区域分布。这样不仅不能很好地重建异常区域，而且还可以改善异常图的细节。对抗性学习的损失函数如下 D,G和concat分别是 判别器(discriminator)，生成器(Generator)，和一个拼接操作。 坐标通道连接在传统的分割任务重，位置信息是最重要的信息。因为正常区域和异常区域会根据它们所处的位置发生变化。为了提供额外的位置信息，我们使用了受CoordConv启发的坐标向量。我们首先生成秩为1的矩阵，并归一化为[-1,1]。然后，我们将这些矩阵与输入图像作为通道连接(图4)。在消融实验中，我们证明了通道坐标连接的有效性。 使用提出的异常图进行异常检测 在本节中，我们设计了一个简单的异常检测器，将所提出的异常图添加到现有的基于gan的检测方法中。提出的异常检测器只通过学习正常数据分布来进行异常检测。我们简单地将输入图像和异常图连接起来，将它们作为检测器的输入，并同时应用对抗损失和经济结构损失。然后，我们使用(Salimans et al.(2016))引入的特征匹配损失来稳定鉴别器的学习并提取异常分数。我们在附录a中详细描述了异常检测的训练过程。在测试过程中(图5)，所提出的异常检测器使用已学习到正常数据分布的判别器获得异常分数。我们首先假设输入的图像是正常的，因此所有内部值设置为1的mask ANor与输入图像成对使用。当输入图像为真正常时，假对(异常映射和重建图像)与真对(正常掩模和输入图像)相似，因此异常检测器的异常评分较低。另一方面，当输入图像异常时，假图像对与真实图像对差异显著，因此具有较高的异常评分。为了比较真实对和虚假对，使用重构损失和特征匹配损失如下: 其中α和β的值分别是1和0.1. Anor和Lmse表示正常的GT mask 和均方差。 f(.)指的是鉴别器的倒数第二层。 训练方法以及网络结构以上方法提出部分是对论文第3部分的翻译，讲的是三个技术的使用，以及最后对于异常图检测评分的方法。下面讲的是具体训练的方法，以及网络结构。也就是论文中的附录a部分以及附录b 异常检测方法的训练过程本文提出的异常检测方法是将由anseg生成的异常图与输入图像结合，以了解正常图像和异常图的分布情况。因此，异常检测器在判断输入图像是否为正常图像的同时，判断异常图是否聚焦于输入图像的正常区域。与AnoSeg不同，提出的异常检测方法没有使用合成异常Xano作为一个对抗损失的真实类，因为异常检测的判别器对于异常检测只需要去学习正常数据分布。异常检测的判别器学习的损失函数如下： 其中x^ nor，Ânor，xnor和Anor分别表示重构的正常图，AnoSeg的异常图，正常图，和正常mask 此外，为了帮助评估正常数据的分布，我们提出了一个区分合成数据和正常数据的合成异常分类损失。如(Odena(2016))所证实的，所提出的综合异常分类损失提高了鉴别器的异常性能。这种合成异常分类损失定义然后，我们使用(Salimans et al.(2016))引入的特征匹配损失来稳定鉴别器的学习，提取异常评分。正常样本和重建样本的高级表示应该是相同的。这个损失如下所示$$L_{fea}=E||f(concat(x_{Nor}, A_{Nor})) - f(concat(\\hat{x}{Nor},\\hat{A}{Nor}))||^2$$ 这里我在讲一下上面的图9，图9是训练异常检测的训练过程。首先输入一个正常图，接着进入了Generator，我觉得这个生成器就是图2的生成器。重构了一张异常图，将异常图和原图组合放入新的Generator，这个Generator的编码器是Discriminator一样的结构，解码出来一张重构的图像。这块重构的图像我没有去实现，直接用的AnoSeg中生成的图像。会不会是导致没有实现出来的原因。将重构的图像和重构的异常图连接，形成Fake，和实际上的真实组合Real(xnor,Anor)分别放入Discriminator，得到两个分数，Fake组合的分数要不断靠近0，Real组合分数不断靠近1。至于论文里面说的Loss feature该如何使用还不太清楚。 在说下图2吧，我觉的图2主要是讲的AnoSeg Generator训练过程。用来重构图像和重构异常图。先将图像进行硬增强，然后将图像添加两个通道，存放x坐标矩阵，y坐标矩阵。相当于一个5通道的图传入Encoder，Encoder最终生成8 * 8 * 512的一个code。有两个decoder，一个用来重构image，一个用来重构异常图的。训练Generator的时候，对于重构image的decoder使用的是对抗学习和重建里面的Lre重构loss，这个里面有个操作就是将异常mask和原图相乘，想一想之前说过异常图的范围是[0,1]之间。我们的GTmask，异常部分是0显示黑色，正常部分是1是白色，这样完成了图2中的mask area的遮挡。挡住异常部分，观察正常部分。对于重构。对于重构异常图（anomaly mask）的decoder使用的是在硬增强自监督学习中讲过，使用Lseg直接训练我们的AnoSeg。至于对抗学习重建当中讲的另一个公式，这个和GAN中讲的一样。在训练中貌似应该加上一个Generator生成图像给Discriminator得到评分，然后修改Generator的参数将评分不断逼近1。我实现的时候没有加上这个，可能是失败的原因之一吧。 详细的网络架构提出的方法网络结构如表6所示，每个网络是有一系列层描述，包括了输出形状，核大小，填充尺寸，步长。此外batch normalization（BN）和activation function分别是否应用BN和应用那个激活函数。用于图像重建的解码器与生成异常图的解码器结构相同，而AnoSeg使用了两个解码器。提出的异常判别结构也是和AnoSeg是相同的结构。 "},{"title":"力扣每日一题 2021/10/20","date":"2021-10-20T02:36:52.000Z","url":"/2021/10/20/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211020/","tags":[["leetcode(简单)","/tags/leetcode-%E7%AE%80%E5%8D%95/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"453.最小操作次数使数组元素相等给你一个长度为 n 的整数数组，每次操作将会使 n - 1 个元素增加 1 。返回让数组所有元素相等的最小操作次数。 示例 1： 示例 2： 提示： n == nums.length 1 &lt;= nums.length &lt;= 105 -109 &lt;= nums[i] &lt;= 109 答案保证符合 32-bit 整数 原文链接 解题思路假设总和为sum，执行操作次数为m，数组长度为n，最终的每个元素的值为x，可以得到公式:$$sum + m*(n - 1) = n * x$$最终的操作执行次数的多少，取决于最小的那个值，设定最小值为min_val 那么可以得到另一个式子m = x - min_val, 转为 x = m + min_val带入上式可以得到。m = sum - n * min_val 最终返回即可。 解题代码"},{"title":"力扣每日一题 2021/10/19","date":"2021-10-19T02:49:52.000Z","url":"/2021/10/19/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211019/","tags":[["leetcode(中等)","/tags/leetcode-%E4%B8%AD%E7%AD%89/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"211.添加与搜索单词-数据结构设计请你设计一个数据结构，支持 添加新单词 和 查找字符串是否与任何先前添加的字符串匹配 。 实现词典类 WordDictionary ： WordDictionary() 初始化词典对象 void addWord(word) 将 word 添加到数据结构中，之后可以对它进行匹配 bool search(word) 如果数据结构中存在字符串与 word 匹配，则返回 true ；否则，返回 false 。word 中可能包含一些 ‘.’ ，每个 . 都可以表示任何一个字母。 原文链接 示例： 提示： 1 &lt;= word.length &lt;= 500 addWord 中的 word 由小写英文字母组成 search 中的 word 由 ‘.’ 或小写英文字母组成 最多调用 50000 次 addWord 和 search 解题思路整个字典是一个Map，map存储一个String的列表，以word的长度为key，当查找的时候，通过长度找出列表，列表中遍历查找对应的单词。 解题代码"},{"title":"力扣每日一题 2021/10/18","date":"2021-10-18T02:49:52.000Z","url":"/2021/10/18/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211018/","tags":[["leetcode(中等)","/tags/leetcode-%E4%B8%AD%E7%AD%89/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"90. 子集II给你一个整数数组 nums ，其中可能包含重复元素，请你返回该数组所有可能的子集（幂集）。 解集 不能 包含重复的子集。返回的解集中，子集可以按 任意顺序 排列。 示例 1： 示例 2： 提示： 1 &lt;= nums.length &lt;= 10 -10 &lt;= nums[i] &lt;= 10 原文链接 解题思路我的方法比较暴力。第一层循环，用来设定窗口大小，内部有一个方法用来递归将所有的组合遍历，对于重复数组，使用的集合将数组排序后存起来。最终集合转数组输出 解题代码"},{"title":"力扣每日一题 2021/10/17","date":"2021-10-17T02:49:52.000Z","url":"/2021/10/17/%E5%8A%9B%E6%89%A3%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%9820211017/","tags":[["leetcode(中等)","/tags/leetcode-%E4%B8%AD%E7%AD%89/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"1104. 二叉树寻路在一棵无限的二叉树上，每个节点都有两个子节点，树中的节点 逐行 依次按 “之” 字形进行标记。 如下图所示，在奇数行（即，第一行、第三行、第五行……）中，按从左到右的顺序进行标记； 而偶数行（即，第二行、第四行、第六行……）中，按从右到左的顺序进行标记。 原文地址 给你树上某一个节点的标号 label，请你返回从根节点到该标号为 label 节点的路径，该路径是由途经的节点标号所组成的。 示例 1： 示例 2： 提示： 1 &lt;= label &lt;= 10^6 解题思路利用公式，计算高度。$$h = \\lceil log_m(n(m - 1) + 1) \\rceil$$其中 m: m叉树 n: n个节点 二叉树当中，每个节点整除之后就是父亲节点。奇数层是顺序，偶数层是逆序，我的做法是，假设都是顺序，那么我可以找到一个顺序形式下的路径，通过2^(floor - 1)和2^(floor) - 1，可以知道当前层最大最小值，根据算出的父亲节点值，知道相对位置，在根据奇偶层，相应更改对应位置的值是顺序的第几个或者逆序的第几个，但是我计算用的值不变。然后数组就用前插就能完成根节点到对应label位置 解题代码"},{"title":"pytorch入门笔记06","date":"2021-10-14T03:17:36.000Z","url":"/2021/10/14/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B006/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"TensorBoard可视化模型，数据，和训练这里用FashionMnist数据集作为例子。 这块没什么难度，和前面加载数据是一样的，不懂得话见前面的笔记。 创建一个简单的网络模型 这块和前面笔记一样的，只不过注意下这里Mnist数据集是单通道的，28x28的格式，相应的地方需要改动。 定义优化器和损失函数 设置TensorBoard这里多说一句没有tensorboard的先装上 从torch.utils中导入tensorboard，定义一个SummaryWriter。 向Tensorboard写入写入一个图片到Tensorboard，使用make_grid生成的网格（多个图片布局到一张上） 这里一定记得要close掉，不然tensorboard报错。后面的例子都是用完writer之后要关闭。 跑命令 logdir后面就是log文件位置。跑完之后会在控制台显示访问那个网址。如图 点击即可访问。 用Tensorboard查看模型TensorBoard的强大之一是他能够将复杂的模型结构可视化，让我们看下我们做的模型。 刷新Tensorboard之后你能看见一个“Graph”标签。 双击“Net”后，能看见网络被展开，查看组成模型的各个操作的详细视图。TensorBoard有一个非常方便的功能来可视化高维数据，如低维空间中的图像数据;下面我们将讨论这个问题。 添加一个“投影器”到TensorBoard我们可以通过add_embedding方法可视化高维数据的低维表示 现在在tensorBoard的“projector”标签中，你能看到这100张图片（每张都是784维度）投影到三维空间。此外，这是交互式的：你能够点击和拖拽去旋转这三维投影。最终，一些技巧可以使得可视化更加容易看：在左上角选择“color:label”，也可以打开“night mode”，这会让图像更容易查看因为他们的背景是白色的： 用TensorBoard跟踪模型训练在前面的例子里，我们简单每2000此迭代打印模型跑的误差。现在，我们将把运行损失记录到TensorBoard，并查看模型通过plot_classes_preds函数做出的预测。 最后，我们用之前教程相同的训练模型代码训练，但是每1000批次写入结果到Tensor board，而不是打印到控制台；用add_scalar方法完成。 此外，在训练时，我们将生成一张图像，显示模型的预测与该批处理中包含的4张图像的实际结果。 你可以在Scalar标签出查看15000次迭代训练中的运行误差 用TensorBoard评估训练模型 你会看到一个“PR Curves”标签，包含了每个类的PR曲线。到处看看看，你会看到一些类型的模型几乎100%在“曲线下方区域”，然而其他类型中要更低。 "},{"title":"pytorch入门笔记05","date":"2021-10-14T01:32:18.000Z","url":"/2021/10/14/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B005/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"pytorch实现CNN这部分是根据深度学习入门里面的简单的卷积网络CNN写的。写了之后会发现pytorch真香 需要注意的点：就是官网说过的，想用GPU跑的话，需要将网络放到GPU中，还有数据也要放到GPU中。就是训练时候的inputs和labels。否则会报错，网络是GPU但是给的数据是CPU Tensor，是常见错误。 下面是对比CPU和GPU训练MNIST训练集速度。 CPU耗时 GPU耗时 测试训练结果用的Opencv对自己画的图像进行处理 效果： "},{"title":"pytorch入门笔记04","date":"2021-10-12T01:40:10.000Z","url":"/2021/10/12/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"训练一个分类器在这里，你已经了解了如何定义神经网络，计算损失和更新网络的权重。现在你可能会想 数据是什么？总的来说，当你解决图像，文字，音频或视频数据，你能用标准的python包加载数据到numpy数组。然后你可以将这个数组转为torch.*Tensor. 对于图像，可以用Pillow，OpenCV 对于音频，可以用scipy和librosa 对于文字，用要么是raw Python 或者 Cython 或NLTK和SpaCy 尤其是视觉，我们创建了一个包名为 torchvision, 它有公共数据集的数据加载器，比如，ImageNet, CIRFAR10, MNIST等等。图像转数据。torchvision.datasets 和 torch.utils.data.DataLoader. 这提供了极大的便利，并避免了编写样板代码。 对于这个教程，我们用CIFAR10数据集。他有的分类：‘airplane’, ‘automobile’, ‘bird’, ‘cat’, ‘deer’, ‘dog’, ‘frog’, ‘horse’, ‘ship’, ‘truck’.CIFAR10的图像是3 * 32 * 32 训练一个图像分类器我们会做如下几步: 使用torchvision加载和标准化CIFAR10训练和测试数据集 定义一个卷积神经网络 定义一个损失函数 在训练集上训练网络 在测试集测试网络 加载和标准化CIFAR10使用torchvision，加载CIFAR10是非常简单的 torchvision的输出数据集是在[0,1]范围的PILImage。我们把他们转换为Tensors的标准范围[-1,1]. 注意：如果在windows平台，有BrokenPipeError，设定torch.utils.data.DataLoader()中的num_worker为0 out: Downloading  to ./cifar10\\cifar-10-python.tar.gz170500096it [02:27, 1156941.89it/s] 展示一些训练的图 out: Files already downloaded and verified dog ship plane ship 定义一个卷积神经网络从神经网络章节复制神经网络，修改它为3通道图像。 定义损失函数和优化器我们使用交叉熵误差和有动量的SGD 训练网络到这里就变得有趣了，我们循环遍历数据迭代器，将数据传入网络并优化。 out: [1, 2000] loss: 2.211[1, 4000] loss: 1.825[1, 6000] loss: 1.648[1, 8000] loss: 1.562[1, 10000] loss: 1.504[1, 12000] loss: 1.448[2, 2000] loss: 1.397[2, 4000] loss: 1.353[2, 6000] loss: 1.341[2, 8000] loss: 1.313[2, 10000] loss: 1.270[2, 12000] loss: 1.280Finished Training 让我们快速保存我们的训练模型 在测试集上测试网络我们在训练数据集上训练了网络2次。但是我们需要检查网络是否学习到了所有东西。我们将通过预测神经网络输出的类标签来验证这一点，并根据ground-truth来验证它。如果预测是正确的，我们将该样本添加到正确预测列表中。 第一步，展示一个测试集图片 out: GroundTruth: cat ship ship plane 下一步，我们加载会之前保存的模型（note:这里没有必要保存并重新加载模型，我们这样做只是为了说明如何这样做） 现在让我们看看神经网络是怎么看待上面这些例子的: 输出是10个类的能量。一个类的能量越高，网络就越认为这个图像属于这个类。那么，让我们得到最高能量的指数: out: 看一下网络对于整个数据集的表现怎么样 out: Accuracy of the network on the 10000 test images: 54 % 这看起来比概率(10%的准确率)要好得多(从10个类型中随机挑选一个类型)。看来网络学到了一些东西。 Hmmm, 那些类型表现好，那些类型表现不好 out: Accuracy for class plane is: 60.1 %Accuracy for class car is: 69.8 %Accuracy for class bird is: 45.2 %Accuracy for class cat is: 26.4 %Accuracy for class deer is: 30.5 %Accuracy for class dog is: 60.4 %Accuracy for class frog is: 70.7 %Accuracy for class horse is: 69.5 %Accuracy for class ship is: 53.3 %Accuracy for class truck is: 61.0 % 在GPU上训练就像你把张量放在GPU上一样，将神经网络放在GPU上。 首先定义我们的设备作为cuda第一可见，如果cuda可用： out: cuda:0 然后这些方法将递归遍历所有模块，并将它们的参数和缓冲区转换为CUDA张量: 记住你每步也要把输入和目标送到GPU中 为什么没有强调与CPU相比MASSIVE的加速？因为你的网络太小。"},{"title":"pytorch入门笔记03","date":"2021-10-11T13:44:01.000Z","url":"/2021/10/11/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B003/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"神经网络神经网络可以通过使用torch.nn包进行构建 现在你粗略了解了autograd，nn依赖autograd去定义模型还有求微分。一个nn.Module含有很多层和forward(input)方法，forward方法返回output 这是一个前馈网络的例子。他接收输入，逐层传递输入，最终给出输出 一个典型的神行网络训练流程如下： 定义有一些可学习参数的神经网络 遍历输入的数据集 通过网络处理输入 计算损失（输出和正确解有多远） 将梯度传回网络参数中 更新网络权重， 经典的更新例子： W = W - lr * grad 定义网络 out: Net( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear(in_features=400, out_features=120, bias=True) (fc2): Linear(in_features=120, out_features=84, bias=True) (fc3): Linear(in_features=84, out_features=10, bias=True)) 你仅仅是需要定义forward方法，backward方法（计算梯度）是自动定义好的。你可以使用任何操作在forward方法中。 学习的模型参数由net.parameters()返回。 10torch.Size([6, 1, 5, 5]) 尝试随机32x32输入，注意：预期的输入这个网络（LeNet）的输入时32x32的。用MNIST数据集在这个网络上的话，请将图像尺寸改为32x32 out: tensor([[-0.0771, 0.0865, 0.1370, 0.1146, -0.0952, -0.0172, -0.0014, -0.0123, 0.0449, 0.1213]], grad_fn=) 所有参数的梯度缓存清零。然后随机梯度传播 Note: torch.nn 仅支持mini-batches。 整个torch.nn包仅支持输入一个样本的mini-batch，而不是单个样本 比如，nn.Conv2d将收到（nSamples x nChannels x Height x Width）的4D（4维张量）。 如果你有一个样本，就用input.unsqueeze(0)去添加一个假的批维度（batch dimensioin） 损失函数一个损失行数有（output, target）两部分的输入，计算输出和目标有多远。 nn包中的loss function有一点不同。 一个简单的loss是：nn.MSELoss，它计算的是输入和目标的均方差 如： out: tensor(0.7784, grad_fn=) 现在，如果你跟着loss的方向走，用它的.grad_fn属性，你会看到一个计算图 所以，当我们调用loss.backward(), 整个图是分化为w.r.t神经网络的参数，然后所有在图中有requires_grad=True的张量会有他们的.grad，Tensor累计的梯度。 为了证明，我们退几步 out: &lt;MseLossBackward object at 0x0000028798DF9D68&gt;&lt;AddmmBackward object at 0x00000287AAC61390&gt;&lt;AccumulateGrad object at 0x00000287AAC61390&gt; 反向传播反向传播误差，我们所需要做的就是loss.backward(). 你需要清理掉已存在的梯度，否则会被累加到存在的梯度上。 现在我们尝试调用loss.backward()，观察在backward前后的conv1的偏置梯度 out: conv1.bias.grad before backwardNoneconv1.bias.grad after backwardtensor([ 8.4313e-05, 5.5547e-03, 1.2019e-02, 4.9749e-03, -9.5904e-05, 2.6212e-03]) 更新权重最简单的在训练中更新规则是随机梯度下降（SGD）： 我们可以通过简单的python代码实现 然而，由于你使用的神经网络，你想使用各种不同的更新规则例如，SGD， Nesterov-SGD,Adam,RMSProp等等。我们构建了一个小包：torch.optim用来实现所有的这些方法。 "},{"title":"kaggle注册以及下载","date":"2021-10-08T08:47:10.000Z","url":"/2021/10/08/kaggle%E6%B3%A8%E5%86%8C%E4%BB%A5%E5%8F%8A%E4%B8%8B%E8%BD%BD/","tags":[["kaggle","/tags/kaggle/"]],"categories":[["undefined",""]],"content":"kaggle注册这里我用的手机注册的，翻墙软件及服务器来自： freefq 我是用的google邮箱注册的，没有机器人认证。网上说的邮箱注册，翻墙之后就能看见机器人验证，但是我试了下一直都没有。最后还是用的谷歌邮箱注册的。 数据下载注册成功后 安装kaggle 安装成功之后，检查C:/Users/当前电脑用户名 下面有没有.kaggle文件夹。可以随便执行kaggle的命令，如（kaggle cometitions list）命令。会报错，执行后就有这个文件夹。 下载数据集进入kaggle网页寻找自己需要的数据集。这里以dogs vs cats为例。 点击Account 在API一栏，选择create new API token 会下载一个kaggle.json文件。将文件放到之前的.kaggle文件夹下。 进入自己要的训练集网页dogs vs cats选择Rules 这里必须同意，否则后面下载会报403问题。 点击Data，下面有代码，直接复制，然后在控制台粘贴代码运行即可。 这里默认的下载位置就是C:/Users/电脑用户名 的目录下。需要修改下载路径的话，在下载前执行这个命令 "},{"title":"深度学习入门笔记07","date":"2021-10-07T11:59:12.000Z","url":"/2021/10/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B007/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"过拟合过拟合: 指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。 发生过拟合的原因： 模型拥有大量参数、表现力强 训练数据少 以下是故意产生过拟合的代码 减少了训练数据为300，为增加网络复杂度，设定了7层网络，每层100个神经元。效果如下 可以看到测试数据识别精度到后边都是100%，而测试数据并不理想。 抑制过拟合权值衰减该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。 具体操作：神经网络的学习目的是减小损失函数的值，这时损失函数加上权重的平方范数。即可抑制过拟合。也就是在计算loss的时候加上如下值：此处的λ是控制正则化强度的超参数，λ越大惩罚越严重。 python实现 λ=0.1时的图像 DropoutDropout是一种通过在学习过程中随机删除神经元的方法。 python实现 卷积神经网络全连接层： 相邻层的所有神经元之间都有连接。 卷积层学过opencv的知道，卷积的意思，边缘检测的sobel算子，双边滤波，高斯模糊，掩模操作都是用到的卷积。卷积核就是将一个nxn的矩阵，里面有不一样的权重，将对应像素的值乘上权重后相加，成为中间的值。同样的概念引用到这里。 卷积核也称为滤波器，填充就是做卷积图像边上的一圈填入固定值，步幅就是滤波器的位置间隔。 特征图： CNN中，卷积层的输入输出数据。有输入特征图，输出特征图 输出图像大小计算假设： 输入大小为 (H,W) H: high, W: width 滤波器大小为 (FH,FW) FH: Filter High, FW: Filter Width 步幅为 (S) S: Stride 填充为 (P) P: Padding 池化层池化是缩小高、长方向上的空间的运算。常见的是Max池化（将目标区域中取出最大元素） 池化层的特征没有要学习的参数：池化只是目标区域中最大值（或平均值）。 通道数不会变化：经过池化运算，输入输出的通道数不会变化。 微小位置变化具有鲁棒性：输入数据发生微小偏差时，仍会返回相同结果。 卷积层实现util工具包 python实现 池化层实现python实现 CNN实现有了卷积层和池化层，现在可以组合这些层，完成搭建MNIST识别的CNN。网络结构如下 Image Input -&gt; Conv -&gt; ReLu -&gt; Pooling -&gt; Affine -&gt; ReLu -&gt; Affine -&gt; Softmax 该类命名为SimpleConvNet。 初始化参数 input_dim：输入数据的维度：（通道，高，长） conv_param：卷积层的超参（字典）。字典key如下： ​ filter_num: 滤波器的数量 ​ filter_size: 滤波器大小 ​ stride：步幅 ​ pad：填充 hidden_size：隐藏层（全连接）的神经元数量 output_size：输出层（全连接）的神经元数量 weight_int_std：初始化时权重的标准差 "},{"title":"深度学习入门笔记06","date":"2021-09-30T02:08:23.000Z","url":"/2021/09/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B006/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"参数更新这里就是将之前的权重更新的算法，封装成一个类。每个类里面都有一个update方法，这样往后如果需要修改更新权重的方法（SGD，Momentum，AdaGrad，Adm），直接更改对应类的对象即可。 e.g SGD随机梯度下降法，和之前一样，这里只是做了封装 python实现 SGD缺点SGD的下降方式呈现“之”字形，比较低效。 Momentum数学表达：此处的v相当于物理中的速度，α模拟的阻力。这种方法会更快的向x轴靠近，减少之字形带来的影响。 AdaGrad学习率衰减法：随着学习进行，学习率逐渐变小，开始多学，后面少学。 数学公式：新变量h，它保存了以前所有梯度值的平方和，更新参数时，通过乘以1除根号h，调整学习尺度。参数的元素中变动较大的元素的学习率将变小。 python实现 Adm这里具体原理没有详讲，只是贴下代码 python实现 权重的初始值权重的初始值设定很重要，这个会导致学习是否会快速进行 隐藏层的激活值的分布假设这里有5层神经网络，激活函数使用sigmoid，传入随机数据，观察分布。 out: 这个代码中主要关注的是权重的尺度，标准差的选择。从图中看，由于是sigmoid函数，随着输出不断靠近0或1，他的导数值逐渐接近0，因此偏向0和1的数据分布会造成反向传播中梯度的值不断变小，最后消失。这个问题就是梯度消失。层次加深的深度学习中，梯度消失问题会更加严重。 将标准差变为0.01后 out: 这次呈现集中在0.5分布，不会发生梯度消失问题。激活值的分布有所偏向，在表现力上会有问题，多个神经元输出几乎相同的值，那他们也没有存在的意义了。 尝试Xavier Glorot推荐的权重初始值: 如果前一层的节点数为n，则初始值使用标准差为1/√n 的分布 从结果可知，越是后面的层，图像变得更歪斜，呈现了比之前更有的广度分布。因为各层间传递的数据有适当的广度，sigmoid表现力不受限制。 ReLu初始值推荐Kaiming He初始值，也称为“He初始值”，使用标准差为√(2/n)的高斯分布。 Batch Normalization使用Batch Normalization，使得各层拥有适当的广度，“强制性”的调整各层的激活值分布。 优点： 使得学习更加快速进行 不那么依赖初始值 抑制过拟合 为了有适当的广度，所以需要在神经网络中添加正规化层，就是Batch Normalization层。 BN层做的事情就是使得数据分布为0、方差为1的正规化。过程如下 这里ε是一个极小值，防止下边变为0导致计算错误。 此处γ的值初始值为1，β值为0，后续通过学习调整到合适的值。 python实现： "},{"title":"pytorch笔记02","date":"2021-09-29T05:14:47.000Z","url":"/2021/09/29/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B002/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"pytorch自动微分torch.Tensor是包的核心类，有个属性.requires_grad设为True就会跟踪tensor所有操作。计算完成后调用backward()自动计算所有梯度。这个张量的梯度将累计到.grad属性中 调用.detach()可以与计算历史记录分离。 也可以用 with torch.no_grad(): 包起来。 Tensor当中有一个属性grad_fn用来记录创建了张量的Function引用。 out: tensor([[1., 1.], [1., 1.]], requires_grad=True) 做一次加法操作 out tensor([[3., 3.], [3., 3.]], grad_fn=) 做更多操作 out: tensor([[27., 27.], [27., 27.]], grad_fn=) tensor(27., grad_fn=) 激活函数pytorch中的激活函数所在的位置是 torch.nn.functional当中，使用的时候引入即可 ![](pytorch入门笔记02/torch Activation Funtion.jpg) 加载数据使用pytorch中的utils.data pytorch的使用个人翻译pytorch官网60min教程 让我们看一下单词训练。 比如， 我们从torchvision中加载一个预训练resnet18模型。我们创建一个随机tensor数据去代表一个有3通道，高和宽都是64的图像，它对应的label初始化为一些随机数。 接下来，我们将输入数据通过模型的每一层做一次预测，这就是前向传播 我们用模型的预测和对应的label去计算误差(loss). 下一步是对网络的误差进行反向传播。当我们调用误差tensor的.backward()的时候，反向传播开始。自动梯度接下来会为每个模型参数计算并存储梯度在参数的.grad属性 下一步，我们加载一个优化器， 在这个例子用的SGD，学习率为0.01，动量为0.9，我们在这个优化器中注册模型的所有参数 最终，我们调用.step()去初始化梯度下降。优化器通过存储在.grad中的梯度调整每一个参数 到这一步，你已经掌握了训练你的神经网络所需的所有东西。 这里官网教程往后是详细介绍梯度原理。求梯度原理请见深度学习入门笔记04"},{"title":"深度学习入门笔记05","date":"2021-09-28T13:55:08.000Z","url":"/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B005/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"误差反向传播法计算图计算图：将计算过程用图形表达出来。 首先说下书上的例子： 问题1：太郎在超市买了2个100日元一个的苹果，消费税是10%，请计算支付金额。 相当简单的一道题， 2 * 100 * 1.1即可。计算图表示则是： 将x2和x1.1节点中的数字取出，符号单独放在○当中。 这种从左往右的计算方式为正向传播 从右往左的计算方式则是反向传播 局部计算简单来说就是只用关注当前的简单计算部分，其他复杂的部分不需要管。意思就是计算偏导的那种感觉。 为何用计算图解题优点： 局部计算，无论全局是多么复杂，都可以通过局部计算使各个节点致力于简单计算，简化问题。 可以将中间的计算结果全部保存起来。 通过反向传播高效计算导数。 基于反向传播的导数的传递 计算图的反向传播先看一个的反向传播例子 就是将信号E乘以节点的局部导数，然后传递到下一个节点。如果假设y = x^2 那么导数为2x，那么向下传播的值就是 E*2x，这里的x是正向传递时记录的。 链式法则 就是高数里对复合函数求导。 e.g令链式法则和计算图 反向传播对于每个层，都有forward方法和backward方法，对应正向反向传播。在训练时创建网络时，将每一层存在一个列表当中，顺序正向传播。当需要计算梯度时，将列表翻转，依次执行backward方法进行反向传播，求得梯度。 加法节点的反向传播正向计算图 此处是 z = x + y，分别对x，y求偏导。结果都是1。按照前面说的反向传播的话就是将前面给的E乘上局部的导数可知，对于加法来说，都是将E*1传给后面。 python实现： 乘法节点的反向传播考虑 f = xy的导数公式正向传播图 反向传播图 由图中可知对于乘法，是将E乘上输入信号的翻转值。 python实现: 回到苹果的例子对于买苹果的例子反向传播则是 首先最终结果是220，导数就是1，向后传播，是一个乘法传播，E乘上输入信号的翻转值，也就是200和1.1交换相乘，得到1.1和200。继续向后传播，也是乘法，交换相乘得到2.2和110。 激活函数层实现ReLU层数学公式：求关于x的导数计算图 python实现： sigmoid层数学公式计算图 python实现： Affine/Softmax 层Affine层： 就是原先乘上权重加上偏置的操作，对于矩阵的正向反向传播 python实现 Softmaxwithloss层 python实现 "},{"title":"深度学习入门笔记04","date":"2021-09-28T12:02:02.000Z","url":"/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"神经网络的学习这章主要讲的是函数斜率的梯度法 计算机视觉领域常用的特征量包括 SIFT、SURF、HOG等。 训练数据和测试数据训练数据和测试数据 ： 首先使用训练数据进行学习，寻找最优的参数，然后使用测试数据评价训练得到的模型的实际能力。 为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。训练数据又称为监督数据。 过拟合：只对某个数据集过度拟合的状态。 损失函数损失函数：表示神经网络性能的“恶劣程度”的指标，当前神经网络对监督数据再大多成都上不拟合，大多程度不一致。通常乘上一个负值，解释为“多大程度上不坏”，“性能上有多好“。 损失函数可以用任何函数，不过一般都用的均方误差或交叉熵 个人理解：就是神经网络的预测与实际的标签，相差距离的多少。 均方误差公式：yk : 神经网络的输出 tk : 监督数据 k : 数据维数 python表示： 拿MNIST数据做一次尝试 假设： 预测数据 [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0] 正确解(one_hot) [0,0,1,0,0,0,0,0,0,0] #正确解为2 out: 0.097500000000000030.5975 很明显显示第一个例子与监督数据更加吻合。 交叉熵误差公式：符号解释，同上面 由于tk中只有正确解为1，其他为0，所以只用计算正确解标签的输出的自然对数。因此可知，交叉熵误差的值是由正确解标签所对应的输出的结果决定的 python实现： 同样拿之前的假设做对比 out: 0.05108254570993380.23025840929945457 mini-batch学习​ 在机器学习使用训练数据进行学习，需要针对训练数据计算损失函数值，找出使这个值尽可能小的参数。计算损失函数的时候需要将所有训练数据作为对象。也就是说，如果有100个训练数据，需要把100个的损失函数的总和作为学习的指标。 ​ 以交叉熵为例：就只是把单个的损失函数数据扩大到了N份，最后还是需要除以N做一次正规化。就是求的“平均损失函数”。对于MNIST数据集有60000个训练数据，不可能每个都加上，这样花费的时间较多。因此会从中随机选择100笔，再用100笔数据进行学习。这就是mini-batch学习 python实现 np.random.choice() 从指定的数字中随机选择想要的数字。 mini-batch交叉熵的实现 另一种实现 设定损失函数原因为什么不以精度为指标？ 为了使损失函数的值尽可能小，需要计算参数的导数(梯度),以导数为指引，逐步更新参数值。 ​ 对权重参数的损失函数求导，表示对如果稍微改变这个权重的值，损失函数的值该如何变化。 ​ 如果为负： 通过改变权重向正方向改变，即可减小损失函数的值。 ​ 如果为正： 通过改变权重向负方向改变，即可减少损失函数的值。 如果用精度作为指标，大多数地方导数变为0，无法更新。 数值微分导数就和高数讲的一样 偏导也是和高数一样 假设公式：偏导 梯度梯度：由全部变量的偏导汇总而成的向量 e.gpython实现也很简单 梯度法梯度法：通过不断地沿梯度方向前进逐渐减小函数值的过程 寻找最小值是梯度下降法，寻找最大值是梯度上升法 数学表达：其中η是更新量，在机器学习中是学习率 学习率是实现设定的值 init_x 是初始值， lr是学习率，step_num 是重复次数 现在可以尝试求解 f(x0,x1) = x0^2 + x1 ^2 的最小值 out: 微分2.0000000000042206梯度[6. 8.]梯度下降法求 f(x0,x1) = x0^2 + x1^2 最小值[-6.11110793e-11 8.14814391e-10] 结果是十分接近（0,0）点，而事实上最低点就是（0,0）点 学习率这样的参数称为超参数。它和权重参数是不同的，权重是可以通过数据和学习自动获得的。学习率这样的超参数是人工设定的。 2层神经网络的类通过梯度下降法更新参数，由于数据是随机选择的mini batch数据，又称之为随机梯度下降法（stochastic gradient descent）简称SGD 写一个名为TwoLayerNet类 首先看__init__方法，input_size，hidden_size，output_size。依次表示的是输入层神经元数、隐藏层神经元数、输出层神经元数。input_size是784，因为输入的图像是（28x28）的，output_size也就是对应输出层，总共10个类型，所以是10。中间隐藏层是设定合适的值即可。书上设定的50个，我设定的100个，感觉效果比50的要好一点。权重使用的高斯分布的随机数进行初始化。 ​ 预测就是和之前的神经元一样，两层。乘上权重加上偏置，过一次sigmoid，然后进入二层，乘上权重加上偏置，过softmax，看概率。 ​ 损失函数，用的mini batch的交叉熵。 ​ 梯度下降，用的偏导，这个地方也是很耗时的，所以最终使用的时候用下边的gradient，使用的误差反向传播算法。 mini batch的实现 整个就是梯度下降实现了，先设定好超参数，循环10000次，每次循环都是随机抽取batch_size个训练数据，扔到梯度下降的函数中去。获得偏导的向量，然后依据偏导向量乘上学习率，一点点修改权值。最终查看loss_list会发现，损失值在不断减小。 基于测试数据的评价这里引入一个新词 **epoch : **epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时更新数据。如10000个数据，用大小为100笔数据的mini batch训练学习时，重复SGD100次，所有的数据都被“看过”则是一个epoch。更正确的做法是，10000个数据全部打乱，然后100个一组训练，顺序训练100组。为一个epoch。 加了epoch的代码，后面有绘制图形。也有将训练好后将网络保存。方便随时读出，用opencv + numpy做识别手写数字。 "},{"title":"pytorch入门笔记01","date":"2021-09-25T09:56:25.000Z","url":"/2021/09/25/pytorch%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B001/","tags":[["python","/tags/python/"],["pytorch","/tags/pytorch/"]],"categories":[["pytorch","/categories/pytorch/"],["python","/categories/pytorch/python/"]],"content":"Pytorch 张量标量（0D）只包含一个元素的张量为标量。类型通常为FloatTensor或LongTensor 向量（1D） out: torch.Size([4]) 矩阵（2D） out: torch.Size([2, 2]) 三维向量多个矩阵累加在一起。比如一张图片，有三个通道，每个通道都有一个矩阵。此处的pic2.jpg是一个28x28x3的图片。 out: torch.Size([28, 28, 3]) 切片张量这里和python的用法一样 out: tensor([1., 2.]) 对于前面的如果需要其中一个通道。一种用切片向量，一种用opencv的split方法。 四维张量对于多张图片，批处理那种 五维张量视频数据。 GPU上的张量 out: CPU cost: 7.888931035995483GPU cost: 0.1765275001525879 常见的方法判断GPU是否可用 生成空矩阵没有初始化 out: tensor([[9.3673e-39, 9.5511e-39, 1.0194e-38], [4.2246e-39, 1.0286e-38, 1.0653e-38], [1.0194e-38, 8.4490e-39, 1.0469e-38], [9.3674e-39, 9.9184e-39, 8.7245e-39], [9.2755e-39, 8.9082e-39, 9.9184e-39]]) 随机初始化 out: tensor([[0.4749, 0.0095, 0.4786], [0.5207, 0.4228, 0.0364], [0.8313, 0.9352, 0.6975], [0.2701, 0.5206, 0.8709], [0.3670, 0.3378, 0.9704]]) 创建零矩阵 out: tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]) 构造一个张量 out: tensor([5.5000, 3.0000]) 基于一个已存在的tensor创建一个 out: tensor([[1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.], [1., 1., 1.]], dtype=torch.float64)tensor([[-0.7830, 0.2870, 0.3721], [ 0.2931, 0.4255, 0.3800], [-0.1016, 0.6011, -0.7567], [ 0.4526, -1.0510, -0.4116], [ 1.4605, 1.4378, 0.4322]]) 获取维度信息 out: torch.Size([5, 3]) 加法 out: tensor([[2., 2., 2.], [2., 2., 2.], [2., 2., 2.], [2., 2., 2.], [2., 2., 2.]]) numpy一样标准的索引和切片 out: tensor([[1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.], [1., 0., 1., 1.]]) 乘法 矩阵乘法计算，行乘列求和的那种 out: tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]])tensor([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) 连接张量连接张量可以使用torch.cat。 将给定维数的张量顺序连接起来，也可以用torch.stack，另一种张量连接，略有不同 out: tensor( ​ [[1., 0., 1., 1., 1., 0., 1., 1.],​ [1., 0., 1., 1., 1., 0., 1., 1.],​ [1., 0., 1., 1., 1., 0., 1., 1.],​ [1., 0., 1., 1., 1., 0., 1., 1.]]) 改变tensor的形状 out: torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8]) 获取标量的值 out: tensor([1])1 numpy和tensor相互转换numpy转pytorch 主要是torch.from_numpy() pytorch转numpy 模型的保存和读取"},{"title":"hexo数学公式显示问题","date":"2021-09-24T03:28:58.000Z","url":"/2021/09/24/hexo%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/","tags":[["hexo","/tags/hexo/"]],"categories":[["hexo","/categories/hexo/"]],"content":"解决方案使用 hexo-math安装： 需要Hexo 5版本以上 使用KaTeX 使用mathjax 详情见：hexo-math 使用hexo-filter-mathjax安装： 更多查看：hexo-filter-mathjax"},{"title":"深度学习入门笔记03","date":"2021-09-23T13:32:13.000Z","url":"/2021/09/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B003/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"输出层设计神经网络可以分为两类问题： 分类问题，回归问题 分类问题：属于哪一类问题。 (e.g 根据图片判断人物是男的还是女的) 回归问题：根据输入的值，预测数值问题。(e.g 根据图片预测这个人的身高体重) 恒等函数和softmax函数恒等函数：将输入原样输出。用于回归问题。 softmax函数：用于分类问题 softmax函数公式表示：python实现： 这里有个操作 exp_a = np.exp(a - c) 用来解决溢出问题。因为是指数函数，a的值如果变得很大，那会使得发生溢出现象。解决方法很简单，分子分母同时乘上一个常数C，推导如下：为一个常数这样一波操作下来，溢出问题解决，y的值是正确的，同时输出的值在0.0到1.0之间，而且softmax输出的值总和为1，因此可以把softmax的输出解释为“概率” 一般来说，神经网络是以输出值最大的作为结果，因为使用了softmax，最大的值概率也是最大的。可以为了减少计算量，不使用softmax，用最大值代替。 输出层神经元数量根据待解决问题决定，分类问题按照类别数量设定。 前向传播前向传播：使用学习到的参数，实现神经网络的“推理处理”，这个推理处理也成为神经网络的前向传播 下面通过实例——手写数字识别，完成前向传播过程 MNIST数据集MNIST数据集是手写数字图像集，有60000个训练样本，10000个测试样本。每张图像是28像素x28像素的单通道灰度图像。 书中提供了方便mnist下载的python文件。python运行mnist.py即可下载数据集，以及训练好的pickle文件。 pickle： python将程序运行中的对象保存为文件，通过加载保存过的pickle文件，可以立刻复原之前运行中的对象。 通过mnist.py中的load_mnist()函数，读出MNIST数据 这里只是读出数据，检查是否读出成功。 load_mnist(flatten=True,normalize=False,one_hot_label=False)中 flatten是确定是否将图像展开，意思是如果图像是 1x28x28的三维数组，展开后就是一个784个元素的一维数组。 normalize就是正规化的意思，打开mnist文件可以发现，他做的一件事就是每个像素的值除了一个255，让他在0.0 ~ 1.0之间。这个操作十分重要。 one_hot_label表示将正确解标签为1，意思是有0 - 9这10个数，10个类别。[0,0,1,0,0,0,0,0,0,0] 表示当前的标签正确的是2 显示图像 神经网络的推理处理 批处理"},{"title":"深度学习入门笔记02","date":"2021-09-23T09:27:36.000Z","url":"/2021/09/23/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B002/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"神经网络一些概念 输入层：最左边的一列，用来输入数据的一层 输出层：最右边的一列，用来输出数据 隐藏层：中间层。 神经网络和感知机是相似的，二者最大的区别就在于激活函数。 激活函数先看一下之前感知机的函数式简单做下变形引入一个函数 h(x)，将函数式继续简化其中这种会将输入信号的总和转换为输出信号，将这个函数h(x)成为激活函数 sigmoid函数 python实现 阶跃函数和之前感知机部分一样，给定一个阈值，当超过阈值就切换输出。这种函数成为阶跃函数 ReLU函数ReLU函数：在输入大于0时，直接输出该值；在小于等于0时为0python实现 图形显示 sigmoid和阶跃函数比较不同点： ​ “平滑性”不同，sigmoid函数是一条平滑的曲线，阶跃函数以0为界，输出发生急剧变化。另一个不同则是，感知机当中神经元之间流动的是0,1的二元信号，而神经网络中流动的是实数信号。 共同点： ​ 二者有相似形状，输入越小，越接近0，输入增大接近1。输出都是在0和1之间。 非线性函数激活函数都是非线性函数，为啥是非线性函数呢？ 举个栗子就能证明出来了，如果激活函数使用了线性函数，h(x) = cx，那么经过三层神经网络， 就变成了 h(h(h(x))) = c * c *c * x，完全相当于 h(x) = c^3 *x , 令a = c^3，那和h(x) = a * x 就没区别了，叠加层数变得毫无意义。为了发挥叠加层的优势，需要用非线性函数。"},{"title":"深度学习入门笔记01","date":"2021-09-22T12:03:16.000Z","url":"/2021/09/22/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B001/","tags":[["python","/tags/python/"],["DeepLearning","/tags/DeepLearning/"],["numpy","/tags/numpy/"]],"categories":[["DeepLearning","/categories/DeepLearning/"]],"content":"感知机​ 笔记是学习《深度学习入门 基于python理论实践》记录的。由于会一些python基础，所以直接从第二章开始记起。 感知机概念​ 感知机接收多个输入信号，输出一个信号。 x1,x2是输入信号，w1,w2是权重，y是输出信号。信号送往神经元时，都会乘以相对应的权重(x1w1, x2w2)。神经元计算传来的信号总和，当总和超过一定阈值则输出1，否则输出0。 数学表达： 简单的逻辑电路与门 X1 X2 Y 0 0 0 1 0 0 0 1 0 1 1 1 通过感知机来表示与门，也就是说需要找到(w1,w2,θ) 相应的权重和偏置，满足条件。如：(0.5,0.5,0.7)满足条件，即 （0.5 * x1 + 0.5 * x2 &lt;= 0.7 时为0， &gt; 0.7时为1）当然(0.5,0.5,0.8)也可以 python实现 与非门 X1 X2 Y 0 0 1 1 0 1 0 1 1 1 1 0 与非门就是跟与门相反，对应(w1,w2,θ)相反即可。(-0.5,-0.5,-0.7) python实现 或门 X1 X2 Y 0 0 0 1 0 1 0 1 1 1 1 1 这部分书上没有给对应的 (w1,w2,θ) ，我自己算了一个。计算过程就是 将x1,x2，作为横竖坐标，并将(0,0),(1,0),(0,1),(1,1)点画到坐标中，每个点Y如果是0，画○，如果是1，画×。 得到一幅图像，那么需要找到一条线将×和○分开，这条线假设为 y = kx + b 转到 x1,x2坐标上就是 x2 = kx1 + θ. 对应 w2x2 = w1x1 + θ，图中可知这个线必然是 大于0，小于1，故 0 &lt; θ &lt; 1，w1 &lt;= -1 ,将w2除到右边去，因为x2 &gt;= 0 ,可知 w2 &lt; 0。 得到条件 0 &lt; θ &lt; 1，w1 &lt;= -1， w2 &lt; 0 我设定的值为 (-1,-1,0.5)，但是好像刚好和或门相反，然后将值取反即可(1,1,0.5) python实现 通过这些代码，就能发现，与门，与非门，或门都是具有相同构造的感知机，他们的区别就在于权重参数的值 多层感知机异或门异或门：仅当x1，x2当中的一方为1时才会输出1 X1 X2 s1 s2 Y 0 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 对于前面的与门，与非门，或门用一层感知机即可实现 或门感知机可视化。可以发现，与门，与非门，或门都可以用一条线来划分开来。那么异或门 无法用一个感知机划分开。单层感知机无法分离非线性空间。 解决方法： 感知机可以叠加，多层感知机即可完成。 上面x1,x2到s1是做的与非门，到s2是做的或门，s1,s2到y做的是与门。 python实现： 注意：感知机总共由3层构成，有权重的层只有2层（0层 ~ 1层，1层 ~ 2层），因此称为”2层感知机”。 具体过程： 第0层的两个神经元接收输入信号，并将信号发送至第1层神经元 第1层的神经元将信号发送至第2层神经元，第2层神经元输出y 单层感知机无法表示的东西，通过增加一层就可以解决。"},{"title":"tensorflow笔记03","date":"2021-09-18T03:23:53.000Z","url":"/2021/09/18/tensorflow%E7%AC%94%E8%AE%B003/","tags":[["python","/tags/python/"],["TensorFlow","/tags/TensorFlow/"]],"categories":[["TensorFlow","/categories/TensorFlow/"]],"content":"Tensorflow TensorBoard数据流图可视化tf.summary.scalar() tf.summary.histogram() tf.summary.Filewriterr: ​ writer = tf.summary.Filewriter(“summary_dir”,sess.graph) 将摘要和图形写入summary_dir中 在控制台输入tensorboard –logdir=summary_dir, 在浏览器中输入他给的地址即可。 拿上一篇的代码举例 浏览器中显示 指定CPU和GPU设备 配置显示使用设备 使用配置： 使用CPU0运行 使用GPU0运行 "},{"title":"tensorflow笔记02","date":"2021-09-18T03:18:23.000Z","url":"/2021/09/18/tensorflow%E7%AC%94%E8%AE%B002/","tags":[["python","/tags/python/"],["TensorFlow","/tags/TensorFlow/"]],"categories":[["TensorFlow","/categories/TensorFlow/"]],"content":"矩阵基本操作及实现常见矩阵操作 "},{"title":"tensorflow笔记01","date":"2021-09-17T07:14:31.000Z","url":"/2021/09/17/tensorflow%E7%AC%94%E8%AE%B001/","tags":[["python","/tags/python/"],["TensorFlow","/tags/TensorFlow/"]],"categories":[["TensorFlow","/categories/TensorFlow/"]],"content":"Tensorflow安装安装python，安装CUDA CUDA版本号查看，打开英伟达控制面板，左下角系统信息 对应的CUDA11，网上的pip安装TensorFlow如果失败，或者想要用GPU的，可以用这里有大佬编译好的包即可。 TensorFlow常量、变量和占位符 一些补充"},{"title":"OpenGl笔记02","date":"2021-06-13T11:26:31.000Z","url":"/2021/06/13/OpenGL%E7%AC%94%E8%AE%B002/","tags":[["OpenGL","/tags/OpenGL/"]],"categories":[["OpenGL","/categories/OpenGL/"]],"content":""},{"title":"OpenGL笔记01(建立OpenGL和用C++创建一个窗口)","date":"2021-06-13T10:01:08.000Z","url":"/2021/06/13/OpenGL%E7%AC%94%E8%AE%B001/","tags":[["OpenGL","/tags/OpenGL/"]],"categories":[["OpenGL","/categories/OpenGL/"]],"content":"建立OpenGL学习使用的环境： Windows 10, Visual studio 2017, 项目架构x86, GLFW 32位。 下载GLFW进入网站： GLFW网站 Download下载GLFW，这里是源码，下边有预编译好的版本。Document可以查看文档（其中就有创建窗口的方法）。 设置库添加依赖文件在项目中添加Dependency文件夹，用来存放GLFW依赖。上边下载的GLFW的预加载文件，解压后。 主要用到两个。在lib-vc2017中，glfw3.dll和glfw3dll.lib暂时可以不需要，主要用到静态链接库glfw3.lib 配置属性打开VS右边的属性管理器，右键项目 -&gt; 属性(properties)，如下配置 点击附加包含目录，添加include位置，但是不推荐直接写绝对路径，因为项目不一定会在这个位置，当然vs提供了一个宏定义(Macros)，点击宏,搜索SolutionDir即可找到。 设置配置 Dependency位置，验证配置正确与否，可以在包含目录点击三角，下边有个编辑，有个计算的值，把值拿出来放在文件夹路径下，看看能不能访问到，能访问到就ok了。 同样的方法设置链接器(linker) 设置需要的依赖项 (Additional Dependencies) 创建一个窗口打开GLFW官网的Document页面，复制代码 F5运行即可得到一个窗口。 绘制一个三角形绘制是在glClear之后进行绘制。需要glBegin(GLenum) glEnd()。添加代码 运行即可。"},{"title":"Kolin笔记08(高阶函数，内联函数)","date":"2021-05-09T12:51:59.000Z","url":"/2021/05/09/Kotlin%E7%AC%94%E8%AE%B008/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"高阶函数定义：如果一个函数接收另一个函数作为参数，或者返回值的类型是另一个函数，该函数称为高阶函数。 Kotlin增加了一个函数类型概念，也就是说可以传递函数作为参数 基本规则 解释： -&gt; 左边部分用来声明该函数接收什么参数，多个参数之间用逗号隔开。 右边部分用于声明该函数返回值是什么类型，如果没有就是 Unit 相当于java中的Void 使用 这里 ::plus ,::minus 的写法是函数的引用写法，表示将plus(),minus()函数作为参数传递给num1AndNum2()函数中。 简化：显然不可能每次使用高阶函数都要创建新的函数，这样太麻烦了。因此可以这么写。 内联函数: inline简单分析下高阶函数的实现原理 用法： noinline与crossinline前面说明了，inline是直接将Lambda表达式替换到了调用的地方，减小了内存开销。考虑一个情形：如果高阶函数接收了两个或更多的函数类型参数，此时加上了inline关键字，Kotlin编译器会自动将所有的引用全部进行内联。如果我们只想让它内联其中一个Lambda表达式怎么办呢？ 这时就可以考虑使用noinline: block2 参数前加上了noinline关键字，现在就对block1进行内联，block2就不会了。 为什么Kotlin要提供noinline关键字 由于内联函数需要进行代码替换，因此它不是真正的参数属性，非内联函数类型参数可以自由的传递给其他任何函数，因为他是一个真实的参数，而内联函数，只能传给另一个内联函数。 绝大多数高阶函数是可以直接声明成内联函数的，但也有少部分情况。 这里会进行报错，首先，在runRunnable函数中，创建了一个Runnable对象，在Runnable的Lambda表达式当红传输了函数类型参数。而Lambda表达式在编译的时候会被转成匿名类的实现方式，实际上上述的代码是在匿名类当中调用了传入的函数类型参数。 内联函数中引用 Lambda表达式允许用return关键字进行返回，但是由于在匿名类中调用的函数类型参数，此时不能进行外层调用函数返回的，只能对匿名类中的函数调用返回。:dizzy_face::dizzy_face::dizzy_face: 此时就可以用crossinline关键字 加上了crossinline就不会报错了，加上crossinline后，无法再调用runRunnable函数时的lambda表达式中用return进行返回了，但是可以使用 return@runRunnable进行局部返回。"},{"title":"Kmp算法","date":"2021-05-05T08:20:14.000Z","url":"/2021/05/05/Kmp%E7%AE%97%E6%B3%95/","tags":[["kmp","/tags/kmp/"],["算法","/tags/%E7%AE%97%E6%B3%95/"],["kotlin","/tags/kotlin/"],["C/C++","/tags/C-C/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"kmp算法本身匹配的方法理解感觉不太难，最大的难点可能就是next数组的计算，不容易理解。匹配的原理就是，提前算出子串的一个next数组，这个next数组记录了子串中到当前位置的字符串的最长前后缀（前后部分的相同部分的最长长度），利用这个next表，当子串和主串比对的时候，如果不同，就会找next表，利用这个最长长度，计算移动到对应的位置，使得最长前面的部分不用在去匹配，用来提高匹配效率。本文是参考的《王道考研数据结构》上写的，只是记录的一些自己的理解哈。 前缀，后缀，部分匹配值前缀：除最后一个字符以外，字符串的所有头部子串 后缀：除第一个字符以外，字符串的所有尾部子串 部分匹配值：字符串的前后缀的最长相等前后缀长度 以 “ababa”为例 字符串 前缀 后缀 部分匹配值 a ∅ ∅ 0 ab {a} {b} 0 aba {a,ab} {a,ba} 1 abab {a,ab,aba} {b,ab,bab} 2 ababa {a,ab,aba,abab} {a,ba,aba,baba} 3 部分匹配值 = (前缀集合 ∩ 后缀集合 ) 的长度 恭喜！这里next数组的雏形就出来了，说实话这就是next数组。真正最后用的也是在这个基础上优化出来的Next数组。接下来就是使用这个数组，和如何优化。 S a b a b a next 0 0 1 2 3 部分匹配值的使用主串：a b a b c a b c a c b a b 子串：a b c a c 首先拿到next数组 序号 1 2 3 4 5 S a b c a c next 0 0 0 1 0 第一次匹配： ababcabcacbab abc a 和 c不匹配。 前面 ‘ab’ 匹配。最后一个匹配字符b对应的匹配值为0 按照 ： 移动位数 = 已匹配的字符数 - 对应的部分匹配值 得出： 移动位数 = 2 - 0 = 2 第二次匹配： ababcabcacbab ​ abcac b 和 c不匹配。前面’abca’匹配，最后一个匹配字符a对应的匹配值为1 移动位数 = 4 - 1 = 4 第三次匹配： ababcabcacbab ​ abcac 子串比对完成。主串没有回退，所以KMP算法时间复杂度为O(n + m) 算法改进当前的算法 移动位数 = 已匹配的字符数 - 对应的部分匹配值 转为伪代码。 优化使用部分匹配的时候，每次失败都需要找到前一个元素匹配的部分匹配值，使用起来不太方便。将数组右移一位，这样一旦失败直接取到对应的值 序号 1 2 3 4 5 S a b c a c next -1 0 0 0 1 第一个元素为右移之后空缺，用-1填充。目的是第一位不匹配，向右移动一位。就是：移动位数 = 已匹配的字符数（0） + 1 = 1 最后一个元素溢出，因为根本用不到。 这样上面的式子就改为 那么计算移动应该到达的位置（子串中指针j应该移动到的位置） 因为每次j = next[j] + 1每次都要+1，很麻烦，把+1放到数组的值里面。这样一旦不匹配就移动到第next[j]的位置即可。 对于数组是从0开始计算的话，不用这个+1操作了，本身就已经是对应的坐标位置了。 OK，以上就是next怎么来的原因。 接下来就是计算next的代码了。 next数组计算代码书上的错误 这块部分我照着改成C++，Kotlin的方式写了，得出来的结果都不对。自己推逻辑感觉没问题，但是计算机算出来就是不对。最终的问题还是出在指针的位置问题。因为书上讲的编号都是以1开头来算的，但是字符串可不是从1开始算的啊。导致字符串和next数组根本是错开的。问题就出在 T.ch[i] == T.ch[j]上面。得到的next数组就是 0,1,1,1,1。当然如果传进来的字符串也是从1开始算的就没问题了。 书上代码纠正 按照从0开始计算的代码从0开始的话就不会出现上面的问题，从1开始逻辑没理清就很容易出现错位问题。 代码讲解额，这块还是按照书上从1开始的方式讲解吧。首先是初始化，i = 1,j = 0。i是持续向后走的，j指针是来回跳的。j主要作用就是和i进行比较，如果i，j的字符相等，i，j同时向后移动，并将j的当前位置赋给i对应的next元素，为什么这么做？分开说 i,j同时向后移动做的事就是为了下一次循环再次比对，是否还是相等 将j的当前位置赋给i对应的next元素，为的是记录当出现不匹配的时候，跳转的位置。同时也是完成了前面说的右移操作。也就是说这个位置是记录在匹配好的时候i位置的后一位，也就是做了++i之后的操作。 还有一个操作就是判断了 j == 0，刚好是第一个的前一个，j = 0是为了方便后面+1直接刚好是第一个，这样只要不匹配，得到的坐标就是第一个的位置，直接子串拉回第一个，从头判断 else部分就是当不匹配也不是最初状态的时候，j回退到上一次匹配到的位置，下次循环紧接着之前的位置继续匹配。 KMP全部代码 这块没有什么难度了。大家自行分析吧。 速成KMP，推荐一个视频。觉得文章有些长，可以看这个b站大佬的KMP视频，讲的非常好。 KMP字符串匹配算法1 KMP字符串匹配算法2"},{"title":"Kotlin笔记07(延迟初始化，密封类，扩展函数，运算符重载)","date":"2021-05-05T01:14:27.000Z","url":"/2021/05/05/Kotlin%E7%AC%94%E8%AE%B007/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"延迟初始化Kotlin的判空等特性，都是为了保证程序安全所设计的，但是有时这些设计会变得比较麻烦。 对于类中存在很多全局变量，为了保证满足kotlin的空指针检查，不得不写上很多非空判断保护才行。问题解决办法则是，对全局变量进行延迟初始化。 这里lateinit关键字，告诉kotlin我会晚些对这个变量初始化。此时value就不用在一开始就赋值为null，并且也不用做判空处理。 注意：使用lateinit也是有风险的，如果在没有初始化的时候使用它，同样会崩溃。所以使用lateinit的前提是，必须确保在他被任何地方调用之前，已经完成初始化工作。 Android例子： 判断变量是否被初始化 ::adapter.isInitialized可以用来判断变量是否初始化，这个是固定用法。 密封类密封类关键字：sealed class 场景： 创建一个Result.kt 定义Result接口，Success 和Failure实现这个接口。然后使用： 这里的else是必须写的，否则Kotlin认为缺少分支条件无法编译通过，实际上Result的结果只可能是Success和Failure，这里的else完全是为了通过kotlin的检查语法而已。另一个缺点就是，当需要添加新的类UnKnown类，也是实现Result接口，用来实现未知情况的处理，但是忘了修改getResultMsg方法，此时编译器也不会提示，而是进入else。 利用 sealed class 密封类是可以被继承的，因此需要添加括号。 此时getResultMsg方法可以写成 当when语句传入密封类时，kotlin会自动检查这个密封类有哪些子类，并强制要求每个子类对应的条件全部处理，这样能够保证没有else条件，也不可能出现漏写分支的情况。 扩展函数扩展函数：即使在不修改某个类的源码的情况下，仍然可以打开这个类，向该类添加新的函数。 语法结构： 情景：一段字符串可能有字母、数字、特殊符号，我们希望统计其中的字母数量 这是常见的实现过程，使用扩展函数 将lettersCount方法定义为String的扩展函数，他就自动带有了String上下文，不再需要接收参数了。定义好扩展函数之后，就可以直接这么用了 扩展函数能够让API更加简洁，String类是一个final类，任何一个类都不能继承他，但是在kotlin就不一样了，可以向String扩展任何函数，让他的api更加丰富。让编程更加简便。 运算符重载语法结构： Kotlin的运算符重载允许我们让任意两个对象进行相加 举个栗子:grin: Money类，默认赋值value 接下来重载实现两个Money相加 那么我想Money直接和数字相加呢，Kotlin是允许运算符多重重载的。 附一下运算符的对照表 扩展函数和运算符重载的应用这里比较好玩的东西就是这俩结合起来。 让String类型的字符串用上乘法表达式，让他重复n遍。 使用： kotlin也提供了重复n遍的repeat函数，因此可以这么写 "},{"title":"Kotlin笔记06(标准函数和静态方法)","date":"2021-05-04T15:32:13.000Z","url":"/2021/05/04/Kotlin%E7%AC%94%E8%AE%B006/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"标准函数with、run和applywith有两个参数，第一个参数传递一个对象，第二个参数是Lambda表达式，with会在lambda表达式中提供第一个参数对象的上下文，并且lambda表达式中最后一行代码作为返回值返回。 作用：在多次调用一个对象的时候，让代码变得更加简洁 使用with之后 runrun函数和with是差不多的。只不过run函数是不能被直接调用的，需要用某个对象进行调用，run也是接受一个Lambda，并传入调用对象的上下文。其他和with一样 applyapply和run是极其类似的，都是在某个对象上使用，都是接受一个Lambda，但是能指定返回值，只会返回调用对象本身 定义静态方法companion object静态方法对于java来说是很简单的一件事，就是在方法上声明一个static Kotlin弱化了静态方法，因为Kotlin提供了一个更好的语法特性，那就是单例类。比如这个工具类，Kotlin推荐的是单例写法 调用方式也是Util.doActioin() 但是如果我们只是希望一个类当中的某个方法变为静态方法怎么办呢？这就用到kotlin的companion object doAction2()方法其实也并不是静态方法，companioin object 这个关键字实际上会在Util类的内部创建一个伴生类，而doAction2()方法就是定义在这个伴生类里面的实例方法。只是Kotlin会保证Util类始终只会存在一个伴生类对象，因此调用Util.doAction2()实际上是调用了Util的伴生类对象的doAction2()方法 定义真正的静态方法Kotlin没有直接定义静态方法的关键字，只是提供了一些语法特征来支持类似静态方法的调用。因此用java调用Kotlin的静态方法时，会发现找不到这个方法。 这时就需要定义真正的静态方法了，两种方法：注解 和 顶层方法 注解 注意：@JvmStatic注解只能加在单例类或者companion object中的方法上。 顶层方法顶层方法：指的是那些没有定义在任何类当中的方法 Kotlin会将所有顶层方法全部编译成静态方法。首先创建一个Kotlin文件，比如Helper.kt文件，里面可以定义一个方法。 顶层方法都是编译成静态方法，那么怎么用它呢？ Kotlin当中：顶层方法可以在任意位置使用，直接调doSomething()就可以了。 java当中：因为java中没有顶层方法的概念，Kotlin编译器会自动创建一个 HelperKt的java类，doSomething就是以静态方法的形式定义在HelperKt类里面。这个类的名字就是 Kotlin文件的名字加上Kt。 tips: 看到这里的时候，我才发现定义真正静态方法的知识点，都忘了。之前代码里的笔记也没写过这部分。哈哈，这里就补上。:joy:"},{"title":"A*寻路算法，启发式搜索（超详细实现）","date":"2021-05-03T06:21:18.000Z","url":"/2021/05/03/AStar/","tags":[["算法","/tags/%E7%AE%97%E6%B3%95/"],["A*","/tags/A/"],["java","/tags/java/"],["寻路算法","/tags/%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"A*寻路算法&nbsp;&nbsp;&nbsp;&nbsp;关于A算法网上优秀的文章有很多，本篇只是参考了那些文章，我自己实现了A算法后，对A的个人理解，在此记录下A的实现过程，同时防止时间久了自己忘了，随时可以回来看。如有不对的地方，欢迎评论指正。 &nbsp;&nbsp;&nbsp;&nbsp;A启发式搜索，什么是启发式？就是给搜索的时候有一个参考，大致的方向，让搜索的时候有一定的方向性的去寻找，这样相比广度遍历要少一些搜索范围。那么A是如何做到启发式搜索，在我看来就是那个公式F = G + H。 G：当前点到起始点路径上的消耗，H：当前点到终点的距离。通过公式可以看出F是受到G和H的影响的，可以说F越小所得到的路径就越小，所以按照F的大小顺序来进行遍历，即可获得达到终点的最短路径。下面先上代码，具体详细实现步骤和思路在代码后面。 MapItem.java 这个只是记录每个格子的F值，G值，H值，前一个格子坐标用的，至于为什么记录前一个格子坐标，原因很简单，就是为了找到路径后，按照坐标回溯从而将路线展现出来。 Main.java Ok,全部代码就在这里了。 &nbsp;&nbsp;&nbsp;&nbsp;Openlist :开放列表，存放预选节点，也就是预选列表，选择下一个被遍历节点就是从这里面选的。 &nbsp;&nbsp;&nbsp;&nbsp;Closelist:关闭列表，存放已经遍历过的节点，也就是说已经确定了，被检查过周围节点的节点，确定了周围节点都是最优路线的，这个节点不能被重复遍历的。 &nbsp;&nbsp;&nbsp;&nbsp;关于F,G,H计算：F = G + H，G的话就是从开始到当前点的消耗，上下左右四个方向走一步距离为1，斜对角那么就是约为1.4，为了方便计算就都扩大10倍，10和14。其实不用那么严格，只要保证两边之和大于第三边即可，朝正方向的步长小于斜对角长度。 H的距离的话，愿意用勾股定理计算也可以，不过为了方便我还是用的x向的距离加上y向的距离。也可表示距离长度。没什么影响的。 大体思路：​ 将起始节点加入openlist当中，然后就进入循环查找路线。一个循环过程： 从openlist当中找F值为最小的节点作为父节点然后遍历周围的节点，若有F相同选择G最小的 将父节点从openlist中移除，加入到closelist当中 遍历父节点周围的节点，如果是墙壁或边界，跳过该节点。如果在closelist当中，跳过该节点。 如果该节点（父节点周围的节点）不在openlist当中，计算F,G,H,并记录父节点坐标，G值为父节点的G值加上到达当前点的步长消耗。（因为这种节点，第一次遍历到，那么到达它目前的最优路径就是通过父亲节点到达）。 如果该节点在openlist当中，说明他也是待选节点，他已经被遍历过了，那么他就有了选择，选择原先的父节点，还是当前的父节点，这取决于G值，因为此时H值是不会变的，G值越小消耗越小，F就会越小，路径会越短。那么F的大小决定了优先级，决定了该节点被当做父节点的优先级。F越小优先级越高，优先级越高被先遍历到的可能性越大。（这块就是启发式搜索，我个人感觉这个F起到优先级作用，改变了搜索的优先顺序，从而使得寻路更加的高效。） 重复以上步骤，直到openlist为空（为什么会空呢，当开始节点在一个被障碍物组成的封闭空间内，边界被跳过，closelist被跳过，每次循环都有节点放入close当中，那么封闭空间内节点终将会被遍历完，全部放入closelist中，自然就为空了），或者目标节点放入了openlist（找到路径）。 我写的时候一些担心和小问题： F值相同的情况 &nbsp;&nbsp;&nbsp;&nbsp;不用担心F值相同的节点，因为都相同的话，位置不同，每次遍历都是优先选择F最小的，终会遍历完所有的相同F的坐标，那么它如果离终点较远的话，要知道H的值是不会改变的，他的G值会变得很大，导致F也会变得很大，那么它周围的节点很大可能是不会被下一次循环选中作为父节点的。 遍历周围节点太麻烦 ​ 当初最早的时候还傻乎乎的用一堆if去判断方向，利用偏移量，这个思路我也忘了是从哪里学过来的。自己先定好一个遍历顺序，顺时针啊逆时针啊随你，然后按照顺序将x,y的偏移写到两个数组中去，反正就装起。遍历的时候，循环这个数组，只用写一个当前节点坐标加上偏移量即可，计算G值也很简单，由于遍历是有顺序的，那么就可以通过循环次数的奇偶性判断是对角线还是正方向，对应加上值即可。 以上就是思路，至于代码详细解释，emmm…代码里的注释应该写的比较清楚吧，havePointInCloseList(),havePointInOpenList(),就是判断节点是否在关闭列表和开启列表内用的。"},{"title":"BFS广度优先遍历寻找最短路径(超详细实现过程)","date":"2021-05-03T06:05:19.000Z","url":"/2021/05/03/bfs/","tags":[["算法","/tags/%E7%AE%97%E6%B3%95/"],["java","/tags/java/"],["寻路算法","/tags/%E5%AF%BB%E8%B7%AF%E7%AE%97%E6%B3%95/"],["bfs","/tags/bfs/"]],"categories":[["算法","/categories/%E7%AE%97%E6%B3%95/"]],"content":"广度优先遍历寻找最短路径&nbsp;&nbsp;&nbsp;&nbsp;最近一直想搞A算法，发现有部分没理解清楚。于是找到了广度优先遍历寻路算法学习了下，想看看可不可以对写A有什么帮助。广度优先遍历寻路算法本身并不难，概括来说就是像雷达一样，一层一层进行寻找目标点。当找到目标点后进行回溯。从而找到最佳路径。也就是说每走一步都要找到到达该点的最短的路径，最终得到到达所有点的最短路径。 废话不多说上代码。具体解释在代码后面 代码Point.java MyMap.java test.java 效果 类的解释： &nbsp;&nbsp;&nbsp;&nbsp;Point.java里面写的就是一个点的类里面就是装的x，y没啥说的。MyMap.java写的类就是记录前一个点的位置prex 和prey其实这两个参数用一个Point来记就好了。还有一个最重要的就是那个price，记录到达当前点所需要的消耗。也就是路径长度。然后就是test.java了。test当中就是bfs函数最重要了。 大体遍历的思路：&nbsp;&nbsp;&nbsp;&nbsp;将开始节点加入队列，然后在循环中先读出队列头，即出队列，读出的头就是当前节点，围绕该节点遍历周围的所有节点，分为：左上，上，右上，右，右下，下，左下，左共8个方向。然后将周围的节点依次加入到队列中，并且设置该节点的权值和前一节点坐标。不断循环重复以上操作，逐层遍历直到找到目的节点，或者队列为空，若队列为空都没有找到目标节点那么就是该节点不可达。 程序的超详细实现过程：&nbsp;&nbsp;&nbsp;&nbsp;首先为了方便起见设置一个char类型的二维数组当做地图，大小是10*10的。然后遍历找到起始点和结束点的位置坐标，Point类型的start和end。还有dx和dy两个数组。里面存的是遍历的顺序，也就是坐标的偏移量，两个数组每个都是8个元素，因为是8个方向的嘛。然后直接进重点bfs()函数，先建立一个queue队列，将开始节点加入队列，设置与地图等大的MyMap类的一个数组，并初始化。 &nbsp;&nbsp;&nbsp;&nbsp;开始遍历，一个while循环，循环条件是queue大小要大于0，循环体中则是读出队列头，出队列，一个for循环用来遍历该点周围8个方向的点，当然要有约束条件周围的点不能超过10*10的范围同时不能为墙壁，判断墙壁用的是之前char数组的地图，而遍历记录price和前一个点的坐标的是那个MyMap数组。queue出来的点为当前点，遍历的是周围点，周围的点因为是8个方向，当扩大的时候必然会存在重复遍历的问题，不能单纯的用一个布尔来标记是否重复遍历，因为如果遍历到就进行标记的话，得到的路径不一定是最短的。因此我用的是一个price来记录到达该点的路径长度，判断是否遍历过了也很简单，如果price为0的话那么说就是第一次遍历，若不为0那就说明不是第一次遍历。两种情况分开讨论。（1）第一次遍历到该点：因为是第一次遍历到，所以要先将其加入到队列中，然后将设置消耗price，就是获取当前点（队列里出来的点）的price加上两点间路径长度，两点间路径长度：斜对角是14，上下左右相邻是10。为什么这么设，其实就是勾股定理的出来乘以10，也可以不这么设定只要相邻的距离相加大于斜对角距离就可以，满足三角形两边之和大于第三边就可以。然后设置前一点坐标为当前点。（2）第二次遍历到该点：因为不是第一次了，所以这里就不用再将其加入队列中，这里要做的就是判断应不应该与这个点连线，怎么说呢，因为他已经有price记录了，说明他已经有主子了，那么此时就要判断他指向的主子称不称职。那他的price与自己的price加上距离进行比较，如果他的price较小说明主子很称职，也就是路径较短，如果自己的price加距离要小于他自己的price说明他当前的主子不称职，就改变他的前一点的标记，改成当前点的坐标，并改变price值。如此一来可以保证到达每个点都是最小权值，按照点记录的前一点坐标进行回溯即可得到到达点的最短路径。 发现节点重复后判断消耗，发现有更好的路径则改记录坐标和消耗值 然后找到路径直接回溯，更改地图，将走过的节点改为‘’符号，最后整体打印地图。即为最短路径。这个算法寻路效率与A算法相比还是比较低。不过还是挺好理解的。"},{"title":"Mysql数据转移Redis","date":"2021-05-03T05:58:57.000Z","url":"/2021/05/03/mysql2Redis/","tags":[["mysql","/tags/mysql/"],["redis","/tags/redis/"]],"categories":[["数据库","/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"]],"content":"​ 最近毕业设计上想加上一个redis，就必须要把MySQL里面的数据转移到redis当中。那么问题来了，关系型数据库的数据怎么转移到非关系型数据库中呢？然后百度MySQL数据迁移redis，然后果不其然，各个博客保持了高度统一，看那么几篇博客，几乎都是转载的，有原创的也很少。当然我也只是个小白，摸索了很久才成功了。 ​ 如果百度过了的同学对events_all_time这个表很熟悉吧，哈哈。我也不多说了，原文写的没问题，也不是说没问题，是思路没问题，语法也没问题，但是实际操作后会报错，这个报错原因也不知道为啥。我在Stack Overflow上面看到的解决方法不是用的redisprotocol。对于我这个小白来说，文章里面有的地方没有解释。看起来有点费劲，搞不明白为啥这么写。然后我这篇就是先解释下redisprotocol，再说下mysql转redis命令的方法，最后说下mysql里面多条数据以什么格式，怎么存到redis。（这里附一下那个不知道被转了多少遍的events_all_time 2333） 正文百度上的mysql迁移redis的方法，看不懂的同学，结合官网的解释，食用更佳哦。 ​ 用redis普通的客户端插入大量数据并不好，所以官方推荐的方法是生成一个符合redis协议的text文件，用redis统一去调用。text文件可以写redis命令，也可以写redis协议。协议格式如下 ​ 对应的 ‘\\r’ (ASCII码13),对应的 ‘\\n’ (ASCII码10)。然后那个和$符号的意义就是。号后面加数字，表示整个命令总共有多少个参数，包括命令本身。$后面加数字，表示对应的参数，有多少字节。 ​ 举个栗子 SET key1 value1 转化为redis的协议就是 *3\\r\\n$3\\r\\nSET\\r\\n$4\\r\\nkey1\\r\\n$6\\r\\nvalue1\\r\\n *3–&gt;有set,key1,value1三个 $3–&gt;后面SET有三个字节 $4–&gt;后面key1有4个字节 $6–&gt;后面value1有6个字节 是不是很简单^_^!!!!! 然后看看那个不知道被转了多少遍的events_to_redis.sql。明白了redis protocol之后再看这个代码，一目了然。作者的意图就是把数据库里面查询出的值，拼接成相应的命令。 最终用 mysql -h ‘ip地址’ -u’用户名’ -p’密码’ ‘database’ –skip-column-names –raw &lt; events_to_redis.sql | redis-cli –pipe 这条命令意思就是 mysql登录后，用‘database’这个数据库， –skip-column-names(使mysql输出不包含列名) –raw（使mysql不转换字段值中的换行符），然后就是运行events_to_redis.sql，用管道传入redis-cli当中运行即可。 这里有一个问题，就是按照 *3\\r\\n$3\\r\\nSET\\r\\n$4\\r\\nkey1\\r\\n$6\\r\\nvalue1\\r\\n这样的方式是不行的。总是报错 ERR Protocol error: expected ‘$’, got ‘ ‘ 具体原因我并不太清楚，如果有同学解决了，希望可以评论说一下，让我这个小白也学习一下。 解决方案 我最后的解决方案其实就是在前面了解了的基础上，把sql语句修改了，让mysql最终输出的格式是普通的 SET key value的格式，然后运行即可。 总而言之，不管你用的是mysql，java，python，C/C++，甚至是记事本。只要输出的格式是redis protocol或redis命令，传入redis-cli当中就可以完成这个任务。 mysql多条数据存入redis 然后对于mysql 的多条数据，我使用的是redis的hmset命令。 HMSET key field value [field value ..] key就是表名称+id，这样查询的时候进行简单处理即可得到对应数据表一条的数据。hmset 表名+id 列名 值 列名 值 … 其实用HSET也是可以的，key就是表名称+id，然后一条数据，可以用String存上，同样简单处理也可得到数据。"},{"title":"pip安装报错 HTTPSConnectionPool：Read timed out.","date":"2021-05-03T05:57:52.000Z","url":"/2021/05/03/pythonQs/","tags":[["python","/tags/python/"],["pip","/tags/pip/"]],"categories":[["python","/categories/python/"]],"content":"pip –default-timeout=1000 install +软件名 e.g. pip –default-timeout=1000 install mxnet-cu90"},{"title":"scheme打开手机内APP","date":"2021-05-03T05:52:43.000Z","url":"/2021/05/03/scheme%E6%89%93%E5%BC%80%E6%89%8B%E6%9C%BA%E5%86%85APP/","tags":[["html5","/tags/html5/"],["android","/tags/android/"]],"categories":[["Android","/categories/Android/"]],"content":"需求：网页上一个链接，点击后会唤醒手机内响应的app，打开指定APP的功能页面。 方法：在AndroidManifest.xml里面对需要打开的页面设置action，两个category, data 一会儿url请求格式是： scheme://host/path?传的参数 对应页面获取数据： kotlin java 本文章参考的博客： 本文也是看这个博客，记录一下。"},{"title":"Studio下载依赖慢","date":"2021-05-03T05:50:17.000Z","url":"/2021/05/03/Studio%E4%B8%8B%E8%BD%BD%E4%BE%9D%E8%B5%96%E6%85%A2/","tags":[["Android","/tags/Android/"],["app","/tags/app/"]],"categories":[["Android","/categories/Android/"]],"content":"在项目下的build.gradle(不是app下的)，把jcenter(),google()改成阿里云的即可 "},{"title":"Kotlin笔记05(空指针检查，小技巧)","date":"2021-05-03T05:45:17.000Z","url":"/2021/05/03/Kotlin%E7%AC%94%E8%AE%B005/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"空指针检查Android系统中，最常见的崩溃问题就是空指针异常，相信其他的系统或者程序，也会常常备受空指针问题的影响。这类问题只能由程序员自己主动通过逻辑判断来避免。可是，很难做到完全考虑到所有潜在的空指针异常。 Kotlin的做法则是将空指针异常检查提前到了编译期，在程序员写程序的时候，时刻能够检测出潜在的空指针问题，及时进行处理。 举例： java代码 这段代码java编译肯定没问题，但是他安全么？不一定。如果这个方法当中传入是一个null，将会导致空指针异常。 修改一下： 这样才会安全。 转到kotlin： 对于kotlin来说，这是没有问题的，没有空指针的风险。因为，在kotlin当中，默认所有参数和变量都不能为空，所以这里传进来的study一定不为空。可以放心的调用它，一旦传入null，在编译的时候直接报错。 可空类型系统 对于业务逻辑需要参数为null的话，kotlin提供的方法是，在参数类名后面加？号，但是函数里参数也是会被检测的，必须要有空的判断。 ?.操作符?.操作符：当对象不为空时正常调用，当为空时什么都不做。 ?:操作符?:操作符：左右两边都接收一个表达式，如果左边结果不为空就返回左边结果，否则返回右边表达式结果 相当于： !!.操作符!!.操作符：非空断言工具，告知kotlin这里的对象不会为空，不用它做非空检查。出错可以抛异常。 这个就和java的那个版本一样了，不安全了，照样运行报错了。 let函数obj对象调用let函数，obj本身会作为参数传递到Lambda表达式中，当列表只有一个参数时用it。 用法： 例子： let通常是和?.操作符结合，之前的study?.doHomework(),study?.readBooks()两个，相当于每次调用方法都需要对study做了if判空处理，相当麻烦。可以先study?. 如果study不为空就运行let，如果为空就什么都不做。此时let里面的lambda表达式当中的it肯定就不是空的了，因此let当中运行是安全的。 let的另一个优势： let函数时可以处理全局变量的判空问题，而if语句是无法做到的 Kotlin的小技巧字符串内嵌表达式Kotlin一开始就支持了字符串内嵌表达式，我们不再需要像java那样傻傻的拼接字符串了。 当表达式中仅有一个变量的时候，大括号可以省略 kotlin的这种方式，不论是易读性易写性方面都比java更胜一筹 函数的参数默认值简单来说就是，kotlin允许我们在定义函数的时候给任意参数设定一个默认值，这样当调用这个函数的时候不强求传此参数。 调用的时候可以 当然默认值也可以给第一个参数 那么填一个参数，就默认到num参数上了。问题不大，kotlin提供了键值对的方式来传参 "},{"title":"Kotlin笔记04(集合,Lambda表达式)","date":"2021-05-03T03:44:54.000Z","url":"/2021/05/03/Kotlin%E7%AC%94%E8%AE%B004/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"Kotlin的集合创建也是相当简单的。集合主要就是List和Set，Map这些的。 集合的创建与遍历List数组java的List 当然kotlin也是可以这么做的 这么多代码，很明显不是kotlin的作风，当然可以进行简化 listOf方法创建的是不可变的集合，需要可变的集合使用mutableListOf() Set集合和List一样 Map集合传统方式，Kotlin不推荐 kotlin推荐的方式 简化方式 这里的to看似是关键字进行关联，实际上to不是关键字，而是infix函数，后面可以自己写类似to的infix函数。也可以自己实现to的方式。 for-in遍历map集合 集合的函数式apiLambda表达式lambda表达式的语法结构： 首先外层大括号，如果有参数传入lambda表达式中，我们需要声明参数列表，参数列表之后 -&gt;符号，表示参数列表的结束以及函数体的开始，函数体可以写任意行代码，并以最后一行代码会作为lambda表达式的返回值。 maxBy函数从一个字符串数组当中找出最长的字符串。 第一时间想到的实现方法是： 如果用上集合的函数式api，可以让这件事变得更加简洁容易 结合maxBy，Lambda定义，还原简化过程 可以不用专门定义lambda Kotlin规定当Lambda参数，(即{fruit:String -&gt; fruit.length})是函数最后一个参数时，可以把Lambda放在括号外面 如果Lambda是函数的唯一一个参数，可以省略括号 利用kotlin的类型推到机制，Lambda中的参数列表，大多数情况不用声明参数类型 当只有一个参数时，可以不写参数名。可以用it关键字代替 map函数map即映射，他可以将每个元素映射成另一个值 filter函数filter函数，就是过滤 any函数any用于判断集合中是否至少存在一个元素满足条件。 all函数all用于判断集合中是否所有元素满足条件 Java函数式API的使用Kotlin中调用一个java方法，并且该方法接受一个java单抽象方法接口函数，就可以用函数式API 单抽象方法接口： 接口中只有一个待实现的方法 比如：Java中的子线程创建 Kotlin当中就是这么写的 由于kotlin舍去了new，因此匿名类实例的时候不能用new了，而是改用object。 进一步精简： 再利用上面的简化方法 其实kotlin提供了一个更简单的方法 这个thread可选参数有很多，可以自己点进去看，这个方法的实现很简单。完全可以不用说，看一眼他的实现就能明白怎么用的。"},{"title":"Kotlin笔记03(数据类，单例类)","date":"2021-05-03T02:04:26.000Z","url":"/2021/05/03/Kotlin%E7%AC%94%E8%AE%B003/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"常见的架构模式有MVP,MVC,MVVM不论是哪个架构，都有M，M就是Model（数据类）。 数据类与单例类数据类通常要写toString(), eauals()等方法。比如java写数据类 tips: java里面的Object ，在kotlin当中是 Any。 instanceof在kotlin中是 is 是不是感觉很长，好的，轮到kotlin了。简单到难以想象 结束了。就只有一句话。kotlin将那些toString，equals等等那些毫无实际逻辑的方法自动生成，大大减少工作量。仅仅需要在class前面加上data即可。’==’相当于equals，’===’就是比较哈希值看是不是同一个对象。 接下来可以测试下 单例模式java常见的单例模式 轮到kotlin，简单到可怕 ok了这个就是单例了，接下来添加方法 Kotlin当中不需要私有化构造函数，不用提供getInstance(),只要把class改为object关键字，单例类就创建完了。 使用： "},{"title":"Kotlin笔记01(变量，函数，逻辑控制)","date":"2021-05-02T05:23:01.000Z","url":"/2021/05/02/Kotlin%E7%AC%94%E8%AE%B001/","tags":[["Kotlin","/tags/Kotlin/"]],"categories":[["Kotlin","/categories/Kotlin/"]],"content":"​ 写过Android的童鞋，应该都听说过kotlin，谷歌也是极力推荐使用kotlin来写Android。kotlin有着很多的优点，比如：代码精简，空指针异常判断，协程的优势等等。我是看了郭霖大佬《第一行代码Android》第三版之后，当初笔记是写在代码里的，现在把代码重新梳理一遍，写到博客里，顺道照着书再复习一遍Android和kotlin。虽然很多公司仍然用的java写的Android，但是技多不压身嘛嘻嘻。这个笔记只是记录kotlin相关的学习。 Kotlin简介 kotlin是jetBrains公司开发与设计的，因此现在IntelliJ IDEA本身就加入了kotlin支持。 kotlin和java是可以共存的，像Android当中，可以同时拥有.kt文件也可以有.java文件，为什么呢？ 首先先搞明白java是 编译型语言 还是 解释型语言。虽然java是先编译后运行，但是他并不是编译型语言。因为java编译之后生成的是class文件，class文件则是放入java虚拟机识别，虚拟机担任的是解释器的角色，class文件解释成二进制数据让计算机执行 java虚拟机只是与class文件打交道，它不关心class文件是哪里来的，因此java与kotlin共存的原理也就了解了，不论是java还是kotlin他们编译生成的文件都是class文件。 第一个HelloKotlin 首先安装IDEA，这个直接官网下载就可以了。打开IDEA ，File-&gt;new Project，选择Kotlin，然后next，写项目名称，然后finish就可以啦。在项目目录下，src右击-&gt; New -&gt; Kotlin file/Class，填写文件名字，类型选择File即可。然后输入以上代码，第一个Hello Kotlin就ok了 变量和函数变量kotlin的变量和java有很大的不同，java中定义变量需要具体声明变量类型，而kotlin只有两种val和var。因为kotlin拥有很优秀的类型推导机制。 注意： kotlin代码是不用写分号的，其实写了也没关系，编译器直接无视掉了 kotlin的类型推导也不总是正常工作的，比如某个变量需要延迟赋值的话仍然需要显示声明变量类型才可以 注意：永远优先使用val，val无法满足需求的时候再改var。 在java中除非主动设定final，否则这个变量一直是可变的。当项目变得复杂的时候，永远不知道这个变量什么时候被谁改掉了，容易导致各种问题，kotlin强制要求必须让开发者主动声明变量是否可变。 函数 语法规则： fun 方法名(参数) : 返回值类型{} 参数格式：参数名:参数类型 无返回值就不写返回值类型，不用写return。 语法糖： 这也是我比较喜欢kotlin的一方面，可以少写很多东西(oﾟ▽ﾟ)o ，越往后学，就会发现很多看起来很复杂的代码，改到kotlin当中，轻轻松松没几行就完成了。 逻辑控制if条件语句kotlin的条件语句有两种实现方式 if 和 when kotlin的if和java的基本没区别 Kotlin的if语句相比java还有一个额外的功能，他是可以有返回值的 if语句将每个条件的最后一行代码作为返回值，并将返回值赋值给了value变量。最终将value变量返回。当然以上代码还可以更加精简。 继续简化 是不是觉得kotlin的简化爽的一批。。。 when语句会java的同学，应该了解switch语句吧，非常不好用，只能传整型变量最为条件，一个一个case比对，还必须每个case写break，十分不爽。那么看了kotlin的when语句解决了以上问题 写个例子：输入学生姓名，返回对应的成绩分数 上面使用了if判断学生姓名，返回分数，用了很多if，else，这个时候就可以考虑使用when了 清清爽爽，而且when语句和if一样也是可以有返回值的，因此也可以用语法糖。 when语句可以使用任意类型的参数，包括类型匹配 循环语句"}]