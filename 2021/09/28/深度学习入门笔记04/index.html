<!DOCTYPE html>
<html lang="zh-CN">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  <link rel="icon" href="">
  <title>深度学习入门笔记04 | 菠萝菠萝卜的博客</title>
  <meta name="author" content="菠萝菠萝卜" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="DeepLearning, numpy, python" />
  
  <meta name="description" content="神经网络的学习这章主要讲的是函数斜率的梯度法 计算机视觉领域常用的特征量包括 SIFT、SURF、HOG等。 训练数据和测试数据训练数据和测试数据 ： 首先使用训练数据进行学习，寻找最优的参数，然后使用测试数据评价训练得到的模型的实际能力。 为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。训练数据又称为监督数据。 过拟合：只对某个数据集过度拟合的状态。 损失函数损失函数：表示神经网络性能">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习入门笔记04">
<meta property="og:url" content="https://www.boluoboluobo.top/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/index.html">
<meta property="og:site_name" content="菠萝菠萝卜的博客">
<meta property="og:description" content="神经网络的学习这章主要讲的是函数斜率的梯度法 计算机视觉领域常用的特征量包括 SIFT、SURF、HOG等。 训练数据和测试数据训练数据和测试数据 ： 首先使用训练数据进行学习，寻找最优的参数，然后使用测试数据评价训练得到的模型的实际能力。 为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。训练数据又称为监督数据。 过拟合：只对某个数据集过度拟合的状态。 损失函数损失函数：表示神经网络性能">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.boluoboluobo.top/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/sdxx.webp">
<meta property="article:published_time" content="2021-09-28T12:02:02.000Z">
<meta property="article:modified_time" content="2021-09-30T13:47:40.247Z">
<meta property="article:author" content="菠萝菠萝卜">
<meta property="article:tag" content="python">
<meta property="article:tag" content="DeepLearning">
<meta property="article:tag" content="numpy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.boluoboluobo.top/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/sdxx.webp">
<meta name="twitter:site" content="@null">
  
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" type="text/css" media="all">
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/night-eighties.min.css" type="text/css" media="all">
  
  
  <link rel="stylesheet" id="fontawe-css" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css" media="all">
  <link rel="stylesheet" id="nprogress-css" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" type="text/css" media="all">
  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-dark.min.css" type="text/css" media="all">
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="https://cdn.jsdelivr.net/npm/qrcode_js@1.0.0/qrcode.min.js"></script>
  
<meta name="generator" content="Hexo 5.4.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                        <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
                                    
                                
                                    
                                        <li><a href="/archives/"><i class="fa fa-file"></i>档案馆</a></li>
                                    
                                
                                    
                                        <li>
                                            <a><i class="fa fa-link"></i>链接</a>
                                            <ul class="sub-menu">
                                                
                                                    
                                                
                                                    
                                                        <li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36571422">CSDN博客</a></li>
                                                    
                                                
                                            </ul>
                                        </li>
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">菠萝菠萝卜的博客</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>菠萝菠萝卜的博客</h2> <br />
                        <span></span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        <section class="col-md-8">
    <article>
        <div class="kratos-hentry kratos-post-inner clearfix">
            <header class="kratos-entry-header">
                <h1 class="kratos-entry-title text-center">深度学习入门笔记04</h1>
                
                <ul class="kratos-post-meta text-center">
                    <li><i class="fa fa-calendar"></i> 2021-09-28</li>
                    <li><i class="fa fa-user"></i> 作者 菠萝菠萝卜</li>
                    <li>
                        <i class="fa fa-edit"></i> 
                        
                        
                            ~10.64K
                        
                        字
                    </li>
                    
                        <li id="/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/" class="leancloud_visitors" data-flag-title="深度学习入门笔记04">
                            <i class="fa fa-eye"></i>
                            <span class="leancloud-visitors-count"> </span> 次阅读
                        </li>
                        
                    
                </ul>
            </header>
            <div class="kratos-post-content">
                <div id="expire-alert" class="alert alert-warning hidden" role="alert">
                    本文最后编辑于 <time datetime="1633009660247"></time> 前，其中的内容可能需要更新。
                </div>
                
                    <div class="kratos-post-inner-toc">
                        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">神经网络的学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.</span> <span class="toc-text">训练数据和测试数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.2.1.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">交叉熵误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mini-batch%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.3.</span> <span class="toc-text">mini-batch学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E5%AE%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9B%A0"><span class="toc-number">1.2.4.</span> <span class="toc-text">设定损失函数原因</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86"><span class="toc-number">1.3.</span> <span class="toc-text">数值微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0"><span class="toc-number">1.3.1.</span> <span class="toc-text">导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%AF%BC"><span class="toc-number">1.3.2.</span> <span class="toc-text">偏导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.3.3.</span> <span class="toc-text">梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="toc-number">1.3.4.</span> <span class="toc-text">梯度法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%B1%BB"><span class="toc-number">1.3.5.</span> <span class="toc-text">2层神经网络的类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mini-batch%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.3.6.</span> <span class="toc-text">mini batch的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%84%E4%BB%B7"><span class="toc-number">1.3.7.</span> <span class="toc-text">基于测试数据的评价</span></a></li></ol></li></ol></li></ol>
                    </div>
                
                <hr />
                <h1 id="神经网络的学习"><a href="#神经网络的学习" class="headerlink" title="神经网络的学习"></a>神经网络的学习</h1><p>这章主要讲的是函数斜率的梯度法</p>
<p>计算机视觉领域常用的特征量包括 SIFT、SURF、HOG等。</p>
<h2 id="训练数据和测试数据"><a href="#训练数据和测试数据" class="headerlink" title="训练数据和测试数据"></a>训练数据和测试数据</h2><p><strong>训练数据</strong>和<strong>测试数据</strong> ： 首先使用训练数据进行学习，寻找最优的参数，然后使用测试数据评价训练得到的模型的实际能力。</p>
<p>为了正确评价模型的<strong>泛化能力</strong>，就必须划分训练数据和测试数据。训练数据又称为<strong>监督数据</strong>。</p>
<p><strong>过拟合：</strong>只对某个数据集过度拟合的状态。</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p><strong>损失函数：</strong>表示神经网络性能的“恶劣程度”的指标，当前神经网络对监督数据再大多成都上不拟合，大多程度不一致。通常乘上一个负值，解释为“多大程度上不坏”，“性能上有多好“。 损失函数可以用任何函数，不过一般都用的均方误差或交叉熵</p>
<p>个人理解：就是神经网络的预测与实际的标签，相差距离的多少。</p>
<h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><p>公式：</p>
<script type="math/tex; mode=display">
E = \frac {1}{2} \sum_k (y_k - t_k)^2</script><p>yk : 神经网络的输出</p>
<p>tk : 监督数据</p>
<p>k  : 数据维数</p>
<p>python表示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error</span>(<span class="params">y,t</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.<span class="built_in">sum</span>((y - t)**<span class="number">2</span>)</span><br></pre></td></tr></table></figure>
<p>拿MNIST数据做一次尝试</p>
<p>假设：</p>
<p>预测数据 [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]</p>
<p>正确解(one_hot) [0,0,1,0,0,0,0,0,0,0]  #正确解为2</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">y = np.array([<span class="number">0.1</span>,<span class="number">0.05</span>,<span class="number">0.6</span>,<span class="number">0.0</span>,<span class="number">0.05</span>,<span class="number">0.1</span>,<span class="number">0.0</span>,<span class="number">0.1</span>,<span class="number">0.0</span>,<span class="number">0.0</span>])</span><br><span class="line"><span class="comment">#设2为正确解</span></span><br><span class="line">t = np.array([<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line"><span class="comment"># t这种将正确解表示1，其他为0 方法称为one-hot表示</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_squared_error</span>(<span class="params">y,t</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.5</span> * np.<span class="built_in">sum</span>((y - t)**<span class="number">2</span>)</span><br><span class="line"><span class="comment">#例子2为概率最高的情况：</span></span><br><span class="line">y2 = np.array([<span class="number">0.1</span>,<span class="number">0.05</span>,<span class="number">0.1</span>,<span class="number">0.0</span>,<span class="number">0.05</span>,<span class="number">0.1</span>,<span class="number">0.0</span>,<span class="number">0.6</span>,<span class="number">0.0</span>,<span class="number">0.0</span>])</span><br><span class="line"><span class="comment">#例1： “2”概率最高的情况</span></span><br><span class="line"><span class="built_in">print</span>(mean_squared_error(y,t))</span><br><span class="line"><span class="comment">#例2： "7"概率最高的情况</span></span><br><span class="line"><span class="built_in">print</span>(mean_squared_error(y2,t))</span><br></pre></td></tr></table></figure>
<p>out:</p>
<p>0.09750000000000003<br>0.5975</p>
<p>很明显显示第一个例子与监督数据更加吻合。</p>
<h3 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h3><p>公式：</p>
<script type="math/tex; mode=display">
E = -\sum_k t_klog(y_k)</script><p>符号解释，同上面</p>
<p>由于tk中只有正确解为1，其他为0，所以只用计算正确解标签的输出的自然对数。因此可知，交叉熵误差的值是由正确解标签所对应的输出的结果决定的</p>
<p>python实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y,t</span>):</span></span><br><span class="line">    delta = <span class="number">1e-7</span>  <span class="comment">#此处添加的一个delta微小值，为的是防止算log的时候得到负无限大的-inf</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + delta)) / y.shape[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<p>同样拿之前的假设做对比</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#y = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]</span></span><br><span class="line"><span class="comment">#t = [0,0,1,0,0,0,0,0,0,0] </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y,t</span>):</span></span><br><span class="line">    delta = <span class="number">1e-7</span>  <span class="comment">#此处添加的一个delta微小值，为的是防止算log的时候得到负无限大的-inf</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + delta)) / y.shape[<span class="number">0</span>]</span><br><span class="line"><span class="comment">## 在对之前的例子做计算</span></span><br><span class="line"><span class="comment">#例1： “2”概率最高的情况</span></span><br><span class="line"><span class="built_in">print</span>(cross_entropy_error(y,t))</span><br><span class="line"><span class="comment">#例2： "7"概率最高的情况</span></span><br><span class="line"><span class="built_in">print</span>(cross_entropy_error(y2,t))</span><br></pre></td></tr></table></figure>
<p>out:</p>
<p>0.0510825457099338<br>0.23025840929945457</p>
<h3 id="mini-batch学习"><a href="#mini-batch学习" class="headerlink" title="mini-batch学习"></a>mini-batch学习</h3><p>​    在机器学习使用训练数据进行学习，需要针对训练数据计算损失函数值，找出使这个值尽可能小的参数。计算损失函数的时候需要将所有训练数据作为对象。也就是说，如果有100个训练数据，需要把100个的损失函数的总和作为学习的指标。</p>
<p>​    以交叉熵为例：</p>
<script type="math/tex; mode=display">
E = -\frac {1}{N} \sum_n \sum_k t_{nk}log(y_{nk})</script><p>就只是把单个的损失函数数据扩大到了N份，最后还是需要除以N做一次正规化。就是求的“平均损失函数”。对于MNIST数据集有60000个训练数据，不可能每个都加上，这样花费的时间较多。因此会从中随机选择100笔，再用100笔数据进行学习。这就是<strong>mini-batch学习</strong></p>
<p>python实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> mnist <span class="keyword">import</span> load_mnist</span><br><span class="line">(x_train,t_train),(x_test,t_test) = load_mnist(normalize=<span class="literal">True</span>,one_hot_label=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">batch_mask = np.random.choice(train_size,batch_size)</span><br><span class="line">x_batch = x_train[batch_mask]</span><br><span class="line">t_batch = t_train[batch_mask]</span><br></pre></td></tr></table></figure>
<p>np.random.choice() 从指定的数字中随机选择想要的数字。</p>
<p><strong>mini-batch交叉熵的实现</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y,t</span>):</span></span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>,t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>,y.size)</span><br><span class="line">        </span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(t * np.log(y + <span class="number">1e-7</span>)) / batch_size</span><br></pre></td></tr></table></figure>
<p>另一种实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cross_entropy_error</span>(<span class="params">y, t</span>):</span></span><br><span class="line">    <span class="keyword">if</span> y.ndim == <span class="number">1</span>:</span><br><span class="line">        t = t.reshape(<span class="number">1</span>, t.size)</span><br><span class="line">        y = y.reshape(<span class="number">1</span>, y.size)</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 监督数据是one-hot-vector的情况下，转换为正确解标签的索引</span></span><br><span class="line">    <span class="keyword">if</span> t.size == y.size:</span><br><span class="line">        t = t.argmax(axis=<span class="number">1</span>)</span><br><span class="line">             </span><br><span class="line">    batch_size = y.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(np.log(y[np.arange(batch_size), t] + <span class="number">1e-7</span>)) / batch_size</span><br></pre></td></tr></table></figure>
<h3 id="设定损失函数原因"><a href="#设定损失函数原因" class="headerlink" title="设定损失函数原因"></a>设定损失函数原因</h3><p>为什么不以精度为指标？</p>
<p>为了使损失函数的值尽可能小，需要计算参数的导数(梯度),以导数为指引，逐步更新参数值。</p>
<p>​    对权重参数的损失函数求导，表示对如果稍微改变这个权重的值，损失函数的值该如何变化。</p>
<p>​    如果为负： 通过改变权重向正方向改变，即可减小损失函数的值。</p>
<p>​    如果为正： 通过改变权重向负方向改变，即可减少损失函数的值。</p>
<p>如果用精度作为指标，大多数地方导数变为0，无法更新。</p>
<h2 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a>数值微分</h2><h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p>就和高数讲的一样</p>
<script type="math/tex; mode=display">
\frac {df(x)}{dx} = \frac {f(x + h) - f(x)} {h}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_diff</span>(<span class="params">f,x</span>):</span></span><br><span class="line">    h = <span class="number">1e-4</span></span><br><span class="line">    <span class="keyword">return</span> (f(x + h) - f(x - h)) / (<span class="number">2</span> * h)</span><br></pre></td></tr></table></figure>
<h3 id="偏导"><a href="#偏导" class="headerlink" title="偏导"></a>偏导</h3><p>也是和高数一样</p>
<p>假设公式：</p>
<script type="math/tex; mode=display">
f(x_0,x_1) = x_0^2 + x_1^2</script><p>偏导</p>
<script type="math/tex; mode=display">
\frac {\sigma f}{\sigma x_0} = 2x_0 \\\\
\frac {\sigma f}{\sigma x_1} = 2x_1</script><h3 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h3><p><strong>梯度：</strong>由全部变量的偏导汇总而成的向量</p>
<p>e.g</p>
<script type="math/tex; mode=display">
(\frac {\sigma f}{\sigma x_0},\frac {\sigma f}{\sigma x_1})</script><p>python实现也很简单</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment">#梯度</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">numerical_gradient</span>(<span class="params">f,x</span>):</span></span><br><span class="line">    h = <span class="number">1e-4</span></span><br><span class="line">    grad = np.zeros_like(x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(x.size):</span><br><span class="line">        tmp_val = x[idx]</span><br><span class="line">        </span><br><span class="line">        x[idx] = tmp_val + h</span><br><span class="line">        fxh1 = f(x)</span><br><span class="line">        </span><br><span class="line">        x[idx] = tmp_val - h</span><br><span class="line">        fxh2 = f(x)</span><br><span class="line">        </span><br><span class="line">        grad[idx] = (fxh1 - fxh2) / (<span class="number">2</span> * h)</span><br><span class="line">        x[idx] = tmp_val</span><br><span class="line">    <span class="keyword">return</span> grad</span><br></pre></td></tr></table></figure>
<h3 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h3><p><strong>梯度法：</strong>通过不断地沿梯度方向前进逐渐减小函数值的过程</p>
<p>寻找最小值是<strong>梯度下降法</strong>，寻找最大值是<strong>梯度上升法</strong></p>
<p>数学表达：</p>
<script type="math/tex; mode=display">
x_0 = x_0 - \eta \frac {\sigma f}{\sigma x_0} \\\\ 
x_1 = x_1 - \eta \frac {\sigma f}{\sigma x_1}</script><p>其中η是更新量，在机器学习中是<strong>学习率</strong></p>
<p>学习率是实现设定的值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradient_descent</span>(<span class="params">f,init_x,lr = <span class="number">0.1</span>, step_num = <span class="number">100</span></span>):</span></span><br><span class="line">    x = init_x</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(step_num):</span><br><span class="line">        grad = numerical_gradient(f, x)</span><br><span class="line">        x -= lr * grad</span><br><span class="line">    <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>init_x 是初始值， lr是学习率，step_num 是重复次数</p>
<p>现在可以尝试求解 f(x0,x1) = x0^2 + x1 ^2 的最小值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#####测试</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun1</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x**<span class="number">2</span> + <span class="number">10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun2</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> x[<span class="number">0</span>]**<span class="number">2</span> + x[<span class="number">1</span>]**<span class="number">2</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'微分'</span>)</span><br><span class="line">    <span class="built_in">print</span>(numerical_diff(fun1, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">### 由全部变量的偏导汇总成的向量称为 梯度</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"梯度"</span>)</span><br><span class="line">    <span class="built_in">print</span>(numerical_gradient(fun2, np.array([<span class="number">3.0</span>,<span class="number">4.0</span>])))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'梯度下降法求 f(x0,x1) = x0^2 + x1^2 最小值'</span>)</span><br><span class="line">    <span class="built_in">print</span>(gradient_descent(fun2, np.array([-<span class="number">0.3</span>,<span class="number">4.0</span>])))</span><br></pre></td></tr></table></figure>
<p>out:</p>
<p>微分<br>2.0000000000042206<br>梯度<br>[6. 8.]<br>梯度下降法求 f(x0,x1) = x0^2 + x1^2 最小值<br>[-6.11110793e-11  8.14814391e-10]</p>
<p>结果是十分接近（0,0）点，而事实上最低点就是（0,0）点</p>
<p>学习率这样的参数称为<strong>超参数</strong>。它和权重参数是不同的，权重是可以通过数据和学习自动获得的。学习率这样的超参数是人工设定的。</p>
<h3 id="2层神经网络的类"><a href="#2层神经网络的类" class="headerlink" title="2层神经网络的类"></a>2层神经网络的类</h3><p>通过梯度下降法更新参数，由于数据是随机选择的mini batch数据，又称之为<strong>随机梯度下降法</strong>（stochastic gradient descent）简称<strong>SGD</strong></p>
<p>写一个名为TwoLayerNet类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayersNet</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self,input_size,hidden_size,output_size,weight_init_std = <span class="number">0.01</span></span>):</span></span><br><span class="line">        self.params = {}</span><br><span class="line">        self.params[<span class="string">"W1"</span>] = weight_init_std * np.random.randn(input_size,hidden_size)</span><br><span class="line">        self.params[<span class="string">"b1"</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">"W2"</span>] = weight_init_std * np.random.randn(hidden_size,output_size)</span><br><span class="line">        self.params[<span class="string">"b2"</span>] = np.zeros(output_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self,x</span>):</span></span><br><span class="line">        W1,W2 = self.params[<span class="string">"W1"</span>],self.params[<span class="string">"W2"</span>]</span><br><span class="line">        b1,b2 = self.params[<span class="string">"b1"</span>],self.params[<span class="string">"b2"</span>]</span><br><span class="line">        </span><br><span class="line">        a1 = np.dot(x,W1) + b1</span><br><span class="line">        z1 = sigmoid(a1)</span><br><span class="line">        a2 = np.dot(z1,W2) + b2</span><br><span class="line">        y = softmax(a2)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span>(<span class="params">self,x,t</span>):</span></span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        <span class="keyword">return</span> cross_entropy_error(y,t)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">self,x,t</span>):</span></span><br><span class="line">        y = self.predict(x)</span><br><span class="line">        y = np.argmax(y,axis=<span class="number">1</span>)</span><br><span class="line">        t = np.argmax(t,axis=<span class="number">1</span>)</span><br><span class="line">        accuracy = np.<span class="built_in">sum</span>(y == t) / <span class="built_in">float</span>(x.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numerical_gradient</span>(<span class="params">self,x,t</span>):</span></span><br><span class="line">        loss_W = <span class="keyword">lambda</span> W: self.loss(x, t)</span><br><span class="line">        grads = {}</span><br><span class="line">        grads[<span class="string">"W1"</span>] = numerical_gradient(loss_W, self.params[<span class="string">'W1'</span>])</span><br><span class="line">        grads[<span class="string">"W2"</span>] = numerical_gradient(loss_W, self.params[<span class="string">'W2'</span>])</span><br><span class="line">        grads[<span class="string">"b1"</span>] = numerical_gradient(loss_W, self.params[<span class="string">'b1'</span>])</span><br><span class="line">        grads[<span class="string">"b2"</span>] = numerical_gradient(loss_W, self.params[<span class="string">'b2'</span>])</span><br><span class="line">        <span class="keyword">return</span> grads</span><br><span class="line">    <span class="comment">#################此处以下是优化算法，提高速度（目前不重要）#############################</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gradient</span>(<span class="params">self, x, t</span>):</span></span><br><span class="line">        W1, W2 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'W2'</span>]</span><br><span class="line">        b1, b2 = self.params[<span class="string">'b1'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">        grads = {}</span><br><span class="line">        </span><br><span class="line">        batch_num = x.shape[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># forward</span></span><br><span class="line">        a1 = np.dot(x, W1) + b1</span><br><span class="line">        z1 = sigmoid(a1)</span><br><span class="line">        a2 = np.dot(z1, W2) + b2</span><br><span class="line">        y = softmax(a2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># backward</span></span><br><span class="line">        dy = (y - t) / batch_num</span><br><span class="line">        grads[<span class="string">'W2'</span>] = np.dot(z1.T, dy)</span><br><span class="line">        grads[<span class="string">'b2'</span>] = np.<span class="built_in">sum</span>(dy, axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        da1 = np.dot(dy, W2.T)</span><br><span class="line">        dz1 = sigmoid_grad(a1) * da1</span><br><span class="line">        grads[<span class="string">'W1'</span>] = np.dot(x.T, dz1)</span><br><span class="line">        grads[<span class="string">'b1'</span>] = np.<span class="built_in">sum</span>(dz1, axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> grads</span><br></pre></td></tr></table></figure>
<p>首先看__init__方法，input_size，hidden_size，output_size。依次表示的是输入层神经元数、隐藏层神经元数、输出层神经元数。input_size是784，因为输入的图像是（28x28）的，output_size也就是对应输出层，总共10个类型，所以是10。中间隐藏层是设定合适的值即可。书上设定的50个，我设定的100个，感觉效果比50的要好一点。权重使用的高斯分布的随机数进行初始化。</p>
<p>​    预测就是和之前的神经元一样，两层。乘上权重加上偏置，过一次sigmoid，然后进入二层，乘上权重加上偏置，过softmax，看概率。</p>
<p>​    损失函数，用的mini batch的交叉熵。</p>
<p>​    梯度下降，用的偏导，这个地方也是很耗时的，所以最终使用的时候用下边的gradient，使用的误差反向传播算法。</p>
<h3 id="mini-batch的实现"><a href="#mini-batch的实现" class="headerlink" title="mini batch的实现"></a>mini batch的实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mnist使用.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> 两层神经网络的类 <span class="keyword">import</span> TwoLayersNet</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">(x_train,t_train),(x_test,t_test) = load_mnist(normalize=<span class="literal">True</span>,one_hot_label=<span class="literal">True</span>,flatten = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_loss_list = []</span><br><span class="line"><span class="comment">#超参数</span></span><br><span class="line">iters_num = <span class="number">10000</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">network = TwoLayersNet(input_size = <span class="number">784</span>, hidden_size = <span class="number">100</span>, output_size = <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    <span class="comment">#获取mini-batch</span></span><br><span class="line">    batch_mask = np.random.choice(train_size,batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#计算梯度</span></span><br><span class="line">    <span class="comment">#grad = network.numerical_gradient(x_batch, t_batch)</span></span><br><span class="line">    grad = network.gradient(x_batch, t_batch)</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">'W1'</span>,<span class="string">'b1'</span>,<span class="string">'W2'</span>,<span class="string">'b2'</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">        </span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br></pre></td></tr></table></figure>
<p>整个就是梯度下降实现了，先设定好超参数，循环10000次，每次循环都是随机抽取batch_size个训练数据，扔到梯度下降的函数中去。获得偏导的向量，然后依据偏导向量乘上学习率，一点点修改权值。最终查看loss_list会发现，损失值在不断减小。</p>
<h3 id="基于测试数据的评价"><a href="#基于测试数据的评价" class="headerlink" title="基于测试数据的评价"></a>基于测试数据的评价</h3><p>这里引入一个新词</p>
<p><strong>epoch : </strong>epoch是一个单位。一个epoch表示学习中所有训练数据均被使用过一次时更新数据。如10000个数据，用大小为100笔数据的mini batch训练学习时，重复SGD100次，所有的数据都被“看过”则是一个epoch。更正确的做法是，10000个数据全部打乱，然后100个一组训练，顺序训练100组。为一个epoch。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mnist使用.mnist <span class="keyword">import</span> load_mnist</span><br><span class="line"><span class="keyword">from</span> 两层神经网络的类 <span class="keyword">import</span> TwoLayersNet</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line">(x_train,t_train),(x_test,t_test) = load_mnist(normalize=<span class="literal">True</span>,one_hot_label=<span class="literal">True</span>,flatten = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_loss_list = []</span><br><span class="line"></span><br><span class="line">train_acc_list = []</span><br><span class="line">test_acc_list = []</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iters_num = <span class="number">50000</span></span><br><span class="line">train_size = x_train.shape[<span class="number">0</span>]</span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">learning_rate = <span class="number">0.1</span></span><br><span class="line">iter_per_epoch = <span class="built_in">max</span>(train_size / batch_size,<span class="number">1</span>)</span><br><span class="line">network = TwoLayersNet(input_size = <span class="number">784</span>, hidden_size = <span class="number">100</span>, output_size = <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(iter_per_epoch)</span><br><span class="line"><span class="built_in">print</span>(train_size)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters_num):</span><br><span class="line">    <span class="comment">#获取mini-batch</span></span><br><span class="line">    batch_mask = np.random.choice(train_size,batch_size)</span><br><span class="line">    x_batch = x_train[batch_mask]</span><br><span class="line">    t_batch = t_train[batch_mask]</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#计算梯度</span></span><br><span class="line">    <span class="comment">#grad = network.numerical_gradient(x_batch, t_batch)</span></span><br><span class="line">    grad = network.gradient(x_batch, t_batch)</span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> (<span class="string">'W1'</span>,<span class="string">'b1'</span>,<span class="string">'W2'</span>,<span class="string">'b2'</span>):</span><br><span class="line">        network.params[key] -= learning_rate * grad[key]</span><br><span class="line">        </span><br><span class="line">    loss = network.loss(x_batch, t_batch)</span><br><span class="line">    train_loss_list.append(loss)</span><br><span class="line">    <span class="keyword">if</span> i % iter_per_epoch == <span class="number">0</span>:</span><br><span class="line">        train_acc = network.accuracy(x_train, t_train)</span><br><span class="line">        test_acc = network.accuracy(x_test, t_test)</span><br><span class="line">        train_acc_list.append(train_acc)</span><br><span class="line">        test_acc_list.append(test_acc)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"train acc, test acc | "</span> + <span class="built_in">str</span>(train_acc) + <span class="string">", "</span> + <span class="built_in">str</span>(test_acc))</span><br><span class="line"><span class="comment">#print(train_loss_list)</span></span><br><span class="line"><span class="comment"># 绘制图形</span></span><br><span class="line">markers = {<span class="string">'train'</span>: <span class="string">'o'</span>, <span class="string">'test'</span>: <span class="string">'s'</span>}</span><br><span class="line">x = np.arange(<span class="built_in">len</span>(train_acc_list))</span><br><span class="line">plt.plot(x, train_acc_list, label=<span class="string">'train acc'</span>)</span><br><span class="line">plt.plot(x, test_acc_list, label=<span class="string">'test acc'</span>, linestyle=<span class="string">'--'</span>)</span><br><span class="line">plt.xlabel(<span class="string">"epochs"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"accuracy"</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">1.0</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">data_output = <span class="built_in">open</span>(<span class="string">'mnistData.pkl'</span>,<span class="string">'wb'</span>)</span><br><span class="line">pickle.dump(network, data_output)</span><br><span class="line">data_output.close()</span><br></pre></td></tr></table></figure>
<p>加了epoch的代码，后面有绘制图形。也有将训练好后将网络保存。方便随时读出，用opencv + numpy做识别手写数字。</p>
<p><img src="/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/pic2.jpg" alt></p>
<p><img src="/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/pic1.jpg" alt></p>

            </div>
            
                <div class="kratos-copyright text-center clearfix">
                    <h5>本作品采用 <a rel="license nofollow" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 国际许可协议</a> 进行许可</h5>
                </div>
            
            <footer class="kratos-entry-footer clearfix">
                
                    <div class="post-like-donate text-center clearfix" id="post-like-donate">
                    
                        <a class="donate" href="javascript:;"><i class="fa fa-bitcoin"></i> 打赏</a>
                    
                    
                        <a class="share" href="javascript:;"><i class="fa fa-share-alt"></i> 分享</a>
                        <div class="share-wrap" style="display: none;">
    <div class="share-group">
        <a href="javascript:;" class="share-plain qq" onclick="share('qq');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-qq"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain qzone" onclick="share('qzone');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-star"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weixin pop style-plain" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weixin"></i>
            </div>
            <div class="share-int">
                <div class="qrcode" id="wechat-qr"></div>
                <p>打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮</p>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weibo" onclick="share('weibo');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weibo"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain facebook style-plain" onclick="share('facebook');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-facebook"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain twitter style-plain" onclick="share('twitter');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-twitter"></i>
            </div>
        </a>
    </div>
    <script type="text/javascript">
        $(()=>{
            new QRCode("wechat-qr", {
                text: "https://www.boluoboluobo.top/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/",
                width: 150,
                height: 150,
                correctLevel : QRCode.CorrectLevel.H
            });
        });
        function share(dest) {
            const qqBase        = "https://connect.qq.com/widget/shareqq/index.html?";
            const weiboBase     = "https://service.weibo.com/share/share.php?";
            const qzoneBase     = "https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?";
            const facebookBase  = "https://www.facebook.com/sharer/sharer.php?";
            const twitterBase   = "https://twitter.com/intent/tweet?";
            const hostUrl       = "https://www.boluoboluobo.top/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/";
            const title         = "「深度学习入门笔记04」";
            const excerpt       = `神经网络的学习这章主要讲的是函数斜率的梯度法
计算机视觉领域常用的特征量包括 SIFT、SURF、HOG等。
训练数据和测试数据训练数据和测试数据 ： 首先使用训练数据进行学习，寻找最优的参数，然后使用测试数据评价训练得到的模型的实际...`;
            let _URL;
            switch (dest) {
                case "qq"       : _URL = qqBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";     break;
                case "weibo"    : _URL = weiboBase+"url="+hostUrl+"&title="+title+excerpt;                                 break;
                case "qzone"    : _URL = qzoneBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";  break;
                case "facebook" : _URL = facebookBase+"u="+hostUrl;                                                        break;
                case "twitter"  : _URL = twitterBase+"text="+title+excerpt+"&url="+hostUrl;                                break;
            }
            window.open(_URL);
        };
    </script>
</div>
                    
                    </div>
                
                <div class="footer-tag clearfix">
                    <div class="pull-left">
                    <i class="fa fa-tags"></i>
                        <a class="tag-none-link" href="/tags/DeepLearning/" rel="tag">DeepLearning</a>, <a class="tag-none-link" href="/tags/numpy/" rel="tag">numpy</a>, <a class="tag-none-link" href="/tags/python/" rel="tag">python</a>
                    </div>
                    <div class="pull-date">
                    <span>最后编辑：2021-09-30</span>
                    </div>
                </div>
            </footer>
        </div>
        
            <nav class="navigation post-navigation clearfix" role="navigation">
                
                <div class="nav-previous clearfix">
                    <a title=" pytorch入门笔记01" href="/2021/09/25/pytorch入门笔记01/">&lt; 上一篇</a>
                </div>
                
                
                <div class="nav-next clearfix">
                    <a title=" 深度学习入门笔记05" href="/2021/09/28/深度学习入门笔记05/">下一篇 &gt;</a>
                </div>
                
            </nav>
        
        
            <div id="v-comments" class="post-comments"></div>
<script>
    var load_comm = () => {
        const init = () => {
            new Valine({
                el: '#v-comments',
                appId: 'pnDNVddr900afG7XglGQgJCb-gzGzoHsz',
                appKey: 'HA3J9tdvziBrLcSml8Kai0qH',
                visitor: true,
                enableQQ: false,
                path: '/2021/09/28/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B004/',
				avatar: ''
            });
        }
        if (typeof Valine == 'undefined') {
            const src = 'https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js';
            $.getScript(src, init);
        } else {
            init();
        }
    };
</script>
<noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="https://valine.js.org/">comments powered by Valine.</a></noscript>

        
    </article>
</section>

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/avatar.jpg" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center">上上下下，左右左右，BABA！</p>
    </div>
</aside>
            
                    <div class="sticky-area">
                
                    <aside id="krw-toc" class="widget widget-kratos-toc clearfix">
    <div class="photo-background"></div>
    <h4 class="widget-title no-after">
        <i class="fa fa-compass"></i>
        文章目录
        <span class="toc-progress-bar"></span>
    </h4>
    <div class="textwidget">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.</span> <span class="toc-text">神经网络的学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE"><span class="toc-number">1.1.</span> <span class="toc-text">训练数据和测试数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.2.1.</span> <span class="toc-text">均方误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AE"><span class="toc-number">1.2.2.</span> <span class="toc-text">交叉熵误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mini-batch%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.2.3.</span> <span class="toc-text">mini-batch学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E5%AE%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%8E%9F%E5%9B%A0"><span class="toc-number">1.2.4.</span> <span class="toc-text">设定损失函数原因</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E5%BE%AE%E5%88%86"><span class="toc-number">1.3.</span> <span class="toc-text">数值微分</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%BC%E6%95%B0"><span class="toc-number">1.3.1.</span> <span class="toc-text">导数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%AF%BC"><span class="toc-number">1.3.2.</span> <span class="toc-text">偏导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.3.3.</span> <span class="toc-text">梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B3%95"><span class="toc-number">1.3.4.</span> <span class="toc-text">梯度法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%B1%BB"><span class="toc-number">1.3.5.</span> <span class="toc-text">2层神经网络的类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mini-batch%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="toc-number">1.3.6.</span> <span class="toc-text">mini batch的实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%84%E4%BB%B7"><span class="toc-number">1.3.7.</span> <span class="toc-text">基于测试数据的评价</span></a></li></ol></li></ol></li></ol>
    </div>
</aside>
                
                
  <aside id="krw-categories" class="widget widget-kratos-categories clearfix">
    <h4 class="widget-title"><i class="fa fa-folder"></i>分类目录</h4>
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DeepLearning/">DeepLearning</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kotlin/">Kotlin</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/OpenGL/">OpenGL</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/python/">python</a><span class="category-list-count">7</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8D%9A%E6%96%87/">博文</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9C/">经典网络</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/">论文学习</a><span class="category-list-count">2</span></li></ul>
  </aside>


            
                
  <aside id="krw-tags" class="widget widget-kratos-tags clearfix">
    <h4 class="widget-title"><i class="fa fa-tags"></i>标签聚合</h4>
      <div class="tag-clouds">
        <a href="/tags/A/" style="font-size: 0.6em;">A*</a> <a href="/tags/Android/" style="font-size: 0.6em;">Android</a> <a href="/tags/C-C/" style="font-size: 0.6em;">C/C++</a> <a href="/tags/CycleGan/" style="font-size: 0.6em;">CycleGan</a> <a href="/tags/DeepLearning/" style="font-size: 0.8em;">DeepLearning</a> <a href="/tags/Kotlin/" style="font-size: 0.8em;">Kotlin</a> <a href="/tags/OpenGL/" style="font-size: 0.67em;">OpenGL</a> <a href="/tags/TensorFlow/" style="font-size: 0.73em;">TensorFlow</a> <a href="/tags/android/" style="font-size: 0.6em;">android</a> <a href="/tags/app/" style="font-size: 0.6em;">app</a> <a href="/tags/bfs/" style="font-size: 0.6em;">bfs</a> <a href="/tags/cgan/" style="font-size: 0.6em;">cgan</a> <a href="/tags/conditional-GAN/" style="font-size: 0.6em;">conditional GAN</a> <a href="/tags/gan/" style="font-size: 0.6em;">gan</a> <a href="/tags/hexo/" style="font-size: 0.6em;">hexo</a> <a href="/tags/html5/" style="font-size: 0.6em;">html5</a> <a href="/tags/java/" style="font-size: 0.67em;">java</a> <a href="/tags/kaggle/" style="font-size: 0.6em;">kaggle</a>
      </div>
  </aside>

            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>最新文章</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
            <a class="list-group-item" href="/2021/12/10/%E5%82%85%E9%87%8C%E5%8F%B6%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"><i class="fa  fa-book"></i> 傅里叶基础学习笔记</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/12/03/LaMa%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/"><i class="fa  fa-book"></i> LaMa论文学习</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/12/02/GoogleNet%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E7%8E%B0/"><i class="fa  fa-book"></i> GoogleNet学习及实现</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/11/23/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"><i class="fa  fa-book"></i> 模型评估</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/11/15/%E7%99%BD%E5%AB%96googleGPU%E8%B5%84%E6%BA%90%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"><i class="fa  fa-book"></i> 白嫖googleGPU资源训练模型</a>
            
          
        
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  class="ap-lrc"  >
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        
                        
                        
                        
                        
                        
                        
                        
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2021 菠萝菠萝卜的博客 版权所有.</li>
                            <li>本站已运行<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by 菠萝菠萝卜.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            <li>Hosted on <a href="https://github.io" target="_blank">Github Pages</a></li>
                        </div>
                        <div>
                            
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.js"></script>
<script>const notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));</script>

    <div>
        <canvas id="snow"></canvas>
        <script async type="text/javascript" src="/js/snow.min.js"></script>
    </div>

<script async src="/js/candy.min.js"></script>

    <script defer src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>
    <meting-js
        server="netease"
        type="playlist"
        id="1469580721"
        order="random"
        fixed="true"
    >
    </meting-js>



    <script defer src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script>
<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>

    <script defer src="https://cdn.jsdelivr.net/npm/layui-src@2.5.5/dist/layui.all.js"></script>


    <script defer src="/js/kr-dark.min.js"></script>



<!-- Extra support for third-party plguins  -->


    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<!-- <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"log":false});</script></body>
</html>