<!DOCTYPE html>
<html lang="zh-CN">
    <head>
  <!-- 元数据 -->
  <meta charset="utf-8">
  <link rel="icon" href="">
  <title>pytorch常用方法记录 | 菠萝菠萝卜的博客</title>
  <meta name="author" content="菠萝菠萝卜" />
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="robots" content="index,follow" />
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <meta name="format-detection" content="telphone=no, email=no" />
  
    <meta name="keywords" content="python, pytorch" />
  
  <meta name="description" content="pytorch常用方法记录 本篇记录在学习使用pytorch时，常用的东西记录，以便随时查看。遇到新学到的会更新在这里。 常用损失函数交叉熵CrossEntropyLoss计算公式：  E &#x3D; -\sum_k t_klog(y_k)这个公式在深度学习入门04中讲过。 python代码： 12cross_entropy_loss &#x3D; torch.CrossEntropyLoss()loss &#x3D; cr">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch常用方法记录">
<meta property="og:url" content="https://www.boluoboluobo.top/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/index.html">
<meta property="og:site_name" content="菠萝菠萝卜的博客">
<meta property="og:description" content="pytorch常用方法记录 本篇记录在学习使用pytorch时，常用的东西记录，以便随时查看。遇到新学到的会更新在这里。 常用损失函数交叉熵CrossEntropyLoss计算公式：  E &#x3D; -\sum_k t_klog(y_k)这个公式在深度学习入门04中讲过。 python代码： 12cross_entropy_loss &#x3D; torch.CrossEntropyLoss()loss &#x3D; cr">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://www.boluoboluobo.top/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/pytorch.jpg">
<meta property="article:published_time" content="2021-11-03T07:19:07.000Z">
<meta property="article:modified_time" content="2022-01-05T13:32:09.917Z">
<meta property="article:author" content="菠萝菠萝卜">
<meta property="article:tag" content="python">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.boluoboluobo.top/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/pytorch.jpg">
<meta name="twitter:site" content="@null">
  
  <!-- 站点验证相关 -->
  
    
    
    
  
  <!-- 样式表文件 -->
  <link rel="stylesheet" id="kratos-css" href="/css/kratosr.min.css" type="text/css" media="all">
  
    <link rel="stylesheet" id="highlight-css" href="/css/highlight/night-eighties.min.css" type="text/css" media="all">
  
  
  <link rel="stylesheet" id="fontawe-css" href="https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css" type="text/css" media="all">
  <link rel="stylesheet" id="nprogress-css" href="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.min.css" type="text/css" media="all">
  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.css">
  
  
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css">
  
  
    <link rel="stylesheet" id="darkmode-css" href="/css/kr-dark.min.css" type="text/css" media="all">
  
  <!-- 不得不预先加载的一些JS文件 -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
  
    <script src="https://cdn.jsdelivr.net/npm/qrcode_js@1.0.0/qrcode.min.js"></script>
  
<meta name="generator" content="Hexo 5.4.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


    <body class="custom-background">
        <div id="kratos-wrapper">
    <div id="kratos-page">
        <div id="kratos-header">
            <header id="kratos-desktop-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="nav-header">
                        <nav id="kratos-menu-wrap">
                            <ul id="kratos-primary-menu" class="sf-menu">
                                
                                    
                                        <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
                                    
                                
                                    
                                        <li><a href="/archives/"><i class="fa fa-file"></i>档案馆</a></li>
                                    
                                
                                    
                                        <li>
                                            <a><i class="fa fa-link"></i>链接</a>
                                            <ul class="sub-menu">
                                                
                                                    
                                                
                                                    
                                                        <li><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36571422">CSDN博客</a></li>
                                                    
                                                
                                            </ul>
                                        </li>
                                    
                                
                            </ul>
                        </nav>
                    </div>
                </div>
            </header>
            <header id="kratos-mobile-topnav" class="kratos-topnav">
                <div class="container">
                    <div class="color-logo"><a href="/">菠萝菠萝卜的博客</a></div>
                    <div class="nav-toggle">
                        <a class="kratos-nav-toggle js-kratos-nav-toggle">
                            <i></i>
                        </a>
                    </div>
                </div>
            </header>
        </div>
        <div class="kratos-start kratos-hero-2">
            <!-- <div class="kratos-overlay"></div> -->
            <div class="kratos-cover kratos-cover-2 text-center">
                <div class="desc desc2 animate-box">
                    <a href="/">
                        <h2>菠萝菠萝卜的博客</h2> <br />
                        <span></span>
                    </a>
                </div>
            </div>
        </div>

        <div id="kratos-blog-post">
            <div class="container">
                <div id="main" class="row">
                    

        <section class="col-md-8">
    <article>
        <div class="kratos-hentry kratos-post-inner clearfix">
            <header class="kratos-entry-header">
                <h1 class="kratos-entry-title text-center">pytorch常用方法记录</h1>
                
                <ul class="kratos-post-meta text-center">
                    <li><i class="fa fa-calendar"></i> 2021-11-03</li>
                    <li><i class="fa fa-user"></i> 作者 菠萝菠萝卜</li>
                    <li>
                        <i class="fa fa-edit"></i> 
                        
                        
                            ~6.98K
                        
                        字
                    </li>
                    
                        <li id="/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/" class="leancloud_visitors" data-flag-title="pytorch常用方法记录">
                            <i class="fa fa-eye"></i>
                            <span class="leancloud-visitors-count"> </span> 次阅读
                        </li>
                        
                    
                </ul>
            </header>
            <div class="kratos-post-content">
                <div id="expire-alert" class="alert alert-warning hidden" role="alert">
                    本文最后编辑于 <time datetime="1641389529917"></time> 前，其中的内容可能需要更新。
                </div>
                
                    <div class="kratos-post-inner-toc">
                        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95"><span class="toc-number">1.</span> <span class="toc-text">pytorch常用方法记录</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">常用损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5CrossEntropyLoss"><span class="toc-number">1.1.1.</span> <span class="toc-text">交叉熵CrossEntropyLoss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E5%B7%AEMSELoss-L2Loss"><span class="toc-number">1.1.2.</span> <span class="toc-text">均方差MSELoss (L2Loss)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BCELoss"><span class="toc-number">1.1.3.</span> <span class="toc-text">BCELoss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L1Loss-MAE"><span class="toc-number">1.1.4.</span> <span class="toc-text">L1Loss (MAE)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">常用激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.1.</span> <span class="toc-text">Sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLu%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">ReLu函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LeakyReLU"><span class="toc-number">1.2.3.</span> <span class="toc-text">LeakyReLU</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E7%A7%8D%E5%B1%82"><span class="toc-number">1.3.</span> <span class="toc-text">各种层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%85%83%E8%87%AA%E9%80%82%E5%BA%94%E5%9D%87%E5%80%BC%E6%B1%87%E8%81%9A%E5%B1%82"><span class="toc-number">1.3.1.</span> <span class="toc-text">二元自适应均值汇聚层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">常用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-squeeze"><span class="toc-number">1.4.1.</span> <span class="toc-text">torch.squeeze()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-unsqueeze"><span class="toc-number">1.4.2.</span> <span class="toc-text">torch.unsqueeze()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-empty"><span class="toc-number">1.4.3.</span> <span class="toc-text">torch.empty()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-split"><span class="toc-number">1.4.4.</span> <span class="toc-text">torch.split()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E7%A7%8Dpadding%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.4.5.</span> <span class="toc-text">四种padding模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ReflectionPad2d"><span class="toc-number">1.4.5.1.</span> <span class="toc-text">nn.ReflectionPad2d</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ZeroPad2d"><span class="toc-number">1.4.5.2.</span> <span class="toc-text">nn.ZeroPad2d</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ReplicationPad2d"><span class="toc-number">1.4.5.3.</span> <span class="toc-text">nn.ReplicationPad2d</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ConstantPad2d"><span class="toc-number">1.4.5.4.</span> <span class="toc-text">nn.ConstantPad2d</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6-torch-cuda-amp"><span class="toc-number">1.4.6.</span> <span class="toc-text">自动混合精度(torch.cuda.amp)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.4.6.1.</span> <span class="toc-text">为什么使用自动混合梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.6.2.</span> <span class="toc-text">使用方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E5%AD%98%E5%82%A8"><span class="toc-number">1.4.7.</span> <span class="toc-text">图片存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-DataParallel%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-number">1.4.8.</span> <span class="toc-text">nn.DataParallel数据并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#net-train-eval"><span class="toc-number">1.4.9.</span> <span class="toc-text">net.train()&#x2F;eval()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-where"><span class="toc-number">1.4.10.</span> <span class="toc-text">torch.where()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-as-tensor-%E5%92%8Ctorch-from-numpy"><span class="toc-number">1.4.11.</span> <span class="toc-text">torch.as_tensor()和torch.from_numpy()</span></a></li></ol></li></ol></li></ol>
                    </div>
                
                <hr />
                <h1 id="pytorch常用方法记录"><a href="#pytorch常用方法记录" class="headerlink" title="pytorch常用方法记录"></a>pytorch常用方法记录</h1><p> 本篇记录在学习使用pytorch时，常用的东西记录，以便随时查看。遇到新学到的会更新在这里。</p>
<h2 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h2><h3 id="交叉熵CrossEntropyLoss"><a href="#交叉熵CrossEntropyLoss" class="headerlink" title="交叉熵CrossEntropyLoss"></a>交叉熵CrossEntropyLoss</h3><p>计算公式：</p>
<script type="math/tex; mode=display">
E = -\sum_k t_klog(y_k)</script><p>这个公式在深度学习入门04中讲过。</p>
<p>python代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cross_entropy_loss = torch.CrossEntropyLoss()</span><br><span class="line">loss = cross_entropy_loss(<span class="built_in">input</span>: Tensor, target: Tensor)</span><br></pre></td></tr></table></figure>
<p><strong>注意：</strong>CrossEntropyLoss的输入就直接是Model的输出，不要是softmax之后的。还有就是label必须是long类型在[0, class-1]之间。</p>
<p><img src="/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/cross_entropy_loss.jpg" alt></p>
<p>查看源码中的解释即可知道，CrossEntropyLoss = log_softmax + NllLoss，因此不需要softmax。其中的C代表的类型。</p>
<h3 id="均方差MSELoss-L2Loss"><a href="#均方差MSELoss-L2Loss" class="headerlink" title="均方差MSELoss (L2Loss)"></a>均方差MSELoss (L2Loss)</h3><p>计算公式：</p>
<script type="math/tex; mode=display">
MSE = \frac {1}{m} \sum_i(y_i - \hat {y_i})^2</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mseLoss = torch.MSELoss()</span><br><span class="line">loss = mseLoss(<span class="built_in">input</span>: Tensor, target: Tensor)</span><br></pre></td></tr></table></figure>
<h3 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h3><p>bce loss分类，用于二分类问题。数学公式如下</p>
<script type="math/tex; mode=display">
loss(X_i,y_i) = -w_i[y_ilogx_i + (1 - y_i)log(1 - xi)]</script><p>pytorch中bceloss</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">torch</span>.<span class="title">nn</span>.<span class="title">BCELoss</span>(<span class="params">weight: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>, size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction: <span class="built_in">str</span> = <span class="string">'mean'</span></span>)</span></span><br></pre></td></tr></table></figure>
<p>weight: 初始化权重矩阵</p>
<p>size_average: 默认是True，对loss求平均数</p>
<p>reduction: 默认求和， 对于batch_size的loss平均数</p>
<h3 id="L1Loss-MAE"><a href="#L1Loss-MAE" class="headerlink" title="L1Loss (MAE)"></a>L1Loss (MAE)</h3><p>指的是一范数损失，也叫MAE，就是<strong>估计值和目标值做差的绝对值</strong></p>
<p>公式：</p>
<script type="math/tex; mode=display">
loss(x,y) = \frac {1}{n} \sum_{i=1}^n|y_i-f(x_i)|</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">l1_loss = nn.L1Loss()</span><br><span class="line">loss = l1_loss(<span class="built_in">input</span>:Tensor, target:Tensor)</span><br></pre></td></tr></table></figure>
<h2 id="常用激活函数"><a href="#常用激活函数" class="headerlink" title="常用激活函数"></a>常用激活函数</h2><h3 id="Sigmoid函数"><a href="#Sigmoid函数" class="headerlink" title="Sigmoid函数"></a>Sigmoid函数</h3><p>公式：</p>
<script type="math/tex; mode=display">
h(x) = \frac {1}{1 + e^{(-x)}}</script><p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Sigmoid()</span><br></pre></td></tr></table></figure>
<h3 id="ReLu函数"><a href="#ReLu函数" class="headerlink" title="ReLu函数"></a>ReLu函数</h3><p>ReLU函数：在输入大于0时，直接输出该值；在小于等于0时为0</p>
<p>公式：</p>
<script type="math/tex; mode=display">
h(x) = \begin {cases}x, &{(x \gt 0)}\\\\ 0, &{(x \leq 0)} \end{cases}</script><p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.ReLU(inplace= <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>inplace若为True，则覆盖之前的值，可以减少内存消耗。</p>
<h3 id="LeakyReLU"><a href="#LeakyReLU" class="headerlink" title="LeakyReLU"></a>LeakyReLU</h3><p>虽然relu在SGD中收敛很快，但是对于小于0的值梯度永远是0，那么激活函数收到的值通常都会有bias，如果偏置很小，导致值一直都是负数，使得梯度无法更新，最终导致神经元“死亡”。所以提出了LeakyReLU，当x小于0时不再等于0而是等于一个weight * x。通常这个权重取0.2</p>
<script type="math/tex; mode=display">
y = max(0,x) + weight * min(0,x)</script><p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.LeakyReLU(<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure>
<h2 id="各种层"><a href="#各种层" class="headerlink" title="各种层"></a>各种层</h2><h3 id="二元自适应均值汇聚层"><a href="#二元自适应均值汇聚层" class="headerlink" title="二元自适应均值汇聚层"></a>二元自适应均值汇聚层</h3><p>在实现ResNet的时候，看了下pytorch里面的ResNet源码，看到平均池化层使用的是<strong>nn.AdaptiveAvgPool2d((1, 1))</strong>，就觉的好奇查了下。</p>
<p>其实就是平均池化层，好处就是不需要我们自己算平均池化的步长啊，核大小啊，填充啊这些，只用传入一个我想要的大小，其他都是去自适应，还是蛮舒服的哈:happy:</p>
<h2 id="常用方法"><a href="#常用方法" class="headerlink" title="常用方法"></a>常用方法</h2><h3 id="torch-squeeze"><a href="#torch-squeeze" class="headerlink" title="torch.squeeze()"></a>torch.squeeze()</h3><p>对数据的维度进行压缩，去掉某行或某列。只能对于为1的维度。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn((<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(t.size())</span><br><span class="line">t = t.squeeze(<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(t.size())</span><br></pre></td></tr></table></figure>
<p>out:</p>
<p>torch.Size([2, 3, 1])<br>torch.Size([2, 3])</p>
<h3 id="torch-unsqueeze"><a href="#torch-unsqueeze" class="headerlink" title="torch.unsqueeze()"></a>torch.unsqueeze()</h3><p>将数据维数进行扩充。比如：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn((<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(t.size())</span><br><span class="line">t = torch.unsqueeze(t,dim=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(t.size())</span><br><span class="line"><span class="comment"># 或者 tensor.unsqueeze(dim)</span></span><br><span class="line">t = t.unsqueeze(<span class="number">2</span>) <span class="comment"># 是一样的</span></span><br></pre></td></tr></table></figure>
<p>out：</p>
<p>torch.Size([2, 3, 3])<br>torch.Size([2, 3, 1, 3])</p>
<h3 id="torch-empty"><a href="#torch-empty" class="headerlink" title="torch.empty()"></a>torch.empty()</h3><p>之前pytorch入门笔记01讲过，这里将如何赋值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">t = torch.empty(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t.data[<span class="number">0</span>].copy_(torch.randn(<span class="number">3</span>))</span><br><span class="line">t.data[<span class="number">1</span>].copy_(torch.randn(<span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<p>data找到对应的值，然后copy进去即可。</p>
<h3 id="torch-split"><a href="#torch-split" class="headerlink" title="torch.split()"></a>torch.split()</h3><p>数据分割使用的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">images = torch.randn((<span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">im1, im2 = images.split(<span class="number">1</span>, dim=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(im1.shape)</span><br><span class="line"><span class="comment"># tensor.split(split_size, dim)</span></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line"><span class="comment"># -- split_size </span></span><br><span class="line"><span class="comment"># 分割大小，比如如果是(3, 6, 5, 5)</span></span><br><span class="line"><span class="comment"># 6通道的图，此时设定split_size为3，那么返回的tuple为两个也就是 </span></span><br><span class="line"><span class="comment"># im1, im2 = tensor.split(3, dim=1)</span></span><br><span class="line"><span class="comment"># -- dim    分割那个维度，这个好理解。</span></span><br></pre></td></tr></table></figure>
<h3 id="四种padding模式"><a href="#四种padding模式" class="headerlink" title="四种padding模式"></a>四种padding模式</h3><p>四个模式为Reflect，Zero，Replication，Constant。注意padding的大小不能超过matrix的大小。这部分的四个模式也是对应Convolution中的padding_mode设置。该设置有四种模式（’zeros’, ‘reflect’, ‘replicate’, ‘circular’）</p>
<p>一下例子以3 x 3大小的矩阵举例。这部分是pytorch源码中用的例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">testA = torch.arange(<span class="number">9</span>, dtype=torch.<span class="built_in">float</span>).reshape(<span class="number">1</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<p>out：</p>
<p>tensor([[[[0., 1., 2.],<br>                  [3., 4., 5.],<br>                  [6., 7., 8.]]]])</p>
<p><strong>注意：</strong>还有这里就是所有的Pad参数都是填padding大小，ConstantPad当中需要在填入一个value。padding的参数如果是int就是周围都填充这么多。如果是一个有4个参数的tuple，表示的是左，右，上，下的顺序依次填充数量。</p>
<h4 id="nn-ReflectionPad2d"><a href="#nn-ReflectionPad2d" class="headerlink" title="nn.ReflectionPad2d"></a>nn.ReflectionPad2d</h4><p>字面意思镜像复制，就是以当前位置作为对称轴，将后面的值按照镜像翻转一份出来。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reflectPadding = nn.ReflectionPad2d(<span class="number">2</span>)</span><br><span class="line">testA = reflectPadding(testA)</span><br></pre></td></tr></table></figure>
<p><img src="/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/reflect.jpg" alt></p>
<h4 id="nn-ZeroPad2d"><a href="#nn-ZeroPad2d" class="headerlink" title="nn.ZeroPad2d"></a>nn.ZeroPad2d</h4><p>这个是使用0进行填充</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zerosPadding = nn.ZeroPad2d(<span class="number">1</span>)</span><br><span class="line">testA = zerosPadding(testA)</span><br></pre></td></tr></table></figure>
<p><img src="/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/zerosPadding.jpg" alt></p>
<h4 id="nn-ReplicationPad2d"><a href="#nn-ReplicationPad2d" class="headerlink" title="nn.ReplicationPad2d"></a>nn.ReplicationPad2d</h4><p>复制填充，将周围的值复制一份填充到边框。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">replicationPadding = nn.ReplicationPad2d(<span class="number">1</span>)</span><br><span class="line">testA = replicationPadding(testA)</span><br></pre></td></tr></table></figure>
<p><img src="/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/replicationPadding.jpg" alt></p>
<h4 id="nn-ConstantPad2d"><a href="#nn-ConstantPad2d" class="headerlink" title="nn.ConstantPad2d"></a>nn.ConstantPad2d</h4><p>设定固定值作为填充</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">constantPadding = nn.ConstantPad2d(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">testA = constantPadding(testA)</span><br></pre></td></tr></table></figure>
<p><img src="/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/ConstantPadding.jpg" alt></p>
<h3 id="自动混合精度-torch-cuda-amp"><a href="#自动混合精度-torch-cuda-amp" class="headerlink" title="自动混合精度(torch.cuda.amp)"></a>自动混合精度(torch.cuda.amp)</h3><p><strong>作用</strong>：减小显存占用，加快训练速度。</p>
<p><strong>适用设备</strong>：支持Tensor Core的CUDA设备。</p>
<p><strong>使用方式</strong>：torch.cuda.amp.autocast和torch.cuda.amp.GradScaler结合适用</p>
<p><strong>混合精度</strong>：表示不止一种精度的Tensor，在AMP中有两种：FloatTensor，HalfTensor</p>
<p><strong>自动</strong>：框架可以按需自动调整Tensor的dtype</p>
<h4 id="为什么使用自动混合梯度"><a href="#为什么使用自动混合梯度" class="headerlink" title="为什么使用自动混合梯度"></a>为什么使用自动混合梯度</h4><p>torch.HalfTensor优势：</p>
<p>存储小、计算快、更好利用Tensor Core，减少显存占用意味着可以多添加batchsize，训练加快。</p>
<p>torch.HalfTensor劣势：</p>
<p>数值范围小容易Overflow、舍入误差。</p>
<p><strong>解决方案</strong></p>
<ol>
<li>梯度Scale，放大loss的值防止梯度underflow。</li>
<li>回落到torch.FloatTensor，至于什么时候回落这个由框架自行决定</li>
</ol>
<h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model = Net().to(<span class="string">"cuda"</span>)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), ...)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练前初实例化一个GradScaler对象</span></span><br><span class="line">scaler = torch.cuda.amp.GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataLoader):</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    images, labels = data</span><br><span class="line">    images = images.to(<span class="string">"cuda"</span>)</span><br><span class="line">    labels = labels.to(<span class="string">"cuda"</span>)</span><br><span class="line">    <span class="comment"># 前向过程(model + loss)开启autocast</span></span><br><span class="line">    <span class="keyword">with</span> torch.cuda.amp.autocast():</span><br><span class="line">        out = model(images)</span><br><span class="line">        loss = loss_fn(output, target)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播在autocast之外</span></span><br><span class="line">    <span class="comment"># Scaler loss,放大梯度</span></span><br><span class="line">    scaler.scale(loss).backward()</span><br><span class="line">    <span class="comment"># scaler.step() 首先吧梯度值unscaler回来</span></span><br><span class="line">    <span class="comment"># 如果梯度值不是infs或者NaNs,那么调用optimizer.step()更新权重</span></span><br><span class="line">    <span class="comment"># 否则忽略step调用，保证权重不更新</span></span><br><span class="line">    scaler.step(optimizer)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 准备着，看是否增大scaler</span></span><br><span class="line">    scaler.update()</span><br></pre></td></tr></table></figure>
<p>这里不需要在input上手动调用<code>.half()</code>,这个是框架自动去做的。</p>
<h3 id="图片存储"><a href="#图片存储" class="headerlink" title="图片存储"></a>图片存储</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.utils.save_image(tensor, filename)</span><br></pre></td></tr></table></figure>
<p>或</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用matplotlib</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.imshow(numpydata)</span><br><span class="line">plt.savefig(name)</span><br></pre></td></tr></table></figure>
<h3 id="nn-DataParallel数据并行"><a href="#nn-DataParallel数据并行" class="headerlink" title="nn.DataParallel数据并行"></a>nn.DataParallel数据并行</h3><p>该方法在模块级别上实现了数据并行。相当nice的一个东西，当有多块显卡的时候，并且单块显存不足的时候，训练慢的时候都可以用。他做的工作是：将模块复制到每个设备上，每个副本都处理输入数据的一部分，然后在后向传递的过程中，每个副本的梯度会被累加到原始模块中。</p>
<p><strong>注意</strong> batch_size应该比GPU数量要多。</p>
<p>官方推荐使用的是DistributedDataParallel，来替代nn.DataParallel。即便只有一块卡，也是推荐使用DistributedDataParallel。两个重要区别就是DataParallel用的是多线程，DistributedDataParallel使用的是多进程。使用multiprocessing，每个GPU都有自己的专用进程，避免了python解释器GIL带来的性能开销。</p>
<p><strong>DataParallel使用方法：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = Net()	<span class="comment"># 网络</span></span><br><span class="line">net = torch.nn.DataParallel(net, device_ids=[<span class="number">0</span>, <span class="number">1</span>]) <span class="comment"># 使用0,1号GPU</span></span><br><span class="line"><span class="comment"># 后面正常使用就可以了</span></span><br></pre></td></tr></table></figure>
<p><strong>DistributedDataParallel使用方法：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> torch.multiprocessing <span class="keyword">as</span> mp</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">example</span>(<span class="params">rank, world_size</span>):</span></span><br><span class="line">    <span class="comment"># create default process group</span></span><br><span class="line">    dist.init_process_group(<span class="string">"gloo"</span>, rank=rank, world_size=world_size)</span><br><span class="line">    <span class="comment"># create local model</span></span><br><span class="line">    model = nn.Linear(<span class="number">10</span>, <span class="number">10</span>).to(rank)</span><br><span class="line">    <span class="comment"># construct DDP model</span></span><br><span class="line">    ddp_model = DDP(model, device_ids=[rank])</span><br><span class="line">    <span class="comment"># define loss function and optimizer</span></span><br><span class="line">    loss_fn = nn.MSELoss()</span><br><span class="line">    optimizer = optim.SGD(ddp_model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass</span></span><br><span class="line">    outputs = ddp_model(torch.randn(<span class="number">20</span>, <span class="number">10</span>).to(rank))</span><br><span class="line">    labels = torch.randn(<span class="number">20</span>, <span class="number">10</span>).to(rank)</span><br><span class="line">    <span class="comment"># backward pass</span></span><br><span class="line">    loss_fn(outputs, labels).backward()</span><br><span class="line">    <span class="comment"># update parameters</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    world_size = <span class="number">2</span></span><br><span class="line">    mp.spawn(example,</span><br><span class="line">        args=(world_size,),</span><br><span class="line">        nprocs=world_size,</span><br><span class="line">        join=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<h3 id="net-train-eval"><a href="#net-train-eval" class="headerlink" title="net.train()/eval()"></a>net.train()/eval()</h3><p>对应修改的模型中的training属性，如果是train()方法，表示训练模式，training属性为True，可以在模型中控制在训练的时候，在模型中通过training属性进行判断，是否执行指定部分代码。如果调用eval()，表示是测试模式，training为False。</p>
<h3 id="torch-where"><a href="#torch-where" class="headerlink" title="torch.where()"></a>torch.where()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.where(condition: Tensor, self, other)</span><br></pre></td></tr></table></figure>
<p>根据条件，将tensor修改。用法第一个参数是条件，第二个是条件为True的值，第三个是条件为False的值。</p>
<h3 id="torch-as-tensor-和torch-from-numpy"><a href="#torch-as-tensor-和torch-from-numpy" class="headerlink" title="torch.as_tensor()和torch.from_numpy()"></a>torch.as_tensor()和torch.from_numpy()</h3><p>两者都是共享数据，所以转换tensor很快，且节省内存。可以推荐使用as_tensor()，因为as_tensor()可以接受任何像Python数据结构这样的数组。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">torch.as_tensor(data, dtype, device)</span></span><br><span class="line"><span class="string">- data(类似数组一样的) ：tensor初始化数据。可以是list, tuple, Numpy的ndarray，scalar，和其他类型。</span></span><br><span class="line"><span class="string">- dtype(torch.dtype) 希望返回的tensor数据类型。默认：如果是None，从data中推断数据类型</span></span><br><span class="line"><span class="string">- device(torch.device) 希望返回的tensor所在设备。默认：如果是None，使用当前设备作为默认tensor类型。</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">torch.as_tensor(np.array(label), dtype=torch.int64) <span class="comment"># 在做图像分割任务时的label为单通道，里面值对应类型数时可以用这个转为tensor</span></span><br></pre></td></tr></table></figure>

            </div>
            
                <div class="kratos-copyright text-center clearfix">
                    <h5>本作品采用 <a rel="license nofollow" target="_blank" href="https://creativecommons.org/licenses/by-sa/4.0/">知识共享署名-相同方式共享 4.0 国际许可协议</a> 进行许可</h5>
                </div>
            
            <footer class="kratos-entry-footer clearfix">
                
                    <div class="post-like-donate text-center clearfix" id="post-like-donate">
                    
                        <a class="donate" href="javascript:;"><i class="fa fa-bitcoin"></i> 打赏</a>
                    
                    
                        <a class="share" href="javascript:;"><i class="fa fa-share-alt"></i> 分享</a>
                        <div class="share-wrap" style="display: none;">
    <div class="share-group">
        <a href="javascript:;" class="share-plain qq" onclick="share('qq');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-qq"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain qzone" onclick="share('qzone');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-star"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weixin pop style-plain" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weixin"></i>
            </div>
            <div class="share-int">
                <div class="qrcode" id="wechat-qr"></div>
                <p>打开微信“扫一扫”，打开网页后点击屏幕右上角分享按钮</p>
            </div>
        </a>
        <a href="javascript:;" class="share-plain weibo" onclick="share('weibo');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-weibo"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain facebook style-plain" onclick="share('facebook');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-facebook"></i>
            </div>
        </a>
        <a href="javascript:;" class="share-plain twitter style-plain" onclick="share('twitter');" rel="nofollow">
            <div class="icon-wrap">
                <i class="fa fa-twitter"></i>
            </div>
        </a>
    </div>
    <script type="text/javascript">
        $(()=>{
            new QRCode("wechat-qr", {
                text: "https://www.boluoboluobo.top/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/",
                width: 150,
                height: 150,
                correctLevel : QRCode.CorrectLevel.H
            });
        });
        function share(dest) {
            const qqBase        = "https://connect.qq.com/widget/shareqq/index.html?";
            const weiboBase     = "https://service.weibo.com/share/share.php?";
            const qzoneBase     = "https://sns.qzone.qq.com/cgi-bin/qzshare/cgi_qzshare_onekey?";
            const facebookBase  = "https://www.facebook.com/sharer/sharer.php?";
            const twitterBase   = "https://twitter.com/intent/tweet?";
            const hostUrl       = "https://www.boluoboluobo.top/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/";
            const title         = "「pytorch常用方法记录」";
            const excerpt       = `pytorch常用方法记录 本篇记录在学习使用pytorch时，常用的东西记录，以便随时查看。遇到新学到的会更新在这里。
常用损失函数交叉熵CrossEntropyLoss计算公式：

E = -\sum_k t_klog(y_k)这...`;
            let _URL;
            switch (dest) {
                case "qq"       : _URL = qqBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";     break;
                case "weibo"    : _URL = weiboBase+"url="+hostUrl+"&title="+title+excerpt;                                 break;
                case "qzone"    : _URL = qzoneBase+"url="+hostUrl+"&title="+title+"&desc=&summary="+excerpt+"&site=cxpy";  break;
                case "facebook" : _URL = facebookBase+"u="+hostUrl;                                                        break;
                case "twitter"  : _URL = twitterBase+"text="+title+excerpt+"&url="+hostUrl;                                break;
            }
            window.open(_URL);
        };
    </script>
</div>
                    
                    </div>
                
                <div class="footer-tag clearfix">
                    <div class="pull-left">
                    <i class="fa fa-tags"></i>
                        <a class="tag-none-link" href="/tags/python/" rel="tag">python</a>, <a class="tag-none-link" href="/tags/pytorch/" rel="tag">pytorch</a>
                    </div>
                    <div class="pull-date">
                    <span>最后编辑：2022-01-05</span>
                    </div>
                </div>
            </footer>
        </div>
        
            <nav class="navigation post-navigation clearfix" role="navigation">
                
                <div class="nav-previous clearfix">
                    <a title=" GAN生成动漫头像" href="/2021/10/29/GAN生成动漫头像/">&lt; 上一篇</a>
                </div>
                
                
                <div class="nav-next clearfix">
                    <a title=" ResNet学习及实现" href="/2021/11/05/ResNet学习及实现/">下一篇 &gt;</a>
                </div>
                
            </nav>
        
        
            <div id="v-comments" class="post-comments"></div>
<script>
    var load_comm = () => {
        const init = () => {
            new Valine({
                el: '#v-comments',
                appId: 'pnDNVddr900afG7XglGQgJCb-gzGzoHsz',
                appKey: 'HA3J9tdvziBrLcSml8Kai0qH',
                visitor: true,
                enableQQ: false,
                path: '/2021/11/03/pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95/',
				avatar: ''
            });
        }
        if (typeof Valine == 'undefined') {
            const src = 'https://cdn.jsdelivr.net/npm/valine@1.4.14/dist/Valine.min.js';
            $.getScript(src, init);
        } else {
            init();
        }
    };
</script>
<noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="https://valine.js.org/">comments powered by Valine.</a></noscript>

        
    </article>
</section>

                
            

<section id="kratos-widget-area" class="col-md-4 hidden-xs hidden-sm">
    <!-- 文章和页面根据splitter来分割，没有的话就从头开始设置为sticky -->
    
    
                <aside id="krw-about" class="widget widget-kratos-about clearfix">
    <div class="photo-background"></div>
    <div class="photo-wrapper clearfix">
        <div class="photo-wrapper-tip text-center">
            <img class="about-photo" src="/images/avatar.jpg" />
        </div>
    </div>
    <div class="textwidget">
        <p class="text-center">上上下下，左右左右，BABA！</p>
    </div>
</aside>
            
                    <div class="sticky-area">
                
                    <aside id="krw-toc" class="widget widget-kratos-toc clearfix">
    <div class="photo-background"></div>
    <h4 class="widget-title no-after">
        <i class="fa fa-compass"></i>
        文章目录
        <span class="toc-progress-bar"></span>
    </h4>
    <div class="textwidget">
        <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95%E8%AE%B0%E5%BD%95"><span class="toc-number">1.</span> <span class="toc-text">pytorch常用方法记录</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.</span> <span class="toc-text">常用损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E7%86%B5CrossEntropyLoss"><span class="toc-number">1.1.1.</span> <span class="toc-text">交叉熵CrossEntropyLoss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9D%87%E6%96%B9%E5%B7%AEMSELoss-L2Loss"><span class="toc-number">1.1.2.</span> <span class="toc-text">均方差MSELoss (L2Loss)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BCELoss"><span class="toc-number">1.1.3.</span> <span class="toc-text">BCELoss</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#L1Loss-MAE"><span class="toc-number">1.1.4.</span> <span class="toc-text">L1Loss (MAE)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.</span> <span class="toc-text">常用激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Sigmoid%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.1.</span> <span class="toc-text">Sigmoid函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReLu%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">ReLu函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LeakyReLU"><span class="toc-number">1.2.3.</span> <span class="toc-text">LeakyReLU</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E7%A7%8D%E5%B1%82"><span class="toc-number">1.3.</span> <span class="toc-text">各种层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E5%85%83%E8%87%AA%E9%80%82%E5%BA%94%E5%9D%87%E5%80%BC%E6%B1%87%E8%81%9A%E5%B1%82"><span class="toc-number">1.3.1.</span> <span class="toc-text">二元自适应均值汇聚层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.</span> <span class="toc-text">常用方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-squeeze"><span class="toc-number">1.4.1.</span> <span class="toc-text">torch.squeeze()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-unsqueeze"><span class="toc-number">1.4.2.</span> <span class="toc-text">torch.unsqueeze()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-empty"><span class="toc-number">1.4.3.</span> <span class="toc-text">torch.empty()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-split"><span class="toc-number">1.4.4.</span> <span class="toc-text">torch.split()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E7%A7%8Dpadding%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.4.5.</span> <span class="toc-text">四种padding模式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ReflectionPad2d"><span class="toc-number">1.4.5.1.</span> <span class="toc-text">nn.ReflectionPad2d</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ZeroPad2d"><span class="toc-number">1.4.5.2.</span> <span class="toc-text">nn.ZeroPad2d</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ReplicationPad2d"><span class="toc-number">1.4.5.3.</span> <span class="toc-text">nn.ReplicationPad2d</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nn-ConstantPad2d"><span class="toc-number">1.4.5.4.</span> <span class="toc-text">nn.ConstantPad2d</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6-torch-cuda-amp"><span class="toc-number">1.4.6.</span> <span class="toc-text">自动混合精度(torch.cuda.amp)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E4%BD%BF%E7%94%A8%E8%87%AA%E5%8A%A8%E6%B7%B7%E5%90%88%E6%A2%AF%E5%BA%A6"><span class="toc-number">1.4.6.1.</span> <span class="toc-text">为什么使用自动混合梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95"><span class="toc-number">1.4.6.2.</span> <span class="toc-text">使用方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E7%89%87%E5%AD%98%E5%82%A8"><span class="toc-number">1.4.7.</span> <span class="toc-text">图片存储</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-DataParallel%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-number">1.4.8.</span> <span class="toc-text">nn.DataParallel数据并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#net-train-eval"><span class="toc-number">1.4.9.</span> <span class="toc-text">net.train()&#x2F;eval()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-where"><span class="toc-number">1.4.10.</span> <span class="toc-text">torch.where()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#torch-as-tensor-%E5%92%8Ctorch-from-numpy"><span class="toc-number">1.4.11.</span> <span class="toc-text">torch.as_tensor()和torch.from_numpy()</span></a></li></ol></li></ol></li></ol>
    </div>
</aside>
                
                
  <aside id="krw-categories" class="widget widget-kratos-categories clearfix">
    <h4 class="widget-title"><i class="fa fa-folder"></i>分类目录</h4>
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Android/">Android</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/DeepLearning/">DeepLearning</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Kotlin/">Kotlin</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/OpenGL/">OpenGL</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/TensorFlow/">TensorFlow</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/hexo/">hexo</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">7</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/python/">python</a><span class="category-list-count">7</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%8D%9A%E6%96%87/">博文</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/">数据库</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AE%97%E6%B3%95/">算法</a><span class="category-list-count">13</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%BD%91%E7%BB%9C%E5%AD%A6%E4%B9%A0/">网络学习</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0/">论文学习</a><span class="category-list-count">1</span></li></ul>
  </aside>


            
                
  <aside id="krw-tags" class="widget widget-kratos-tags clearfix">
    <h4 class="widget-title"><i class="fa fa-tags"></i>标签聚合</h4>
      <div class="tag-clouds">
        <a href="/tags/A/" style="font-size: 0.6em;">A*</a> <a href="/tags/Android/" style="font-size: 0.6em;">Android</a> <a href="/tags/C-C/" style="font-size: 0.6em;">C/C++</a> <a href="/tags/CycleGan/" style="font-size: 0.6em;">CycleGan</a> <a href="/tags/DeepLearning/" style="font-size: 0.8em;">DeepLearning</a> <a href="/tags/Kotlin/" style="font-size: 0.8em;">Kotlin</a> <a href="/tags/OpenGL/" style="font-size: 0.67em;">OpenGL</a> <a href="/tags/TensorFlow/" style="font-size: 0.73em;">TensorFlow</a> <a href="/tags/android/" style="font-size: 0.6em;">android</a> <a href="/tags/app/" style="font-size: 0.6em;">app</a> <a href="/tags/bfs/" style="font-size: 0.6em;">bfs</a> <a href="/tags/cgan/" style="font-size: 0.6em;">cgan</a> <a href="/tags/conditional-GAN/" style="font-size: 0.6em;">conditional GAN</a> <a href="/tags/deeplearning/" style="font-size: 0.6em;">deeplearning</a> <a href="/tags/gan/" style="font-size: 0.6em;">gan</a> <a href="/tags/hexo/" style="font-size: 0.6em;">hexo</a> <a href="/tags/html5/" style="font-size: 0.6em;">html5</a> <a href="/tags/java/" style="font-size: 0.67em;">java</a>
      </div>
  </aside>

            
                
  <aside id="krw-posts" class="widget widget-kratos-posts">
  <h4 class="widget-title"><i class="fa fa-file"></i>最新文章</h4>
  <div class="tab-content">
      <ul class="list-group">
        
        
          
          
            <a class="list-group-item" href="/2022/01/06/%E4%B8%89%E7%A7%8D%E4%B8%8A%E9%87%87%E6%A0%B7%E7%9A%84%E6%96%B9%E5%BC%8F%E6%80%BB%E7%BB%93/"><i class="fa  fa-book"></i> 三种上采样的方式总结</a>
            
          
        
          
          
            <a class="list-group-item" href="/2022/01/05/SegNet%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E7%8E%B0/"><i class="fa  fa-book"></i> SegNet学习及实现</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/12/02/GoogleNet%E5%AD%A6%E4%B9%A0%E5%8F%8A%E5%AE%9E%E7%8E%B0/"><i class="fa  fa-book"></i> GoogleNet学习及实现</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/11/23/%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/"><i class="fa  fa-book"></i> 模型评估</a>
            
          
        
          
          
            <a class="list-group-item" href="/2021/11/15/%E7%99%BD%E5%AB%96googleGPU%E8%B5%84%E6%BA%90%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"><i class="fa  fa-book"></i> 白嫖googleGPU资源训练模型</a>
            
          
        
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
      </ul>
  </div>
  </aside>

            
    </div>
</section>
        
        </div>
    </div>
</div>
<footer>
    <div id="footer"  class="ap-lrc"  >
        <div class="kr-tool text-center">
            <div class="tool">
                
                    <div class="box search-box">
                        <a href="/search/">
                            <span class="fa fa-search"></span>
                        </a>
                    </div>
                
                
                    <div class="box theme-box" id="darkmode-switch">
                        <span class="fa fa-adjust"></span>
                    </div>
                
                
            </div>
            <div class="box gotop-box">
                <span class="fa fa-chevron-up"></span>
            </div>
        </div>
        <div class="container">
            <div class="row">
                <div class="col-md-6 col-md-offset-3 footer-list text-center">
                    <ul class="kratos-social-icons">
                        
                        
                        
                        
                        
                        
                        
                        
                        
                    </ul>
                    <ul class="kratos-copyright">
                        <div>
                            <li>&copy; 2022 菠萝菠萝卜的博客 版权所有.</li>
                            <li>本站已运行<span id="span_dt">Loading...</span></li>
                        </div>
                        <div>
                            <li>Theme <a href="https://github.com/Candinya/Kratos-Rebirth" target="_blank">Kratos:Rebirth</a></li>
                            <li>Site built with&nbsp;<i class="fa fa-heart throb" style="color:#d43f57"></i>&nbsp;by 菠萝菠萝卜.</li>
                        </div>
                        <div>
                            <li>Powered by <a href="https://hexo.io" target="_blank" rel="nofollow">Hexo</a></li>
                            <li>Hosted on <a href="https://github.io" target="_blank">Github Pages</a></li>
                        </div>
                        <div>
                            
                            
                        </div>
                    </ul>
                </div>
            </div>
        </div>
    </div>
</footer>
</div>
</div>

        <script defer src="https://cdn.jsdelivr.net/npm/bootstrap@3.3.4/dist/js/bootstrap.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/nprogress@0.2.0/nprogress.js"></script>
<script>const notMobile = (!(navigator.userAgent.match(/(phone|pad|pod|iPhone|iPod|ios|iPad|Android|Mobile|BlackBerry|IEMobile|MQQBrowser|JUC|Fennec|wOSBrowser|BrowserNG|WebOS|Symbian|Windows Phone)/i)));</script>

    <div>
        <canvas id="snow"></canvas>
        <script async type="text/javascript" src="/js/snow.min.js"></script>
    </div>

<script async src="/js/candy.min.js"></script>

    <script defer src="https://cdn.jsdelivr.net/npm/aplayer@1.10.1/dist/APlayer.min.js"></script>
    
    <script defer src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>
    <meting-js
        server="netease"
        type="playlist"
        id="1469580721"
        order="random"
        fixed="true"
    >
    </meting-js>



    <script defer src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

<script defer src="https://cdn.jsdelivr.net/npm/clipboard@2.0.6/dist/clipboard.min.js"></script>
<script defer src="/js/kratosr.min.js"></script>
<script defer src="/js/pjax.min.js"></script>

    <script defer src="https://cdn.jsdelivr.net/npm/layui-src@2.5.5/dist/layui.all.js"></script>


    <script defer src="/js/kr-dark.min.js"></script>



<!-- Extra support for third-party plguins  -->


    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<!-- <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>-->
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/assets/hijiki.model.json"},"log":false});</script></body>
</html>